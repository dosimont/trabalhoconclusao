#+TITLE: tcbozzetti's LabBook
#+AUTHOR: Tiago
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Tiago(T) Arnaud(A) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Issues with =.tit= <2015-09-10 Thu>                                                   :tit:
I was testing the conversion of the =lulesh2.0_p216_n108_t1.chop1= trace
and I came across the following problem: This code snippet opens a
file for each node.
#+BEGIN_SRC perl
my $nb_proc = 0;
foreach my $node (@{$$resource_name{NODE}}) { 
	my $filename = $output."_$nb_proc.tit";
	open($fh[$nb_proc], "> $filename") or die "Cannot open > $filename: $!";
	nb_proc++;
}
#+END_SRC
While this code snippet assumes the number of opened files is equal to the number of tasks.
#+BEGIN_SRC perl
$task = $task - 1;
defined($tit_translate{$sname}) or die "Unknown state '$sname' for tit\n";
if($tit_translate{$sname} ne "") {
	print { $fh[$task] } "$task $tit_translate{$sname} $sname_param\n",
}
#+END_SRC
Currrently, I am assuming that there must be one =.tit= file for each
node, therefore, I modified the last snippet to write on the task's
node =.tit= file.

* Some notes <2015-09-13 Sun>                                                         :notes:
Understanding the performance of parallel applications is a concern in
the HPC community.  When considering platforms that are not available,
it is necessarty to simulate the application. Therefore, it is
possible to determine a cost-effective hardware configuration for that
particular application.  When considering a platform that is
available, simulating is also important since the access to
large-scale platforms can be costly.

Two frameworks for simulating MPI applications:
1. On-line simulations
	* The same amount of hardware is required to simulate.
	* Usually uses a simple network model to calculate the
          communication delays.
	* Can simulate the computational delays, but data-dependent
          application behavior is lost.
2. Off-line simulations
	* Use a trace of the parallel application.
	* Can be performed on a single computer.
	* Computational delays are scaled based on the performance
          differential between the original and the simulated
          platform.
		* If simulator uses time information, target and
                  original platform must be the same.
		* If trace is time independent, the only condition is
                  that the processors of the target platforms must be
                  of the same family as the original platform.
	* Communication delays are computed based on a network
          simulator.
	* Partially addresses the simulation of data-dependent
          application. Some data-dependent behavior can be captured in
          the trace and simulated.

Off-line simulators differ by the simulation models they employ to
compute simulated durations of CPU bursts and communication
operations.

On a time-independent trace, the CPU bursts or communication
operations are logged with its volume (in number of executed
instructions or in number of transferred bytes) instead of the time
when it begins and ends or its duration.  Therefore, the trace can not
be associated with a platform anymore (with the exception of the
processor family).  This imply that the MPI application can not modify
its execution according to the execution platform (AMPI applications).

*For large numbers of processes and/or numbers of actions, it may be
preferable to split the trace so as to obtain one trace file per
process.*

*Table: Time-independent actions corresponding to supported MPI communication operations.*
| MPI actions        | Trace entry                                  |
|--------------------+----------------------------------------------|
| =CPU burst=          | =<rank> compute <volume>=                      |
| =MPI_Send=           | =<rank> send <dst_rank> <volume>=              |
| =MPI_Isend=          | =<rank> Isend <dst_rank> <volume>=             |
| =MPI_Recv=           | =<rank> recv <src_rank> <volume>=              |
| =MPI_Irecv=          | =<rank> Irecv <src_rank> <volume>=             |
| =MPI_Broadcast=      | =<rank> bcast <volume>=                        |
| =MPI_Reduce=         | =<rank> reduce <vcomm> <vcomp>=                |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_counts> <vcomp>=   |
| =MPI_Allreduce=      | =<rank> allReduce <vcomm> <vcomp>=             |
| =MPI_Alltoall=       | =<rank> allToAll <send_volume> <recv_volume>=  |
| =MPI_Alltoallv=      | =<rank> allToAllv <send_volume> <send_counts>= |
|                    | =<recv_volume> <recv_counts>=                  |
| =MPI_Gather=         | =<rank> gather <send_volume> <recv_volume>=    |
| =MPI_Allgatherv=     | =<rank> allGatherV <send_count> <recv_counts>= |
| =MPI_Barrier=        | =<rank> barrier=                               |
| =MPI_Wait=           | =<rank> wait=                                  |
| =MPI_Waitall=        | =<rank> waitAll=                               |
Source: references/ref1.pdf
* Starting LabBook <2015-09-16 Wed> :journal:
Lucas setup the org mode file for the journal.
** Copied the entries from the README file
I have been looking at some org-mode documentation to make this
journal better. It turns out they have a ton of features and I plan
to make good use of them to make this project more organized.
* Testing the off-line simulation on SimGrid <2015-09-17 Thu> :tit:simgrid:
I was trying to simulate the time-independent trace files generated by
the =EXTRAE_Paraver_trace_mpich= trace. There was a problem, however.
More specifically, the issue was the ranks of the processes on the
=.tit= files. The little piece of code that I modified last thursday
assumed that the number of =.tit= files were equivalent to the number
of nodes, however that was only executed for state entries in the
trace. When an event was processed, the old code was executed and
therefore, the resulting =.tit= was inconsistent. I fixed this by
also changing the code that handles the events. Another problem is
that the ranks of the processes on the =.tit= files must start with
zero (I am not completely sure about this). If I set the ranks of
the processes to start with one, there is an exception during the
simulation. When I use the option =-map= on the =smpirun= command I get
the following (even when there is no rank zero on the =.tit= files):
#+BEGIN_SRC shell
[rank 0] -> graphene-1.nancy.grid5000.fr
[rank 1] -> graphene-2.nancy.grid5000.fr
[rank 2] -> graphene-3.nancy.grid5000.fr
[rank 3] -> graphene-4.nancy.grid5000.fr
[rank 4] -> graphene-5.nancy.grid5000.fr
[rank 5] -> graphene-6.nancy.grid5000.fr
[rank 6] -> graphene-7.nancy.grid5000.fr
[rank 7] -> graphene-8.nancy.grid5000.fr
#+END_SRC
This led me to believe that the ranks of the processes must start with
zero. This fact can cause some headache since the Paraver trace files
do not assume that they should start with zero. During the conversion
to the time-independent trace format, we simply compute the rank that
will be written on the =.tit= file by subtracting one. However, this
solution will not work for every case. Imagine if the Paraver trace uses
the rank zero...
* Compilation of all smpi replay operations <2015-09-17 Thu> :trace_replay:simgrid:tit:
I took a look at the source code of the =smpi_replay= (the
=references/ref1.pdf= does not contain all operations and all possible
arguments) and compiled all the smpi trace replay operations and its
arguments. I expect that this can be useful in the future.

| MPI actions        | Trace entry                                     |
|--------------------+-------------------------------------------------|
| =CPU burst=          | =<rank> compute <flops>=                          |
| =MPI_Send=           | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Isend=          | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Recv=           | =<rank> recv <src> <comm_size> [<datatype>]=      |
| =MPI_Irecv=          | =<rank> Irecv <src> <comm_size> [<datatype>]=     |
| =MPI_Broadcast=      | =<rank> bcast <comm_size> [<root> [<datatype>]]=  |
| =MPI_Reduce=         | =<rank> reduce <comm_size> <comp_size>=           |
|                    | =[<root> [<datatype>]]=                           |
| =MPI_AllReduce=      | =<rank> allReduce <comm_size> <comp_size>=        |
|                    | =[<datatype>]=                                    |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_sizes†> <comp_size>=  |
|                    | =[<datatype>]=                                    |
| =MPI_Gather=         | =<rank> gather <send_size> <recv_size> <root>=    |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGather=      | =<rank> allGather <send_size> <recv_size>=        |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoall=       | =<rank> allToAll <send_size> <recv_recv>=         |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoallv=      | =<rank> allToAllV <send_size> <send_sizes†>=      |
|                    | =<recv_size> <recv_sizes†>=                       |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_GatherV=        | =<rank> gatherV <send_size> <recv_sizes†> <root>= |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGatherV=     | =<rank> allGatherV <send_size> <recv_sizes†>=     |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Barrier=        | =<rank> barrier=                                  |
| =MPI_Wait=           | =<rank> wait=                                     |
| =MPI_Waitall=        | =<rank> waitAll=                                  |
| =MPI_Init=           | =<rank> init [<set_default_double>]=              |
| =MPI_Finalize=       | =<rank> finalize=                                 |
| =MPI_Comm_size=      | =<rank> comm_size <size>=                         |
| =MPI_Comm_split=     | =<rank> comm_split=                               |
| =MPI_Comm_dup=       | =<rank> comm_dup=                                 |
† =send_sizes/recv_sizes= is an array, the number of elements must be equal
to the number of processes in the communicator.

* More Paraver traces with more MPI primitives <2015-09-20 Sun> :tracing:paraver:
We need to make sure the conversion script is able to handle any kind of Paraver trace.
So far, our script does not handle some MPI primitives that are less used, such as MPI_AlltoAllv.
I tried to install the Extrae tool to use it to generate some traces but I failed in completing this task.

* New script for converting to =.tit= format <2015-10-01 Thu> :tit:
I created a new script based on the old one so I could have a better ideia of what was being done.
The script started supporting the MPI calls on the =EXTRAE_Paraver_trace_mpich= trace

* Immediate send format on the Paraver trace <2015-10-01 Thu> :paraver:tracing:
The =cgpop.linux_icc_mt.180x120.24tasks.chop1= trace contains immediate
sends and this was one of the reasons the script was failing. I
examined how the trace tool generates the entries in the trace file
when an immediate send happens. First we have a state entry telling us
that the task is in an immediate send state, then we have a event
entry (right after the state entry) with the =MPI_Isend= event. Keep in
mind that the parameters we need for an immediate send is the destiny
and the communication size. The event entry, however, does not provide
any of those. If we continue on the trace, we will find a
communication entry that is associated with the immediate send. This
communication entry is not necessarily after the event entry.  Also,
this entry contains the destiny and the message size. In the example
below the immediate send event entry is the first line while the
communication entry of the immediate send is the last line.
#+BEGIN_SRC shell
2:3:1:3:1:52372287:50000001:3:42000050:14833...
1:6:1:6:1:52372820:52379709:1
2:6:1:6:1:52372820:50000001:0:42000050:8708...
1:6:1:6:1:52379709:52390070:11
2:6:1:6:1:52379709:50000001:4:42000050:2191...
1:3:1:3:1:52389139:52396811:1
2:3:1:3:1:52389139:50000001:0
3:3:1:3:1:52372287:52389139:2:1:2:1:52853369:53050068:9912:103
#+END_SRC

* 2015-09-15 Questions
The list below contains all questions that I came across during this project.
Feel free to contribute with questions or answers :)
1. The use of a time-independent trace collected on a platform X can
   be used to simulate a platform Y under what conditions? Condition
   example, X and Y must contain the same family of processors or same
   number of processors. The reference 1 was not so clear about this.

* 2015-10-02 Using Arnaud's init.org                                  :Lucas:

Please, follow the instructions of:

http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php

* 2015-10-02 Installing Extrae                                  :Lucas:Tiago:

That's really hard, lots of dependencies, problems, log below tries to
handle all that, but several indications are Debian testing-specific.

Download latest Extrae from:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Dyninst dependencies in my box:

#+BEGIN_SRC sh
sudo apt-get install libboost-thread-dev libboost-system-dev libelf-dev
#+END_SRC

Download dyinst, configure and install:

#+begin_src sh :results output :session :exports both
cd ~/misc
wget --quiet http://www.paradyn.org/release9.0.3/DyninstAPI-9.0.3.tgz
tar xfz DyninstAPI-9.0.3.tgz
cd DyninstAPI-9.0.3
mkdir build
cd build
cmake ..
make
make install
#+end_src

Let's install other Extrae's dependencies:

#+begin_src R :results output :session :exports both
sudo apt-get install libopempi-dev libdwarf-dev libpapi-dev libbinutils-dev
#+end_src

I have put in my *misc* directory:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.1.0.tar.bz2
cd extrae-3.1.0
./configure --prefix=$HOME/install/extrae-3.1.0/ \
            --with-mpi=/usr/lib/openmpi/ \
            --with-unwind=/usr \
            --with-dyninst=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/usr/ --with-papi=/usr --with-binutils=/usr
make
make install
#+END_SRC

After the configure and before make, I had to comment the line on the
Makefile that contained -lsymLite, since I was unable to find such
library in my system (or any other solution in the internet). I have
mailed tools@bsc.es to get a proper solution.

* 2015-10-02 Checking if Extrae was correctly installed               :Lucas:

So, I have both Extrae and Dyninst installed.

These are the steps to see if it is working:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR="/home/schnorr/install/stow/"
rm -rf $STOW_DIR/*
mkdir -p $STOW_DIR
echo $STOW_DIR
stow /home/schnorr/install/dyninst-9.0.3/
stow /home/schnorr/install/extrae-3.1.0/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin/
extrae
#+END_SRC

I wasn't able to run the following commands from org-mode.

If it works, you should have something like this:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
extrae
#+END_SRC

You should see the message:

+ *Extrae: You have to provide a binary to instrument*

* 2015-10-02 Checking if Extrae is able to instrument                 :Lucas:

Environment variables:

#+BEGIN_SRC sh :results output :session myses
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
#+END_SRC

#+RESULTS:


#+BEGIN_SRC C :tangle mpi_init_finalize.c
#include <mpi.h>
int main (int argc, char **argv)
{
  MPI_Init(&argc, &argv);
  MPI_Finalize();
}
#+END_SRC

Compile:

#+BEGIN_SRC sh :results output :session myses
mpicc mpi_init_finalize.c
#+END_SRC

#+RESULTS:

Instrument:

#+BEGIN_SRC sh :results output :session myses
export EXTRAE_CONFIG_FILE=/home/schnorr/install/extrae-3.1.0/share/example/MPI/extrae.xml
extrae a.out
#+END_SRC

#+RESULTS:
: 
: Welcome to Extrae 3.1.0 revision 3316 based on extrae/trunk launcher using DynInst 9.0.3
: Extrae: Creating process for image binary a.out
: Extrae: Error creating the target application process

* 2015-10-15 New Paraver traces                                       :Tiago:
After installing EXTRAE, we were able to generate the traces of a few
simple MPI applications.  Those applications can be found in the
directory =applications=, and a script to generate their traces is also
on the directory.  You need to have EXTRAE installed, however.  The
traces generated will be helpful to validade the translation to tit.

* 2015-10-15 New approach                                             :Tiago:
We came to the conclusion that we can not generate the =.tit= entries
one by one while reading the Paraver trace file.  The new script
=prv2tit.pl= uses buffers to store the MPI calls that have missing
parameters.  Those parameters can be obtained in a trace entry that
can be 10 or 20 lines after the event.  The immediate send MPI call is
an example of this situation.  Anyway, the script is able to translate
all MPI calls supported by SIMGRID except for the V operations.  The V
operations require some extra functionality in the script that will be
soon implemented.

* 2015-10-15 How to run it?                                           :Tiago:
To convert a Paraver trace file to the time-independent trace format execute the following command:
#+BEGIN_SRC shell
$ perl prv2tit.pl -i <paraver_trace_file>
#+END_SRC

* 2015-10-16 Meeting with Tiago                                 :Lucas:Tiago:

+ The /MPI_Comm_size/ is a function that receives the size of the
  communicator. It is pretty easy to obtain, Tiago will do it soon.
  An MPI application might have multiple communicators, but luckly the
  paraver trace file keeps track of all this. So we basically need a
  hash mapping the communicator identifier to its size. When the event
  /MPI_Comm_size/ appears for a given process, we only have to lookup in
  that hash table.
+ All operations that receive as parameter the *size* (of something:
  receive or send sizes particular to each process) are going to be
  implemented by Tiago soon. They are not yet translated because they
  are slightly more complicated to get the size parameter for each
  processes. Tiago already has an idea on how to deal with that, he's
  going to implement that soon. These are the concerned functions:
  /MPI_Reduce_scatter/, /MPI_Alltoallv/, /MPI_Allgatherv/, /MPI_GatherV/.
+ All operations that have /comp_size/ are not yet completed (for
  example: allReduce and reduce). This comp size means "computation"
  since they run an operation for the reduce. We have to take into
  account the computation cost of doing so, but as of now, we have no
  idea on how to obtain such metric from the trace. We could take the
  time spent in the reduce operation, but that means it would also
  take the communication time. We intend to discuss this with Arnaud
  to get his opinion.
+ Micro-applications for validation: Tiago has already done four
  micro-applications (in the applications dir). He is going to extend
  that in order to check if every single MPI operation supported by
  tit is being correctly translated.
+ The next step is simulating the tit traces using SimGrid. It would
  be better to use the *git* version since any changes can be considered
  at the moment they are pushed to there. Here's the git you have to
  clone:

  git+ssh://schnorr@scm.gforge.inria.fr//gitroot//simgrid/simgrid.git

+ We have used smpi2pj.sh script, but reading of tit files generated
  by Tiago's script is not working. The current idea of why is that
  ranks are starting at 1, instead of zero. We have looked in Arnaud's
  script and he indeed subtract 1 from task identifiers. We have done
  so rapidly (see commit) and it works.

+ We convert the paraver to tit, we simulate with /smpi_replay/ and we
  got a paje trace file, and then we convert it to CSV using /pj_dump/
  from pajeng package [1]. With that, we have the "SimGrid size".  To
  have the "Dimemas size", we get the paraver, we feed to to dimemas,
  we get another paraver that reflects the simulation that has been
  conducted by dimemas (so timestamps are going to change), and then
  we have to convert this second simulated paraver trace file to Paje,
  and then use /pj_dump/ to CSV, to finally tackle the comparison
  between Simgrid and Dimemas.

  + "SimGrid size" is almost ready.
  + "Dimemas size" depends on paraver2paje (Tiago has to build a
    paraver to paje converter as well, which is going a copy of the
    prv2tit, but much more easy).

[1]: https://github.com/schnorr/pajeng/

* 2015-10-17 SMPI replay and MPI_Comm_* calls                         :Tiago:
The MPI calls =MPI_Comm_size=, =MPI_Comm_split= and =MPI_Comm_dup= are
supported by the smpi replay command. These calls, however, do not
affect the simulation process in any way. Therefore, we will be
ignoring the events in the trace file that contain these MPI calls.

* 2015-10-18 Support of "V" operations                                :Tiago:
In order to support the "v" operations we made a tweak in the script
so it stores the communicators information.  Each "v" operation has a
buffer for storing its events in the communicator data structure.  To
generate a tit entry for that v operation, the mpi call event from all
the tasks in the communicator must be read first.  Currently
=MPI_Gatherv=, =MPI_Allgatherv=, =MPI_Reduce_scatter= are supported.
=MPI_Alltoallv= has a slightly more different implementation and will
be supported soon.  The next step will be to create a few MPI
applications for testing the translation of this MPI calls.

* 2015-10-24 New MPI apps                                             :Tiago:
I created a few more MPI applications on the =applications/=
folder. They contain all MPI calls supported by SIMGRID. The shell
script in the applications folder was used to generate their Paraver
traces, which can be found on the =paraver_traces/= folder. The next
step is use the conversion script to make sure all MPI calls are being
converted correctly.

* 2015-10-25 Validation with the new apps traces                      :Tiago:
I tested the generated application traces with the conversion
scripts. Only a few minor adjustments had to be made (print the
correct task index for example).  All of the MPI calls are correctly
translated into its tit version with one exception.  The MPI call
=MPI_Alltoallv= requires a serie of parameters that apparently can not
be obtained from the Paraver trace.  The tit format of this call
requires:

+ the size of the send buffer
+ an array containing the size of the msg sent to each process in the communicator
+ the size of the receive buffer and 
+ an array with containing the size of the msg to be received from with each process in the communicator.

The Paraver trace, however, only contain the total size of the msg
sent of each process. Therefore, the script is currently not
supporting this call until we find a workaround for this problem.
* 2016-01-18 Dynamic instrumentation only with Dyninst 8.2.1          :Lucas:

Here's the e-mail message I got from German Llort (from BSC).

We've noticed in the output logs that you sent that you're using
Dyninst version 9.0.3.

The last version that we've tested of Dyninst is 8.2.1, and it is very
likely that with the upgrade to version 9.x there's been some major
changes that broke compatibility. While we run some tests with this
last version, I can suggest you two ways to proceed:

- You can try installing Dyninst 8.2.1, which should work.

- If you don't need to instrument user functions, you can trace your
  application with the alternate method based on LD_PRELOAD (this
  mechanism doesn't use Dyninst). You have examples of this mechanism
  in section 7.2 of the user guide, and under the
  "share/examples/MPI/ld-preload" directory Extrae's home directory.

* 2016-01-18 Extrae installation in another system (Sara)             :Lucas:

I skip dyninst installation since I don't think I need it now to trace MPI.

I have download the latest 3.2.1 version from extrae here:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

I have installed *libiberty-dev*.

Now I have issued the following commands:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.2.1.tar.bz2
cd extrae-3.2.1
#I need MPICC on Sara.
MPICC=/usr/bin/mpicc ./configure --prefix=/home/schnorr/install/extrae-3.2.1/ \
   --with-mpi=/usr/lib/openmpi/ \
   --without-unwind \
   --without-dyninst \
   --without-papi \
   --disable-openmp
make
make install
#+END_SRC

Configure, make and make install.
* 2016-01-18 Checking if extrae is able to instrument                 :Lucas:
TODO
* 2016-01-18 Problems of converted traces during replay               :Lucas:

I finally was able to run a replay from a tit file generated by the
conversion script. Here's how to do it. I'm supposing latest simgrid
from git was compiled and installed, all tools including =smpi_replay=
are in the PATH and can be correctly executed (=LD_LIBRARY_PATH= is also
configured).

#+begin_src sh :results output :session :exports both
export PATH=$PATH:$HOME/install/stow/bin/
export LD_LIBRARY_PATH=$HOME/install/stow/lib/
smpirun -keep-temps --log=replay.thresh:critical --log=smpi_replay.thresh:verbose --log=no_loc --cfg=smpi/running_power:1 --cfg=smpi/cpu_threshold:-1 -np 8 -platform griffon.xml -hostfile machine.txt smpi_replay paraver_traces/test.tit
#+end_src

#+RESULTS:

The smpirun command above seg faults, so running with keep-temps I can
run it manually with gdb. I was able to find out that the problem
comes from the gather event.

+ *Problems to be solved*:
  + =<comp_size>= should be replaced by 0 or the computational cost (if
    available, which is not the case for the paraver trace file)
  + =gather= event is not being correctly translated, that's the reason
    it segfaults. Documentation tells that we are not providing all
    parameters necessary for the gather. Here's where one of the
    problems appear:

    #+BEGIN_SRC text
    #4  0x00007ffff7a14188 in action_gather (action=action@entry=0x8280b0) at /home/schnorr/workspace/simgrid/src/smpi/smpi_replay.c:694
    694	    MPI_CURRENT_TYPE2=decode_datatype(action[6]);
    #+END_SRC

    From the documentation in =src/smpi/smpi_replay.c=:

    The structure of the gather action for the rank 0 (total 4
    processes) is the following:

    0 gather 68 68 0 0 0

    where: 
    1) 68 is the sendcounts
    2) 68 is the recvcounts
    3) 0 is the root node
    4) 0 is the send datatype id, see =decode_datatype()=
    5) 0 is the recv datatype id, see =decode_datatype()=

    Fred Suter told me there is something strange in the
    implementation of the gather replay. He just committed a solution
    that solves the problem very quickly.
* 2016-01-18 Current situation of translation                         :Lucas:

The following table is inspired from Table 1 of:
+ https://hal.inria.fr/hal-01064561/document

And also from the SimGrid code (=smpi_replay.c=) as of January 19th, 2016.


|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| *Name of the MPI action* | *TIT Entry*                                                                                                                                         | *Converted*                  |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Init=               | <r> init [default datatype]                                                                                                                       | ok (no datatype)           |
| =MPI_Finalize=           | <r> finalize (*not implemented in* =mpi_replay.c)=.                                                                                                   | ok                         |
| =MPI_Comm_size=          | <r> =comm_size= <double>                                                                                                                            |                            |
| =MPI_Comm_split=         | <r> =comm_split=                                                                                                                                    | ok                         |
| =MPI_Comm_dup=           | <r> =comm_dup=                                                                                                                                      | ok                         |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Send=               | <r> send <dst> <volume> [datatype]                                                                                                                | ok (no datatype)           |
| =MPI_Isend=              | <r> isend <dst> <volume> [datatype]                                                                                                               | ok (no datatype)           |
| =MPI_Recv=               | <r> recv <src> <volume> [datatype]                                                                                                                | ok (no datatype)           |
| =MPI_Irecv=              | <r> irecv <src> <volume> [datatype]                                                                                                               | ok (no datatype)           |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Test=               | <r> test                                                                                                                                          |                            |
| =MPI_Wait=               | <r> wait                                                                                                                                          | ok                         |
| =MPI_Waitall=            | <r> waitall                                                                                                                                       | ok                         |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Barrier=            | <r> barrier                                                                                                                                       | ok                         |
| =MPI_Bcast=              | <r> bcast <volume> [root] [datatype]                                                                                                              | ok (no datatype) / BUG     |
| =MPI_Reduce=             | <r> reduce <vcomm> <vcomp> [root] [datatype]                                                                                                      | ok (no datatype) / BUG     |
| =MPI_Allreduce=          | <r> allreduce <vcomm> <vcomp> [datatype]                                                                                                          | ok (no datatype)           |
| =MPI_Alltoall=           | <r> alltoall <sendvolume> <recvvolume> [datatype send] [datatype recv]                                                                            | ok (no datatype)           |
| =MPI_Alltoallv=          | <r> alltoallv <sendvolume> <sendcounts times communicator size> <recvvolume> <recvcounts times communicator size> [datatype send] [datatype recv] |                            |
| =MPI_Gather=             | <r> gather <sendvolume> <recvcounts> [send datatype] [recv datatype]                                                                              | ok (no datatypes) / BUG    |
| =MPI_Gatherv=            | <r> gatherv <sendvolume> <recvvolume times communicator size> <root> [send datatype] [recv datatype]                                              | ok (no datatypes)          |
| =MPI_Allgather=          | <r> allgather <sendvolume> <recvvolume> [send datatype] [recv datatype]                                                                           | ok (no datatypes)          |
| =MPI_Allgatherv=         | <r> allgatherv <sendvolume> <recvvolume times communicator size> [send datatype] [recv datatype]                                                  | ok (no datatypes)          |
| =MPI_Reduce_scatter=     | <r> reducescatter <recvvolume times communicator size> <vcomp> [datatype]                                                                         | ok (no datatypes)          |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| CPU burst              | <r> compute <volume>                                                                                                                              | ok (check =power_reference=) |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|

** Datatype translation

Check function =decode_datatype= at =smpi_replay.c=.

|----------------------+------------------|
| *Datatype translation* | *Kind*           |
|----------------------+------------------|
|                    0 | =MPI_DOUBLE=       |
|                    1 | =MPI_INT=          |
|                    2 | =MPI_CHAR=         |
|                    3 | =MPI_SHORT=        |
|                    4 | =MPI_LONG=         |
|                    5 | =MPI_FLOAT=        |
|                    6 | =MPI_BYTE=         |
|                    7 | =MPI_DEFAULT_TYPE= |
|----------------------+------------------|

** Action replay in Simgrid

#+BEGIN_SRC C
    xbt_replay_action_register("init",       action_init);
    xbt_replay_action_register("finalize",   action_finalize);
    xbt_replay_action_register("comm_size",  action_comm_size);
    xbt_replay_action_register("comm_split", action_comm_split);
    xbt_replay_action_register("comm_dup",   action_comm_dup);
    xbt_replay_action_register("send",       action_send);
    xbt_replay_action_register("Isend",      action_Isend);
    xbt_replay_action_register("recv",       action_recv);
    xbt_replay_action_register("Irecv",      action_Irecv);
    xbt_replay_action_register("test",       action_test);
    xbt_replay_action_register("wait",       action_wait);
    xbt_replay_action_register("waitAll",    action_waitall);
    xbt_replay_action_register("barrier",    action_barrier);
    xbt_replay_action_register("bcast",      action_bcast);
    xbt_replay_action_register("reduce",     action_reduce);
    xbt_replay_action_register("allReduce",  action_allReduce);
    xbt_replay_action_register("allToAll",   action_allToAll);
    xbt_replay_action_register("allToAllV",  action_allToAllv);
    xbt_replay_action_register("gather",  action_gather);
    xbt_replay_action_register("gatherV",  action_gatherv);
    xbt_replay_action_register("allGather",  action_allgather);
    xbt_replay_action_register("allGatherV",  action_allgatherv);
    xbt_replay_action_register("reduceScatter",  action_reducescatter);
    xbt_replay_action_register("compute",    action_compute);
#+END_SRC
** Comments about the implementation

+ =prv2tit.pl= / =power_reference= / =compute_action=
+ 
* 2016-02-15 DIMEMAS Installation                                     :Lucas:

Download DIMEMAS from

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Current version is:

+ dimemas-5.2.12.tar.gz

There are dependencies, check the README file and install them.

#+BEGIN_SRC sh
cd misc
tar xfz dimemas-5.2.12.tar.gz
cd dimemas-5.2.12
./configure --prefix=$HOME/install/dimemas-5.2.12/
make
make install
#+END_SRC

Here's the content after installation:

#+begin_src sh :results output :session :exports both
find /home/schnorr/install/dimemas-5.2.12/
#+END_SRC

#+RESULTS:
#+begin_example
/home/schnorr/install/dimemas-5.2.12/
/home/schnorr/install/dimemas-5.2.12/lib
/home/schnorr/install/dimemas-5.2.12/lib/GUI
/home/schnorr/install/dimemas-5.2.12/lib/GUI/dimemas-gui-5.2.12.jar
/home/schnorr/install/dimemas-5.2.12/lib/GUI/commons-io-2.4.jar
/home/schnorr/install/dimemas-5.2.12/sendrecv4.dim
/home/schnorr/install/dimemas-5.2.12/bin
/home/schnorr/install/dimemas-5.2.12/bin/trf2dim
/home/schnorr/install/dimemas-5.2.12/bin/DimemasGUI
/home/schnorr/install/dimemas-5.2.12/bin/DimemasUpdateCFG
/home/schnorr/install/dimemas-5.2.12/bin/Dimemas
/home/schnorr/install/dimemas-5.2.12/bin/prv2dim
/home/schnorr/install/dimemas-5.2.12/include
/home/schnorr/install/dimemas-5.2.12/include/extern_comm_model.h
/home/schnorr/install/dimemas-5.2.12/share
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/README
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/extern_comm_model.c
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/Makefile
/home/schnorr/install/dimemas-5.2.12/sendrecv4.pcf
/home/schnorr/install/dimemas-5.2.12/sendrecv4.row
#+end_example

We have a =Dimemas= binary and a =prv2dim= that interest us.

Converting a file from =prv= to =dim= file format.

#+begin_src sh :results output :session :exports both
cd ~/install/dimemas-5.2.12/
./bin/prv2dim ~/svn/bozzetti/paraver_traces2/sendrecv4.prv sendrecv4.dim
#+END_SRC

#+RESULTS:
#+begin_example
INITIALIZING PARSER... OK!
SPLITTING COMMUNICATIONS 000 %SPLITTING COMMUNICATIONS 002 %SPLITTING COMMUNICATIONS 003 %SPLITTING COMMUNICATIONS 004 %SPLITTING COMMUNICATIONS 005 %SPLITTING COMMUNICATIONS 006 %SPLITTING COMMUNICATIONS 007 %SPLITTING COMMUNICATIONS 008 %SPLITTING COMMUNICATIONS 009 %SPLITTING COMMUNICATIONS 010 %SPLITTING COMMUNICATIONS 011 %SPLITTING COMMUNICATIONS 012 %SPLITTING COMMUNICATIONS 013 %SPLITTING COMMUNICATIONS 014 %SPLITTING COMMUNICATIONS 015 %SPLITTING COMMUNICATIONS 016 %SPLITTING COMMUNICATIONS 017 %SPLITTING COMMUNICATIONS 018 %SPLITTING COMMUNICATIONS 019 %SPLITTING COMMUNICATIONS 020 %SPLITTING COMMUNICATIONS 021 %SPLITTING COMMUNICATIONS 022 %SPLITTING COMMUNICATIONS 023 %SPLITTING COMMUNICATIONS 024 %SPLITTING COMMUNICATIONS 025 %SPLITTING COMMUNICATIONS 026 %SPLITTING COMMUNICATIONS 027 %SPLITTING COMMUNICATIONS 028 %SPLITTING COMMUNICATIONS 029 %SPLITTING COMMUNICATIONS 030 %SPLITTING COMMUNICATIONS 031 %SPLITTING COMMUNICATIONS 032 %SPLITTING COMMUNICATIONS 033 %SPLITTING COMMUNICATIONS 034 %SPLITTING COMMUNICATIONS 035 %SPLITTING COMMUNICATIONS 036 %SPLITTING COMMUNICATIONS 037 %SPLITTING COMMUNICATIONS 038 %SPLITTING COMMUNICATIONS 039 %SPLITTING COMMUNICATIONS 040 %SPLITTING COMMUNICATIONS 041 %SPLITTING COMMUNICATIONS 042 %SPLITTING COMMUNICATIONS 043 %SPLITTING COMMUNICATIONS 044 %SPLITTING COMMUNICATIONS 045 %SPLITTING COMMUNICATIONS 046 %SPLITTING COMMUNICATIONS 047 %SPLITTING COMMUNICATIONS 048 %SPLITTING COMMUNICATIONS 049 %SPLITTING COMMUNICATIONS 050 %SPLITTING COMMUNICATIONS 051 %SPLITTING COMMUNICATIONS 052 %SPLITTING COMMUNICATIONS 053 %SPLITTING COMMUNICATIONS 054 %SPLITTING COMMUNICATIONS 055 %SPLITTING COMMUNICATIONS 056 %SPLITTING COMMUNICATIONS 057 %SPLITTING COMMUNICATIONS 058 %SPLITTING COMMUNICATIONS 059 %SPLITTING COMMUNICATIONS 060 %SPLITTING COMMUNICATIONS 061 %SPLITTING COMMUNICATIONS 062 %SPLITTING COMMUNICATIONS 063 %SPLITTING COMMUNICATIONS 064 %SPLITTING COMMUNICATIONS 065 %SPLITTING COMMUNICATIONS 066 %SPLITTING COMMUNICATIONS 067 %SPLITTING COMMUNICATIONS 068 %SPLITTING COMMUNICATIONS 069 %SPLITTING COMMUNICATIONS 070 %SPLITTING COMMUNICATIONS 071 %SPLITTING COMMUNICATIONS 072 %SPLITTING COMMUNICATIONS 073 %SPLITTING COMMUNICATIONS 074 %SPLITTING COMMUNICATIONS 075 %SPLITTING COMMUNICATIONS 076 %SPLITTING COMMUNICATIONS 077 %SPLITTING COMMUNICATIONS 078 %SPLITTING COMMUNICATIONS 079 %SPLITTING COMMUNICATIONS 080 %SPLITTING COMMUNICATIONS 081 %SPLITTING COMMUNICATIONS 082 %SPLITTING COMMUNICATIONS 083 %SPLITTING COMMUNICATIONS 084 %SPLITTING COMMUNICATIONS 085 %SPLITTING COMMUNICATIONS 086 %SPLITTING COMMUNICATIONS 087 %SPLITTING COMMUNICATIONS 088 %SPLITTING COMMUNICATIONS 089 %SPLITTING COMMUNICATIONS 090 %SPLITTING COMMUNICATIONS 091 %SPLITTING COMMUNICATIONS 092 %SPLITTING COMMUNICATIONS 093 %SPLITTING COMMUNICATIONS 094 %SPLITTING COMMUNICATIONS 095 %SPLITTING COMMUNICATIONS 096 %SPLITTING COMMUNICATIONS 097 %SPLITTING COMMUNICATIONS 098 %SPLITTING COMMUNICATIONS 099 %SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          210
   * Splitted communications 30
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK
CREATING TRANSLATION STRUCTURES  001/010CREATING TRANSLATION STRUCTURES  002/010CREATING TRANSLATION STRUCTURES  003/010CREATING TRANSLATION STRUCTURES  004/010CREATING TRANSLATION STRUCTURES  005/010CREATING TRANSLATION STRUCTURES  006/010CREATING TRANSLATION STRUCTURES  007/010CREATING TRANSLATION STRUCTURES  008/010CREATING TRANSLATION STRUCTURES  009/010CREATING TRANSLATION STRUCTURES  010/010CREATING TRANSLATION STRUCTURES  010/010
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION
TRANSLATING RECORDS 000 %TRANSLATING RECORDS 002 %TRANSLATING RECORDS 003 %TRANSLATING RECORDS 004 %TRANSLATING RECORDS 005 %TRANSLATING RECORDS 006 %TRANSLATING RECORDS 007 %TRANSLATING RECORDS 008 %TRANSLATING RECORDS 009 %TRANSLATING RECORDS 010 %TRANSLATING RECORDS 011 %TRANSLATING RECORDS 012 %TRANSLATING RECORDS 013 %TRANSLATING RECORDS 014 %TRANSLATING RECORDS 015 %TRANSLATING RECORDS 016 %TRANSLATING RECORDS 017 %TRANSLATING RECORDS 018 %TRANSLATING RECORDS 019 %TRANSLATING RECORDS 020 %TRANSLATING RECORDS 021 %TRANSLATING RECORDS 022 %TRANSLATING RECORDS 023 %TRANSLATING RECORDS 024 %TRANSLATING RECORDS 025 %TRANSLATING RECORDS 026 %TRANSLATING RECORDS 027 %TRANSLATING RECORDS 028 %TRANSLATING RECORDS 029 %TRANSLATING RECORDS 030 %TRANSLATING RECORDS 031 %TRANSLATING RECORDS 032 %TRANSLATING RECORDS 033 %TRANSLATING RECORDS 034 %TRANSLATING RECORDS 035 %TRANSLATING RECORDS 036 %TRANSLATING RECORDS 037 %TRANSLATING RECORDS 038 %TRANSLATING RECORDS 039 %TRANSLATING RECORDS 040 %TRANSLATING RECORDS 041 %TRANSLATING RECORDS 042 %TRANSLATING RECORDS 043 %TRANSLATING RECORDS 044 %TRANSLATING RECORDS 045 %TRANSLATING RECORDS 046 %TRANSLATING RECORDS 047 %TRANSLATING RECORDS 048 %TRANSLATING RECORDS 049 %TRANSLATING RECORDS 050 %TRANSLATING RECORDS 051 %TRANSLATING RECORDS 052 %TRANSLATING RECORDS 053 %TRANSLATING RECORDS 054 %TRANSLATING RECORDS 055 %TRANSLATING RECORDS 056 %TRANSLATING RECORDS 057 %TRANSLATING RECORDS 058 %TRANSLATING RECORDS 059 %TRANSLATING RECORDS 060 %TRANSLATING RECORDS 061 %TRANSLATING RECORDS 062 %TRANSLATING RECORDS 063 %TRANSLATING RECORDS 064 %TRANSLATING RECORDS 065 %TRANSLATING RECORDS 066 %TRANSLATING RECORDS 067 %TRANSLATING RECORDS 069 %TRANSLATING RECORDS 070 %TRANSLATING RECORDS 071 %TRANSLATING RECORDS 072 %TRANSLATING RECORDS 073 %TRANSLATING RECORDS 074 %TRANSLATING RECORDS 075 %TRANSLATING RECORDS 076 %TRANSLATING RECORDS 077 %TRANSLATING RECORDS 078 %TRANSLATING RECORDS 079 %TRANSLATING RECORDS 080 %TRANSLATING RECORDS 081 %TRANSLATING RECORDS 082 %TRANSLATING RECORDS 083 %TRANSLATING RECORDS 084 %TRANSLATING RECORDS 085 %TRANSLATING RECORDS 086 %TRANSLATING RECORDS 087 %TRANSLATING RECORDS 088 %TRANSLATING RECORDS 089 %TRANSLATING RECORDS 090 %TRANSLATING RECORDS 091 %TRANSLATING RECORDS 092 %TRANSLATING RECORDS 093 %TRANSLATING RECORDS 094 %TRANSLATING RECORDS 095 %TRANSLATING RECORDS 096 %TRANSLATING RECORDS 097 %TRANSLATING RECORDS 098 %TRANSLATING RECORDS 099 %TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES
   * Merging task    1 100 %   * Merging task    2 100 %   * Merging task    3 100 %   * Merging task    4 100 %   * Merging task    5 100 %   * Merging task    6 100 %   * Merging task    7 100 %   * Merging task    8 100 %   * Merging task    9 100 %   * Merging task   10 100 %   * All task merged!         

********************************************************************************
 *                               WARNING                                        *
********************************************************************************
5 tasks of this application execute 'non-deterministic' communications 
primitives (MPI_Test[*] | MPI_Waitany | MPI_Waitall | MPI_Waitsome)
The simulation of this trace will not capture the possible indeterminism 
********************************************************************************

TRANSLATION FINISHED
GENERATING PCF
   * Input PCF /home/schnorr/svn/bozzetti/paraver_traces2/sendrecv4.pcf correctly copied to sendrecv4.pcf
COPYING ROW FILE
#+end_example

Great, it works.

Dimemas CLI simulator:

#+begin_src sh :results output :session :exports both
cd ~/install/dimemas-5.2.12/
./bin/Dimemas -h
#+end_src

#+RESULTS:
#+begin_example
Dimemas version 5.2.12

Compiled on Mon Feb 15 07:27:46 BRST 2016 
Usage: ./bin/Dimemas [-h] [-v] [-d] [-x[s|e|p]] [-o[l] output-file] [-T time] [-l] [-C]
	[-p[a|b] paraver-file [-y time] -z time]] [-x[s|e]]
	[-e event_type] [-g event_output_info] [-F] [-S sync_size]
	[-w] [-ES] [-Eo eee_network_definition] [-Ef eee_frame_size]
	[--dim input-trace] [--bw bandwidth] [--lat latency]
	[--ppn processors_per_node] [--fill] [--interlvd]
	 config-file

Required arguments:
	config-file	Dimemas configuration file

Supported options:
	-h		Display this help message
	-v		Display Dimemas version information
	-d		Enable debug output
	-xa		Force assertations - Vladimir: check if optimizations caused some errors
	-xs		Enable extra scheduling debug output
	-xe		Enable extra event manager debug output
	-xp		Enable extra Paraver trace generation debug output
	-o[l] file	Set output file (default: stdout) (l:long info) 
	-t		Show only simulation time as output
	-T time		Set simulation stop time
	-l		Enable in-core operation
	-C		Perform critical path analysis
	-p file		Generate Paraver tracefile (ASCII)
	-pc file	Use the given file as output Paraver configuration file
	-y time		Set Paraver tracefile start time
	-z time		Set Paraver tracefile stop time
	-e event_type	Show time distance between events occurrence
	-g event_output	File for output information on events occurrence
	-F		Ignore synchronism send trace field
	-S sync_size	Minimum message size to use Rendez vous
	-w	When generating Paraver trace, output the LOGICAL_RECV times when Wait primitives take place (Default: at IRecv execution time)
	-ES	Enables the EEE network model (you must use '-Eo' and '-Ef'
	-Eo eee_network_definition	Set the filename where the EEE network is defined
	-Ef frame_size	Sets the EEE network frame size (in bytes)
	--dim input-trace	Set input trace (overrides the configuration file)
	--bw  bandwidth	Set inter-node bandwidth (MBps, overrides the configuration file)
	--lat latency	Set inter-node latency for all nodes (seconds, overrides the configuration file)
	--fill	Set node filling task mapping (overrides the configuration file)
	--ppn tasks_per_node	Set 'n' tasks per node mapping (overrides the configuration file)
	--interlvd	Set interleaved node tasks mapping (overrides the configuration file)
#+end_example

Now I have to find out how to use Dimemas.

Looks like a configuration file is essential.

There is some interesting tutorials here:
+ https://www.bsc.es/computer-sciences/performance-tools/documentation

More specifically these ones:
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/2.introduction_to_dimemas.tar.gz
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/3.introduction_to_paraver_and_dimemas_methodology.tar.gz
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/4.methodology_of_analysis.tar.gz

User-guide is here:
+ https://www.bsc.es/media/1324.pdf

Download all this:

#+begin_src sh :results output :session :exports both
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/2.introduction_to_dimemas.tar.gz
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/3.introduction_to_paraver_and_dimemas_methodology.tar.gz
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/4.methodology_of_analysis.tar.gz
wget -q https://www.bsc.es/media/1324.pdf
#+end_src

#+RESULTS:

* 2016-03-29 Plan for next weeks                                      :Lucas:

Here's what I think is missing from the technical side:

- A converter from paraver to =pj_dump=-like. I think Arnaud already did
  that, but we should merge what he did with the latest changes I have
  made in the =prv2tit.pl= script.
- Perhaps some performance improvements in the conversion script
  - Dump rightaway whenever information is complete, release memory
  - Today that's not the case, we buffer everything in memory (slow)
  - This is not strictly necessary (focus on next experimental part)

Them, for the experiments (from a non-partial paraver trace file):

1. _Replay with Dimemas_, get the trace from the replay (in prv format)
   - Understand how Dimemas model the Marenostrum machine
   - Get a complete trace from Barcelonians to start working with
   - Use =prv2pj.pl= script to convert paraver files to CSV (for comparison)
     #+begin_src sh :results output :session :exports both
     perl ./prv2pj.pl -f csv -i  ./paraver_traces2/sendrecv1
     #+end_src
2. _Replay with SimGrid_, using the tit file from original paraver
   - Create a Marenostrum platform file for Simgrid
   - Convert original paraver file to tit with =prv2tit.pl=
   - Get the trace, convert with =pj_dump= to the CSV-like
3. _Find a comparison metric_
   - It might be a simple gantt-chart with ggplot2
   - Then evolve proposing a metric (should discuss this with Arnaud)

You might find some information here.
- http://simgrid.gforge.inria.fr/contrib/smpi-paraver.php
* TODO 2016-03-29 Replaying with Dimemas to get a CSV for comparison

- How Dimemas CLI works?
- Receipt to replay with dimemas
  - With a step by step to get the CSV

* TODO 2016-03-29 Replaying with SimGrid to get a CSV for comparison

- Get inspiration from =smpi2pj.sh=
* 2016-03-29 Mercier's traces                                         :Lucas:

#+begin_src sh :results output :session :exports both
ls mercier_traces | grep .tar.gz
#+end_src

#+RESULTS:
: lu.B.2_prv_24MB.tar.gz
: lu.C.8_prv_377MB.tar.gz

Download here:

https://www.dropbox.com/sh/hnr7rhytapkuwtb/AACvpzo3MvHHm8LkAfklp61Ya?dl=0
* 2016-03-30 From the ground up

In order to install all the tools needed for this project,
we start by installing SimGrid.

#+BEGIN_SRC sh
cd simgrid
wget https://gforge.inria.fr/frs/download.php/file/35215/SimGrid-3.12.tar.gz
tar -xvzf SimGrid-3.12.tar.gz
cd SimGrid-3.12
cmake -Denable_smpi=on ./
make
make install
#+END_SRC
 
SimGrid is installed. We can try the smpi replay tool with one tit trace to check if the
simulation is running fine. The script that is executed separate the tit trace into files
for each task and creates a task mapping for an example machine model =graphene.xml=.

#+BEGIN_SRC sh
cd ..
cd examples
sh simulator.sh
#+END_SRC

This should output a file out.trace in the PAJE format.

Next we will install Dimemas. I created a directory called dimemas to contain the dimemas installation
and some examples. On that directory, copy the Dimemas source and execute the following commands.

#+BEGIN_SRC sh
tar jxf dimemas-5.2.12.tar.bz2
cd dimemas-5.2.12
./configure
make
make install
#+END_SRC

Now let`s try some examples.
First, we will convert a prv trace to the dimemas format.

#+BEGIN_SRC sh
cd ..
cd examples
./../dimemas-5.2.12/prv2dim/prv2dim sendrecv1.prv sendrecv1.dim
#+END_SRC

We should have a file named sendrecv1.dim with the trace in the dimemas format.
Now, we will simulate this trace on Dimemas using an example configuration file.

#+BEGIN_SRC sh
./../dimemas-5.2.12/Simulator/Dimemas -p out.prv 2dc_I_L2mr.cfg
#+END_SRC

Dimemas will create three files with the result of the simulation.

* 2014-04-03 Closing the cycle

So we got Simgrid and Dimemas working and we have some idea of how to use them.
It would be nice to have the simulation results in the same format for comparison.

Let`s start with Simgrid.
We will use the =pj_dump= command available in the PajeNG trace visualization tool.
We will start by installing this tool.

#+BEGIN_SRC sh
sudo apt-get install git cmake build-essential libqt4-dev libqt4-opengl-dev libboost-dev freeglut3-dev asciidoc flex bison;
git clone git://github.com/schnorr/pajeng.git ; mkdir -p pajeng/b ; cd pajeng/b ; cmake .. ; make install
#+END_SRC

Pjdump should be installed, let`s try it out.

#+BEGIN_SRC sh
cd ../../examples
sh simulator
pj_dump out.trace > out.csv
#+END_SRC

Great, we have our Simgrid output in a csv format.

We have to to the same with Dimemas output. First, we will install the prv2pjdump tool,
which will convert the prv file to a csv file in the pjdump format. Then, we will test it
with the prv file that the Dimemas simulator generated earlier.

#+BEGIN_SRC sh
cd ../../dimemas
git clone https://github.com/soctrace-inria/prv2pjdump.git
cd prv2pjdump
make
cd ../examples
./../prv2pjdump/linux_x64/prv2pjdump out.prv
#+END_SRC

The file out.pjdump should contain the result of our conversion.

* 2016-04-07 Trying to replicate latest entries                 :Lucas:Tiago:

_SimGrid_

#+begin_src sh :results output :session :exports both
cd simgrid; rm -rf SimGrid-3.12;
wget https://gforge.inria.fr/frs/download.php/file/35215/SimGrid-3.12.tar.gz
tar xzf SimGrid-3.12.tar.gz; cd SimGrid-3.12; mkdir build; cd build
cmake -Denable_smpi=ON -Denable_tracing=ON -Denable_java=OFF -Denable_documentation=OFF ..
make -j 4
#+end_src

#+RESULTS:
#+begin_example
-- Cmake version 3.4
-- The C compiler identification is GNU 5.3.1
-- The CXX compiler identification is GNU 5.3.1
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test HAVE_C_STACK_CLEANER
-- Performing Test HAVE_C_STACK_CLEANER - Failed
-- Performing Test COMPILER_SUPPORTS_CXX11
-- Performing Test COMPILER_SUPPORTS_CXX11 - Success
-- Performing Test COMPILER_SUPPORTS_C11
-- Performing Test COMPILER_SUPPORTS_C11 - Success
-- Found Perl: /usr/bin/perl (found version "5.22.1") 
-- Looking for sys/types.h
-- Looking for sys/types.h - found
-- Looking for stdint.h
-- Looking for stdint.h - found
-- Looking for stddef.h
-- Looking for stddef.h - found
-- Check size of int
-- Check size of int - done
-- Check size of long
-- Check size of long - done
-- Check size of long long
-- Check size of long long - done
-- Check size of void*
-- Check size of void* - done
-- System processor: x86_64 (x86_64, 64 bits)
-- Check if the system is big endian
-- Searching 16 bit integer
-- Check size of unsigned short
-- Check size of unsigned short - done
-- Using unsigned short
-- Check if the system is big endian - little endian
-- Looking for agraph.h
-- Looking for agraph.h - not found
-- Looking for cgraph.h
-- Looking for cgraph.h - found
-- Looking for graph.h
-- Looking for graph.h - not found
-- Looking for lib agraph
-- Looking for lib agraph - not found
-- Looking for lib cgraph
-- Looking for lib cgraph - found
-- Looking for lib graph
-- Looking for lib graph - not found
-- Looking for lib cdt
-- Looking for lib cdt - found
-- Looking for sigc++/sigc++.h
-- Looking for sigc++/sigc++.h - not found
-- Looking for sigc++config.h
-- Looking for sigc++config.h - not found
-- Looking for libsigc++
-- Looking for libsigc++ - not found
-- Boost version: 1.58.0
-- Boost version: 1.58.0
-- Found the following Boost libraries:
--   context
-- Looking for dlopen in dl
-- Looking for dlopen in dl - found
-- Looking for backtrace in execinfo
-- Looking for backtrace in execinfo - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Looking for sem_init in pthread
-- Looking for sem_init in pthread - found
-- Looking for sem_open in pthread
-- Looking for sem_open in pthread - found
-- Looking for sem_timedwait in pthread
-- Looking for sem_timedwait in pthread - found
-- Looking for pthread_mutex_timedlock in pthread
-- Looking for pthread_mutex_timedlock in pthread - found
-- Looking for clock_gettime in rt
-- Looking for clock_gettime in rt - found
-- Looking for 4 include files stdlib.h, ..., float.h
-- Looking for 4 include files stdlib.h, ..., float.h - found
-- Looking for valgrind/valgrind.h
-- Looking for valgrind/valgrind.h - found
-- Looking for socket.h
-- Looking for socket.h - not found
-- Looking for stat.h
-- Looking for stat.h - not found
-- Looking for sys/stat.h
-- Looking for sys/stat.h - found
-- Looking for windows.h
-- Looking for windows.h - not found
-- Looking for errno.h
-- Looking for errno.h - found
-- Looking for unistd.h
-- Looking for unistd.h - found
-- Looking for execinfo.h
-- Looking for execinfo.h - found
-- Looking for signal.h
-- Looking for signal.h - found
-- Looking for sys/time.h
-- Looking for sys/time.h - found
-- Looking for sys/param.h
-- Looking for sys/param.h - found
-- Looking for sys/sysctl.h
-- Looking for sys/sysctl.h - found
-- Looking for time.h
-- Looking for time.h - found
-- Looking for string.h
-- Looking for string.h - found
-- Looking for ucontext.h
-- Looking for ucontext.h - found
-- Looking for stdio.h
-- Looking for stdio.h - found
-- Looking for linux/futex.h
-- Looking for linux/futex.h - found
-- Looking for gettimeofday
-- Looking for gettimeofday - found
-- Looking for nanosleep
-- Looking for nanosleep - found
-- Looking for getdtablesize
-- Looking for getdtablesize - found
-- Looking for sysconf
-- Looking for sysconf - found
-- Looking for readv
-- Looking for readv - found
-- Looking for popen
-- Looking for popen - found
-- Looking for signal
-- Looking for signal - found
-- Looking for snprintf
-- Looking for snprintf - found
-- Looking for vsnprintf
-- Looking for vsnprintf - found
-- Looking for asprintf
-- Looking for asprintf - found
-- Looking for vasprintf
-- Looking for vasprintf - found
-- Looking for makecontext
-- Looking for makecontext - found
-- Looking for process_vm_readv
-- Looking for process_vm_readv - found
-- Looking for mmap
-- Looking for mmap - found
-- Looking for bin gfortran
-- Found gfortran: /usr/bin/gfortran
-- Looking for dlfcn.h
-- Looking for dlfcn.h - found
-- We are using GNU dynamic linker
-- sem_open is compilable
-- sem_open is executable
-- sem_init is compilable
-- sem_init is executable
-- timedwait is compilable
-- timedlock is compilable
-- #define pth_skaddr_makecontext(skaddr,sksize) ((skaddr))
-- #define pth_sksize_makecontext(skaddr,sksize) ((sksize))
-- Found Doxygen: /usr/bin/doxygen (found version "1.8.11") 
-- Doxygen version: 1.8.11
-- Configuring done
-- Generating done
-- Build files have been written to: /home/schnorr/svn/bozzetti/simgrid/SimGrid-3.12/build
#+end_example

_Dimemas_

- Download it manually (version 5.2.12) and put it in =dimemas= dir.

#+begin_src sh :results output :session :exports both
cd dimemas; rm -rf dimemas-5.2.12
tar xjf dimemas-5.2.12.tar.bz2
cd dimemas-5.2.12
./configure 2>&1 > x
head x
#+end_src

#+RESULTS:
#+begin_example
configure: error: invalid value: boost_major_version=
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking target system type... x86_64-unknown-linux-gnu
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether configure should try to set CFLAGS... yes
#+end_example

I have problem with boost, let's try to solve it.


* 2016-04-07 Let's take a look into the Dimemas and SimGrid traces :Lucas:Tiago:

We have two files that we want to analyze

#+begin_src sh :results output :session :exports both
find simgrid dimemas | grep pjdump$
#+end_src

#+RESULTS:
: simgrid/examples/out-simgrid.pjdump
: dimemas/examples/out-dimemas.pjdump

Let's prepare them to load into R.

#+begin_src sh :results output :session :exports both
cat simgrid/examples/out-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > simgrid/examples/out-simgrid-filter.pjdump
cat simgrid/examples/out-simgrid-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 3, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 3, 0.000051, 0.000117, 0.000066, action_recv
 3, 0.000126, 0.000128, 0.000002, smpi_replay_run_finalize
 2, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 2, 0.000052, 0.000052, 0.000000, action_send
 2, 0.000061, 0.000128, 0.000067, smpi_replay_run_finalize
 1, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 1, 0.000054, 0.000123, 0.000069, action_recv
 1, 0.000128, 0.000128, 0.000000, smpi_replay_run_finalize
 0, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 0, 0.000058, 0.000058, 0.000000, action_send
 0, 0.000063, 0.000128, 0.000065, smpi_replay_run_finalize
#+end_example

Let's look into the Dimemas converted trace file.

#+begin_src sh :results output :session :exports both
cat dimemas/examples/out-dimemas.pjdump | grep ^State | cut -d, -f2,4-6,8 | sed -e "s/THREAD 1\.//" -e "s/\.1,/,/" -e "s/\t//" -e "s/1R/R/" -e "s/4B/B/" > dimemas/examples/out-dimemas-filter.pjdump
cat dimemas/examples/out-dimemas-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
1,0,3,3,Running
2,0,3,3,Running
2,0,0,0,Running
3,0,3,3,Running
3,0,0,0,Running
4,0,3,3,Running
4,0,0,0,Running
1,3,4,1,Running
1,3,3,0,Running
1,3,3,0,Running
1,3,3,0,Running
2,3,3,0,Blocked
2,3,3,0,a message
2,3,4,1,Running
2,3,3,0,Running
2,3,3,0,Running
2,3,3,0,Running
2,3,3,0,Running
3,3,3,0,Blocked
3,3,4,1,Running
3,3,3,0,Running
3,3,3,0,Running
3,3,3,0,Running
3,3,3,0,Running
4,3,3,0,Blocked
4,3,3,0,a message
4,3,4,1,Running
4,3,3,0,Running
4,3,3,0,Running
4,3,3,0,Running
4,3,3,0,Running
1,4,5,1,created
1,4,4,0,Running
1,4,4,0,Running
1,4,4,0,Running
1,4,4,0,Running
2,4,5,1,created
2,4,4,0,Running
2,4,4,0,Running
2,4,4,0,Running
3,4,5,1,created
3,4,4,0,Running
3,4,4,0,Running
3,4,4,0,Running
4,4,5,1,created
4,4,4,0,Running
4,4,4,0,Running
4,4,4,0,Running
#+end_example

Great, let's load them into R.

#+begin_src R :results output :session :exports both
df_s <- read.csv("simgrid/examples/out-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";


df_d <- read.csv("dimemas/examples/out-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";


df <- rbind(df_s, df_d);
df
#+end_src

#+RESULTS:
#+begin_example
   Thread    Start      End Duration                     State Simulator
1       3 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
2       3 0.000051 0.000117  6.6e-05               action_recv   simgrid
3       3 0.000126 0.000128  2.0e-06  smpi_replay_run_finalize   simgrid
4       2 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
5       2 0.000052 0.000052  0.0e+00               action_send   simgrid
6       2 0.000061 0.000128  6.7e-05  smpi_replay_run_finalize   simgrid
7       1 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
8       1 0.000054 0.000123  6.9e-05               action_recv   simgrid
9       1 0.000128 0.000128  0.0e+00  smpi_replay_run_finalize   simgrid
10      0 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
11      0 0.000058 0.000058  0.0e+00               action_send   simgrid
12      0 0.000063 0.000128  6.5e-05  smpi_replay_run_finalize   simgrid
13      1 0.000000 3.000000  3.0e+00                   Running   dimemas
14      2 0.000000 3.000000  3.0e+00                   Running   dimemas
15      2 0.000000 0.000000  0.0e+00                   Running   dimemas
16      3 0.000000 3.000000  3.0e+00                   Running   dimemas
17      3 0.000000 0.000000  0.0e+00                   Running   dimemas
18      4 0.000000 3.000000  3.0e+00                   Running   dimemas
19      4 0.000000 0.000000  0.0e+00                   Running   dimemas
20      1 3.000000 4.000000  1.0e+00                   Running   dimemas
21      1 3.000000 3.000000  0.0e+00                   Running   dimemas
22      1 3.000000 3.000000  0.0e+00                   Running   dimemas
23      1 3.000000 3.000000  0.0e+00                   Running   dimemas
24      2 3.000000 3.000000  0.0e+00                   Blocked   dimemas
25      2 3.000000 3.000000  0.0e+00                 a message   dimemas
26      2 3.000000 4.000000  1.0e+00                   Running   dimemas
27      2 3.000000 3.000000  0.0e+00                   Running   dimemas
28      2 3.000000 3.000000  0.0e+00                   Running   dimemas
29      2 3.000000 3.000000  0.0e+00                   Running   dimemas
30      2 3.000000 3.000000  0.0e+00                   Running   dimemas
31      3 3.000000 3.000000  0.0e+00                   Blocked   dimemas
32      3 3.000000 4.000000  1.0e+00                   Running   dimemas
33      3 3.000000 3.000000  0.0e+00                   Running   dimemas
34      3 3.000000 3.000000  0.0e+00                   Running   dimemas
35      3 3.000000 3.000000  0.0e+00                   Running   dimemas
36      3 3.000000 3.000000  0.0e+00                   Running   dimemas
37      4 3.000000 3.000000  0.0e+00                   Blocked   dimemas
38      4 3.000000 3.000000  0.0e+00                 a message   dimemas
39      4 3.000000 4.000000  1.0e+00                   Running   dimemas
40      4 3.000000 3.000000  0.0e+00                   Running   dimemas
41      4 3.000000 3.000000  0.0e+00                   Running   dimemas
42      4 3.000000 3.000000  0.0e+00                   Running   dimemas
43      4 3.000000 3.000000  0.0e+00                   Running   dimemas
44      1 4.000000 5.000000  1.0e+00                   created   dimemas
45      1 4.000000 4.000000  0.0e+00                   Running   dimemas
46      1 4.000000 4.000000  0.0e+00                   Running   dimemas
47      1 4.000000 4.000000  0.0e+00                   Running   dimemas
48      1 4.000000 4.000000  0.0e+00                   Running   dimemas
49      2 4.000000 5.000000  1.0e+00                   created   dimemas
50      2 4.000000 4.000000  0.0e+00                   Running   dimemas
51      2 4.000000 4.000000  0.0e+00                   Running   dimemas
52      2 4.000000 4.000000  0.0e+00                   Running   dimemas
53      3 4.000000 5.000000  1.0e+00                   created   dimemas
54      3 4.000000 4.000000  0.0e+00                   Running   dimemas
55      3 4.000000 4.000000  0.0e+00                   Running   dimemas
56      3 4.000000 4.000000  0.0e+00                   Running   dimemas
57      4 4.000000 5.000000  1.0e+00                   created   dimemas
58      4 4.000000 4.000000  0.0e+00                   Running   dimemas
59      4 4.000000 4.000000  0.0e+00                   Running   dimemas
60      4 4.000000 4.000000  0.0e+00                   Running   dimemas
#+end_example

How to select part of this dataframe:

#+begin_src R :results output :session :exports both
df[df$Simulator == "simgrid",];
#+end_src

#+RESULTS:
#+begin_example
   Thread    Start      End Duration                     State Simulator
1       3 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
2       3 0.000051 0.000117  6.6e-05               action_recv   simgrid
3       3 0.000126 0.000128  2.0e-06  smpi_replay_run_finalize   simgrid
4       2 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
5       2 0.000052 0.000052  0.0e+00               action_send   simgrid
6       2 0.000061 0.000128  6.7e-05  smpi_replay_run_finalize   simgrid
7       1 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
8       1 0.000054 0.000123  6.9e-05               action_recv   simgrid
9       1 0.000128 0.000128  0.0e+00  smpi_replay_run_finalize   simgrid
10      0 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
11      0 0.000058 0.000058  0.0e+00               action_send   simgrid
12      0 0.000063 0.000128  6.5e-05  smpi_replay_run_finalize   simgrid
#+end_example


Let's plot this with a space/time view.

Learn =ggplot2=:
- http://ggplot2.org/

How to install gpplot2:

#+begin_src R :results output :session :exports both
install.packages("ggplot2");
#+end_src

Let's plot only Simgrid.

#+begin_src R :results output graphics :file results/old/img/sendrecv-simgrid-0.png :exports both :width 600 :height 200 :session
library(ggplot2);
ggplot(df[df$Simulator == "simgrid",], aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4);
#+end_src

#+RESULTS:
[[file:results/old/img/sendrecv-simgrid-0.png]]

First problem detected with Simgrid:
- =action_send= is too fast, should simulator with higher precision
  - See below how to do it

#+begin_src sh :results output :session :exports both
/home/schnorr/workspace/simgrid.git/b/examples/smpi/smpi_replay --help | grep precision
#+end_src

#+RESULTS:
:    maxmin/precision: Numerical precision used when computing resource sharing (hence this value is expressed in ops/sec or bytes/sec)
:    surf/precision: Numerical precision used when updating simulation times (hence this value is expressed in seconds)
:    tracing/precision: Numerical precision used when timestamping events (hence this value is expressed in number of digits after decimal point)

So, you should run =smpi_replay= with =--cfg=tracing/precision:9=.

Let's get back to the plot.

#+begin_src R :results output graphics :file results/old/img/sendrecv-simgrid-1.png :exports both :width 600 :height 400 :session
library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1, scale="free_x");
#+end_src

#+RESULTS:
[[file:results/old/img/sendrecv-simgrid-1.png]]

The first four states are from simgrid, others from dimemas
(paraver). This is a problem because it makes the comparison
harder. We should check =prv2tit.pl= to try to convert from paraver
(generated by Dimemas) to CSV exporting MPI states instead of simple
high-level states such as =Blocked=, =a message=, =created=, and =Running=. We

I suggest Tiago to create a copy of current =prv2tit.pl= and simplify it
in order to only convert MPI states (and Running) in another perl
script called =prv2pjdump.pl=.

At the end, Tiago will have three facets in a single plot for each application:
- Real (behavior of the original paraver trace file)
- Dimemas (behavior of the trace file after Dimemas simulation)
- SimGrid (behavior of the trace file after SimGrid simulation based on TIT files)
* 2016-04-09 Paraver to Pjdump conversion

It was discussed in the last meeting that we need to convert the prv traces into the pjdump format
in a way that we can compare the result of the simulation using Simgrid and using Dimemas.
I took a look at the pjdump files generated by the =pj_dump= command and the one created by the prv2pjdump
tool we had installed early. It turns out that they are very different (considering the same original prv trace).
We created a perl script that does the conversion of the prv trace to the pjdump format trying to get a
similar output compared to the =pj_dump= command.
The script is very similar to the script that converts prv to tit traces. It is much simpler actually.
Here is how the conversion is made:
1. A container is created for the application.
2. A container is created for each task.
3. Each communication entry generates a Link entry (using the containers of the sending and receiving tasks).
4. A state is created for each prv state. We use the state name as the type and value. If the state is a MPI operation,
we use the name of the MPI call as the value instead.
5. An event is created for each event in the prv trace that is a MPI operation.

That`s it. Let`s try to convert a prv traces and see what the output looks like.
#+begin_src sh :results output :exports both
perl prv2pjdump.pl -i paraver_traces/sendrecv1
#+end_src

#+RESULTS:
#+begin_example
Container, 0, APP, 0, 0.004025387, 0.004025387, 0
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-0
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-1
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-2
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-3
Link, 0, LINK, 0.00348182, 0.003484691, 0.000002871, LINK, rank-2, rank-3
Link, 0, LINK, 0.003693602, 0.00369535, 0.000001748, LINK, rank-0, rank-1
State, rank-0, RUNNING, 0, 0.00316019, 0.00316019, 0, RUNNING
State, rank-1, NOT CREATED, 0, 0.00001743, 0.00001743, 0, NOT CREATED
State, rank-2, NOT CREATED, 0, 0.000157444, 0.000157444, 0, NOT CREATED
State, rank-3, NOT CREATED, 0, 0.000219304, 0.000219304, 0, NOT CREATED
State, rank-1, RUNNING, 0.00001743, 0.003124832, 0.003107402, 0, RUNNING
State, rank-2, RUNNING, 0.000157444, 0.003125267, 0.002967823, 0, RUNNING
State, rank-3, RUNNING, 0.000219304, 0.003125908, 0.002906604, 0, RUNNING
State, rank-1, OTHERS, 0.003124832, 0.003170029, 0.000045197, 0, MPI_INIT
State, rank-2, OTHERS, 0.003125267, 0.003169742, 0.000044475, 0, MPI_INIT
State, rank-3, OTHERS, 0.003125908, 0.003170472, 0.000044564, 0, MPI_INIT
State, rank-0, OTHERS, 0.00316019, 0.003170485, 0.000010295, 0, MPI_INIT
State, rank-2, RUNNING, 0.003169742, 0.003226001, 0.000056259, 0, RUNNING
State, rank-1, RUNNING, 0.003170029, 0.003225703, 0.000055674, 0, RUNNING
State, rank-3, RUNNING, 0.003170472, 0.003226223, 0.000055751, 0, RUNNING
State, rank-0, RUNNING, 0.003170485, 0.003363346, 0.000192861, 0, RUNNING
State, rank-1, OTHERS, 0.003225703, 0.003389114, 0.000163411, 0, OTHERS
State, rank-2, OTHERS, 0.003226001, 0.003388687, 0.000162686, 0, OTHERS
State, rank-3, OTHERS, 0.003226223, 0.003388393, 0.00016217, 0, OTHERS
State, rank-0, OTHERS, 0.003363346, 0.003483197, 0.000119851, 0, OTHERS
State, rank-3, RUNNING, 0.003388393, 0.003395288, 0.000006895, 0, RUNNING
State, rank-2, RUNNING, 0.003388687, 0.003395362, 0.000006675, 0, RUNNING
State, rank-1, RUNNING, 0.003389114, 0.003395381, 0.000006267, 0, RUNNING
State, rank-3, OTHERS, 0.003395288, 0.003432306, 0.000037018, 0, OTHERS
State, rank-2, OTHERS, 0.003395362, 0.003432702, 0.00003734, 0, OTHERS
State, rank-1, OTHERS, 0.003395381, 0.003432431, 0.00003705, 0, OTHERS
State, rank-3, RUNNING, 0.003432306, 0.003437348, 0.000005042, 0, RUNNING
State, rank-1, RUNNING, 0.003432431, 0.003437863, 0.000005432, 0, RUNNING
State, rank-2, RUNNING, 0.003432702, 0.003437722, 0.00000502, 0, RUNNING
State, rank-3, WAITING A MESSAGE, 0.003437348, 0.003484691, 0.000047343, 0, MPI_RECV
State, rank-2, BLOCKING SEND, 0.003437722, 0.00348182, 0.000044098, 0, MPI_SEND
State, rank-1, WAITING A MESSAGE, 0.003437863, 0.00369535, 0.000257487, 0, MPI_RECV
State, rank-2, RUNNING, 0.00348182, 0.00351402, 0.0000322, 0, RUNNING
State, rank-0, RUNNING, 0.003483197, 0.003488339, 0.000005142, 0, RUNNING
State, rank-3, RUNNING, 0.003484691, 0.003515642, 0.000030951, 0, RUNNING
State, rank-0, OTHERS, 0.003488339, 0.003645036, 0.000156697, 0, OTHERS
State, rank-2, OTHERS, 0.00351402, 0.003550242, 0.000036222, 0, MPI_FINALIZE
State, rank-3, OTHERS, 0.003515642, 0.00355024, 0.000034598, 0, MPI_FINALIZE
State, rank-3, RUNNING, 0.00355024, 0.004005538, 0.000455298, 0, RUNNING
State, rank-2, RUNNING, 0.003550242, 0.004004832, 0.00045459, 0, RUNNING
State, rank-0, RUNNING, 0.003645036, 0.003651735, 0.000006699, 0, RUNNING
State, rank-0, BLOCKING SEND, 0.003651735, 0.003693602, 0.000041867, 0, MPI_SEND
State, rank-0, RUNNING, 0.003693602, 0.003850525, 0.000156923, 0, RUNNING
State, rank-1, RUNNING, 0.00369535, 0.003724869, 0.000029519, 0, RUNNING
State, rank-1, OTHERS, 0.003724869, 0.003761334, 0.000036465, 0, MPI_FINALIZE
State, rank-1, RUNNING, 0.003761334, 0.004005498, 0.000244164, 0, RUNNING
State, rank-0, OTHERS, 0.003850525, 0.003887338, 0.000036813, 0, MPI_FINALIZE
State, rank-0, RUNNING, 0.003887338, 0.004004403, 0.000117065, 0, RUNNING
State, rank-0, I/O, 0.004004403, 0.004014528, 0.000010125, 0, I/O
State, rank-2, I/O, 0.004004832, 0.004016896, 0.000012064, 0, I/O
State, rank-1, I/O, 0.004005498, 0.004016864, 0.000011366, 0, I/O
State, rank-3, I/O, 0.004005538, 0.004016591, 0.000011053, 0, I/O
State, rank-0, RUNNING, 0.004014528, 0.00402323, 0.000008702, 0, RUNNING
State, rank-3, RUNNING, 0.004016591, 0.004023898, 0.000007307, 0, RUNNING
State, rank-1, RUNNING, 0.004016864, 0.004025387, 0.000008523, 0, RUNNING
State, rank-2, RUNNING, 0.004016896, 0.004025282, 0.000008386, 0, RUNNING
Event, rank-1, MPI_CALL, 0.003124832, MPI_INIT
Event, rank-2, MPI_CALL, 0.003125267, MPI_INIT
Event, rank-3, MPI_CALL, 0.003125908, MPI_INIT
Event, rank-0, MPI_CALL, 0.00316019, MPI_INIT
Event, rank-3, MPI_CALL, 0.003437348, MPI_RECV
Event, rank-2, MPI_CALL, 0.003437722, MPI_SEND
Event, rank-1, MPI_CALL, 0.003437863, MPI_RECV
Event, rank-2, MPI_CALL, 0.00351402, MPI_FINALIZE
Event, rank-3, MPI_CALL, 0.003515642, MPI_FINALIZE
Event, rank-0, MPI_CALL, 0.003651735, MPI_SEND
Event, rank-1, MPI_CALL, 0.003724869, MPI_FINALIZE
Event, rank-0, MPI_CALL, 0.003850525, MPI_FINALIZE
#+end_example

Next step is to execute the cycle using this new script and see how we can compare
the traces.



* 2016-04-10 Simulation time precision

As discussed in the last meeting, the duration of the MPI_Send state on the Simgrid trace was zero.
Also, the Dimemas trace contained very small time values for some reason. This made the comparison
of the Simgrid trace with the Dimemas trace very hard.
I took a look at the Dimemas configuration file used to execute the simulations and noted
a parameter named =speed_ratio_instrumented_vs_simulated=. I changed it to 1, so the computing speed
of the simulated app will be the same as the original trace.

On Simgrid, I adjusted the options in order to generate a pjdump trace with the same precision as the
trace generated by Dimemas (ns precision). Using the ==tacing/precision== option with the value of 9
  and using pj_dump option =--float-precision=9= will generate a pjdump trace with ns precision.

  However, these adjustments did not solve the MPI_Send state problem on the Simgrid trace. After looking
  at the Simgrid help command, we set two more options: =smpi/send_is_detached_thres= and =smpi/async_small_thres=.
  The first option, according to the documentation, is the threshold of message size where MPI_Send stops behaving
  like MPI_Isend and becomes MPI_Send. Since our application was using very small message sizes, we set this
  option to 2.
  The second option, according to the documentation, is the maximal size of messages that are to be sent
  asynchronously, without waiting for the receiver. We set this value to 0.

  Additionally, we used the ==tracing/smpi/computing= on =smpi_replay= so the trace will also show the computing
  states.

  Let`s see what the trace looks like.

#+begin_src sh :results output :exports both
  cd simgrid/examples/
  sh simulator.sh
  pj_dump --float-precision=9 out.trace
#+end_src

#+RESULTS:
#+begin_example
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing' to 'yes'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/filename' to 'smpi_simgrid.trace'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/smpi' to 'yes'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/TCP_gamma' to '4194304'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/smpi/computing' to 'yes'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/precision' to '9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/send_is_detached_thres' to '2'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/async_small_thres' to '0'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/cpu_threshold' to '-1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/filename' to 'out.trace'
[griffon-1.nancy.grid5000.fr:0:(0) 3.647679] [smpi_replay/INFO] Simulation time 0.487489
Container, 0, 0, 0, 3.64768, 3.64768, 0
Link, 0, MPI_LINK, 3.035777000, 3.035873744, 0.000096744, PTP, rank-2, rank-3
Link, 0, MPI_LINK, 3.364892000, 3.364988744, 0.000096744, PTP, rank-0, rank-1
Container, 0, MPI, 0, 3.64768, 3.64768, rank-3
State, rank-3, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-3, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-3, MPI_STATE, 0.000000000, 2.906604000, 2.906604000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.906604000, 2.962355000, 0.055751000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.962355000, 2.969250000, 0.006895000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.969250000, 2.974292000, 0.005042000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.974292000, 3.035873744, 0.061581744, 1.000000000, action_recv
State, rank-3, MPI_STATE, 3.035873744, 3.066824744, 0.030951000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.066824744, 3.522122744, 0.455298000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.522122744, 3.529429744, 0.007307000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.529429744, 3.647678744, 0.118249000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 3.64768, 3.64768, rank-2
State, rank-2, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-2, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-2, MPI_STATE, 0.000000000, 2.967823000, 2.967823000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.967823000, 3.024082000, 0.056259000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.024082000, 3.030757000, 0.006675000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.030757000, 3.035777000, 0.005020000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.035777000, 3.035873744, 0.000096744, 1.000000000, action_send
State, rank-2, MPI_STATE, 3.035873744, 3.068073744, 0.032200000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.068073744, 3.522663744, 0.454590000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.522663744, 3.531049744, 0.008386000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.531049744, 3.647678744, 0.116629000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 3.64768, 3.64768, rank-1
State, rank-1, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-1, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-1, MPI_STATE, 0.000000000, 3.107402000, 3.107402000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.107402000, 3.163076000, 0.055674000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.163076000, 3.169343000, 0.006267000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.169343000, 3.174775000, 0.005432000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.174775000, 3.364988744, 0.190213744, 1.000000000, action_recv
State, rank-1, MPI_STATE, 3.364988744, 3.394507744, 0.029519000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.394507744, 3.638671744, 0.244164000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.638671744, 3.647194744, 0.008523000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.647194744, 3.647678744, 0.000484000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 3.64768, 3.64768, rank-0
State, rank-0, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-0, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-0, MPI_STATE, 0.000000000, 3.160190000, 3.160190000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.160190000, 3.353051000, 0.192861000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.353051000, 3.358193000, 0.005142000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.358193000, 3.364892000, 0.006699000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.364892000, 3.364988744, 0.000096744, 1.000000000, action_send
State, rank-0, MPI_STATE, 3.364988744, 3.521911744, 0.156923000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.521911744, 3.638976744, 0.117065000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.638976744, 3.647678744, 0.008702000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.647678744, 3.647678744, 0.000000000, 1.000000000, smpi_replay_run_finalize
#+end_example

  We can see that the MPI_Send states last longer that "zero" and we got a nanosecond precision in our
  time values. However, there is a little problem: The first state of each task (computing state) starts
  at time "zero" (which is ok) but ends at the highest possible time. All the other states occur while
  the first state is still valid. To properly plot the data, we will have to remove this first state
  for each task.

* 2016-04-10 Comparison between the traces

Once again, we will perform the complete cycle to obtain a =pjdump= file for the Simgrid trace,
the Dimemas trace and the original trace.
We will be using the =paraver_traces/sendrecv1.prv= trace. Let`s begin.

#+begin_src sh :results output :exports both
  cd simgrid/examples/
  sh simulator.sh
  pj_dump --float-precision=9 out.trace > ../../simgrid.pjdump
  cd ../../dimemas/examples/
  ./../dimemas-5.2.12/prv2dim/prv2dim sendrecv1.prv sendrecv1.dim
  ./../dimemas-5.2.12/Simulator/Dimemas -p out.prv 2dc_I_L2mr.cfg
  perl ../../prv2pjdump.pl -i out > ../../dimemas.pjdump
  cd ../../
  perl prv2pjdump.pl -i paraver_traces/sendrecv1 > original.pjdump
#+end_src

Ok. We got our traces.

#+begin_src sh :results output :exports both
  ls *.pjdump
#+end_src

#+RESULTS:
: dimemas.pjdump
: original.pjdump
: simgrid.pjdump

Let`s prepare it to load into R

#+begin_src sh :results output :exports both
cat simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > simgrid-filter.pjdump
cat simgrid-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 3, 0.000000000, 3.647678744, 3.647678744, computing
 3, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 3, 0.000000000, 2.906604000, 2.906604000, computing
 3, 2.906604000, 2.962355000, 0.055751000, computing
 3, 2.962355000, 2.969250000, 0.006895000, computing
 3, 2.969250000, 2.974292000, 0.005042000, computing
 3, 2.974292000, 3.035873744, 0.061581744, action_recv
 3, 3.035873744, 3.066824744, 0.030951000, computing
 3, 3.066824744, 3.522122744, 0.455298000, computing
 3, 3.522122744, 3.529429744, 0.007307000, computing
 3, 3.529429744, 3.647678744, 0.118249000, smpi_replay_run_finalize
 2, 0.000000000, 3.647678744, 3.647678744, computing
 2, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 2, 0.000000000, 2.967823000, 2.967823000, computing
 2, 2.967823000, 3.024082000, 0.056259000, computing
 2, 3.024082000, 3.030757000, 0.006675000, computing
 2, 3.030757000, 3.035777000, 0.005020000, computing
 2, 3.035777000, 3.035873744, 0.000096744, action_send
 2, 3.035873744, 3.068073744, 0.032200000, computing
 2, 3.068073744, 3.522663744, 0.454590000, computing
 2, 3.522663744, 3.531049744, 0.008386000, computing
 2, 3.531049744, 3.647678744, 0.116629000, smpi_replay_run_finalize
 1, 0.000000000, 3.647678744, 3.647678744, computing
 1, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 1, 0.000000000, 3.107402000, 3.107402000, computing
 1, 3.107402000, 3.163076000, 0.055674000, computing
 1, 3.163076000, 3.169343000, 0.006267000, computing
 1, 3.169343000, 3.174775000, 0.005432000, computing
 1, 3.174775000, 3.364988744, 0.190213744, action_recv
 1, 3.364988744, 3.394507744, 0.029519000, computing
 1, 3.394507744, 3.638671744, 0.244164000, computing
 1, 3.638671744, 3.647194744, 0.008523000, computing
 1, 3.647194744, 3.647678744, 0.000484000, smpi_replay_run_finalize
 0, 0.000000000, 3.647678744, 3.647678744, computing
 0, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 0, 0.000000000, 3.160190000, 3.160190000, computing
 0, 3.160190000, 3.353051000, 0.192861000, computing
 0, 3.353051000, 3.358193000, 0.005142000, computing
 0, 3.358193000, 3.364892000, 0.006699000, computing
 0, 3.364892000, 3.364988744, 0.000096744, action_send
 0, 3.364988744, 3.521911744, 0.156923000, computing
 0, 3.521911744, 3.638976744, 0.117065000, computing
 0, 3.638976744, 3.647678744, 0.008702000, computing
 0, 3.647678744, 3.647678744, 0.000000000, smpi_replay_run_finalize
#+end_example

As noted in the last entry on the Labbook, the first state of every task
should not be there. Let`s manually remove them. We hope to have an easier 
mechanism for doing this in the future.
This is what the file should look like:

 #+begin_src sh :results output :exports both
    cat simgrid-filter.pjdump
   #+end_src

 #+RESULTS:
 #+begin_example
  3, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  3, 0.000000000, 2.906604000, 2.906604000, computing
  3, 2.906604000, 2.962355000, 0.055751000, computing
  3, 2.962355000, 2.969250000, 0.006895000, computing
  3, 2.969250000, 2.974292000, 0.005042000, computing
  3, 2.974292000, 3.035873744, 0.061581744, action_recv
  3, 3.035873744, 3.066824744, 0.030951000, computing
  3, 3.066824744, 3.522122744, 0.455298000, computing
  3, 3.522122744, 3.529429744, 0.007307000, computing
  3, 3.529429744, 3.647678744, 0.118249000, smpi_replay_run_finalize
  2, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  2, 0.000000000, 2.967823000, 2.967823000, computing
  2, 2.967823000, 3.024082000, 0.056259000, computing
  2, 3.024082000, 3.030757000, 0.006675000, computing
  2, 3.030757000, 3.035777000, 0.005020000, computing
  2, 3.035777000, 3.035873744, 0.000096744, action_send
  2, 3.035873744, 3.068073744, 0.032200000, computing
  2, 3.068073744, 3.522663744, 0.454590000, computing
  2, 3.522663744, 3.531049744, 0.008386000, computing
  2, 3.531049744, 3.647678744, 0.116629000, smpi_replay_run_finalize
  1, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  1, 0.000000000, 3.107402000, 3.107402000, computing
  1, 3.107402000, 3.163076000, 0.055674000, computing
  1, 3.163076000, 3.169343000, 0.006267000, computing
  1, 3.169343000, 3.174775000, 0.005432000, computing
  1, 3.174775000, 3.364988744, 0.190213744, action_recv
  1, 3.364988744, 3.394507744, 0.029519000, computing
  1, 3.394507744, 3.638671744, 0.244164000, computing
  1, 3.638671744, 3.647194744, 0.008523000, computing
  1, 3.647194744, 3.647678744, 0.000484000, smpi_replay_run_finalize
  0, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  0, 0.000000000, 3.160190000, 3.160190000, computing
  0, 3.160190000, 3.353051000, 0.192861000, computing
  0, 3.353051000, 3.358193000, 0.005142000, computing
  0, 3.358193000, 3.364892000, 0.006699000, computing
  0, 3.364892000, 3.364988744, 0.000096744, action_send
  0, 3.364988744, 3.521911744, 0.156923000, computing
  0, 3.521911744, 3.638976744, 0.117065000, computing
  0, 3.638976744, 3.647678744, 0.008702000, computing
  0, 3.647678744, 3.647678744, 0.000000000, smpi_replay_run_finalize
#+end_example


We have to to the same with the Dimemas pjdump

#+begin_src sh :results output :exports both
    cat dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > dimemas-filter.pjdump
    cat dimemas-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 0, 0, 0.00316019, 0.00316019, MPI_INIT
 1, 0, 0.00001743, 0.00001743, RUNNING
 2, 0, 0.000157444, 0.000157444, RUNNING
 3, 0, 0.000219304, 0.000219304, RUNNING
 1, 0.00001743, 0.003124832, 0.003107402, RUNNING
 2, 0.000157444, 0.003125267, 0.002967823, RUNNING
 3, 0.000219304, 0.003125908, 0.002906604, RUNNING
 1, 0.003124832, 0.00316019, 0.000035358, MPI_INIT
 2, 0.003125267, 0.00316019, 0.000034923, MPI_INIT
 3, 0.003125908, 0.00316019, 0.000034282, MPI_INIT
 0, 0.00316019, 0.003353051, 0.000192861, RUNNING
 1, 0.00316019, 0.003215864, 0.000055674, RUNNING
 2, 0.00316019, 0.003216449, 0.000056259, RUNNING
 3, 0.00316019, 0.003215941, 0.000055751, RUNNING
 1, 0.003215864, 0.003222131, 0.000006267, RUNNING
 3, 0.003215941, 0.003222836, 0.000006895, RUNNING
 2, 0.003216449, 0.003223124, 0.000006675, RUNNING
 1, 0.003222131, 0.003227563, 0.000005432, RUNNING
 3, 0.003222836, 0.003227878, 0.000005042, RUNNING
 2, 0.003223124, 0.003228144, 0.00000502, MPI_SEND
 1, 0.003227563, 0.003364892, 0.000137329, MPI_RECV
 3, 0.003227878, 0.003228144, 0.000000266, MPI_RECV
 2, 0.003228144, 0.003260344, 0.0000322, MPI_FINALIZE
 3, 0.003228144, 0.003259095, 0.000030951, MPI_FINALIZE
 3, 0.003259095, 0.003714393, 0.000455298, RUNNING
 2, 0.003260344, 0.003714934, 0.00045459, RUNNING
 0, 0.003353051, 0.003358193, 0.000005142, RUNNING
 0, 0.003358193, 0.003364892, 0.000006699, MPI_SEND
 0, 0.003364892, 0.003521815, 0.000156923, MPI_FINALIZE
 1, 0.003364892, 0.003394411, 0.000029519, MPI_FINALIZE
 1, 0.003394411, 0.003638575, 0.000244164, RUNNING
 0, 0.003521815, 0.00363888, 0.000117065, RUNNING
 1, 0.003638575, 0.003649941, 0.000011366, RUNNING
 0, 0.00363888, 0.003649005, 0.000010125, RUNNING
 0, 0.003649005, 0.003657463, 0.000008458, RUNNING
 1, 0.003649941, 0.003658186, 0.000008245, RUNNING
 0, 0.003657463, 0.003657707, 0.000000244, RUNNING
 0, 0.003657707, 0.003657708, 0.000000001, NOT CREATED
 1, 0.003658186, 0.003658464, 0.000000278, RUNNING
 1, 0.003658464, 0.003658465, 0.000000001, NOT CREATED
 3, 0.003714393, 0.003725446, 0.000011053, RUNNING
 2, 0.003714934, 0.003726998, 0.000012064, RUNNING
 3, 0.003725446, 0.003732523, 0.000007077, RUNNING
 2, 0.003726998, 0.003735078, 0.00000808, RUNNING
 3, 0.003732523, 0.003732753, 0.00000023, RUNNING
 3, 0.003732753, 0.003732754, 0.000000001, NOT CREATED
 2, 0.003735078, 0.003735384, 0.000000306, RUNNING
 2, 0.003735384, 0.003735385, 0.000000001, NOT CREATED
#+end_example

Now, the original trace


#+begin_src sh :results output :exports both
    cat original.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > original-filter.pjdump
cat original-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 0, 0, 0.00316019, 0.00316019, RUNNING
 1, 0, 0.00001743, 0.00001743, NOT CREATED
 2, 0, 0.000157444, 0.000157444, NOT CREATED
 3, 0, 0.000219304, 0.000219304, NOT CREATED
 1, 0.00001743, 0.003124832, 0.003107402, RUNNING
 2, 0.000157444, 0.003125267, 0.002967823, RUNNING
 3, 0.000219304, 0.003125908, 0.002906604, RUNNING
 1, 0.003124832, 0.003170029, 0.000045197, MPI_INIT
 2, 0.003125267, 0.003169742, 0.000044475, MPI_INIT
 3, 0.003125908, 0.003170472, 0.000044564, MPI_INIT
 0, 0.00316019, 0.003170485, 0.000010295, MPI_INIT
 2, 0.003169742, 0.003226001, 0.000056259, RUNNING
 1, 0.003170029, 0.003225703, 0.000055674, RUNNING
 3, 0.003170472, 0.003226223, 0.000055751, RUNNING
 0, 0.003170485, 0.003363346, 0.000192861, RUNNING
 1, 0.003225703, 0.003389114, 0.000163411, OTHERS
 2, 0.003226001, 0.003388687, 0.000162686, OTHERS
 3, 0.003226223, 0.003388393, 0.00016217, OTHERS
 0, 0.003363346, 0.003483197, 0.000119851, OTHERS
 3, 0.003388393, 0.003395288, 0.000006895, RUNNING
 2, 0.003388687, 0.003395362, 0.000006675, RUNNING
 1, 0.003389114, 0.003395381, 0.000006267, RUNNING
 3, 0.003395288, 0.003432306, 0.000037018, OTHERS
 2, 0.003395362, 0.003432702, 0.00003734, OTHERS
 1, 0.003395381, 0.003432431, 0.00003705, OTHERS
 3, 0.003432306, 0.003437348, 0.000005042, RUNNING
 1, 0.003432431, 0.003437863, 0.000005432, RUNNING
 2, 0.003432702, 0.003437722, 0.00000502, RUNNING
 3, 0.003437348, 0.003484691, 0.000047343, MPI_RECV
 2, 0.003437722, 0.00348182, 0.000044098, MPI_SEND
 1, 0.003437863, 0.00369535, 0.000257487, MPI_RECV
 2, 0.00348182, 0.00351402, 0.0000322, RUNNING
 0, 0.003483197, 0.003488339, 0.000005142, RUNNING
 3, 0.003484691, 0.003515642, 0.000030951, RUNNING
 0, 0.003488339, 0.003645036, 0.000156697, OTHERS
 2, 0.00351402, 0.003550242, 0.000036222, MPI_FINALIZE
 3, 0.003515642, 0.00355024, 0.000034598, MPI_FINALIZE
 3, 0.00355024, 0.004005538, 0.000455298, RUNNING
 2, 0.003550242, 0.004004832, 0.00045459, RUNNING
 0, 0.003645036, 0.003651735, 0.000006699, RUNNING
 0, 0.003651735, 0.003693602, 0.000041867, MPI_SEND
 0, 0.003693602, 0.003850525, 0.000156923, RUNNING
 1, 0.00369535, 0.003724869, 0.000029519, RUNNING
 1, 0.003724869, 0.003761334, 0.000036465, MPI_FINALIZE
 1, 0.003761334, 0.004005498, 0.000244164, RUNNING
 0, 0.003850525, 0.003887338, 0.000036813, MPI_FINALIZE
 0, 0.003887338, 0.004004403, 0.000117065, RUNNING
 0, 0.004004403, 0.004014528, 0.000010125, I/O
 2, 0.004004832, 0.004016896, 0.000012064, I/O
 1, 0.004005498, 0.004016864, 0.000011366, I/O
 3, 0.004005538, 0.004016591, 0.000011053, I/O
 0, 0.004014528, 0.00402323, 0.000008702, RUNNING
 3, 0.004016591, 0.004023898, 0.000007307, RUNNING
 1, 0.004016864, 0.004025387, 0.000008523, RUNNING
 2, 0.004016896, 0.004025282, 0.000008386, RUNNING
#+end_example

Repeating the steps that we did before.. We will load this data into R.

#+begin_src R 
df_s <- read.csv("simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df_o <- read.csv("original-filter.pjdump", header=F, sep=",");
names(df_o) <- c("Thread", "Start", "End", "Duration", "State");
df_o$Simulator <- "original";

df <- rbind(df_s, df_d, df_o);
df
#+end_src

#+RESULTS:
| 3 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 3 |           0 |    2.906604 |    2.906604 | computing              | simgrid  |
| 3 |    2.906604 |    2.962355 |    0.055751 | computing              | simgrid  |
| 3 |    2.962355 |     2.96925 |    0.006895 | computing              | simgrid  |
| 3 |     2.96925 |    2.974292 |    0.005042 | computing              | simgrid  |
| 3 |    2.974292 | 3.035873744 | 0.061581744 | action_recv             | simgrid  |
| 3 | 3.035873744 | 3.066824744 |    0.030951 | computing              | simgrid  |
| 3 | 3.066824744 | 3.522122744 |    0.455298 | computing              | simgrid  |
| 3 | 3.522122744 | 3.529429744 |    0.007307 | computing              | simgrid  |
| 3 | 3.529429744 | 3.647678744 |    0.118249 | smpi_replay_run_finalize | simgrid  |
| 2 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 2 |           0 |    2.967823 |    2.967823 | computing              | simgrid  |
| 2 |    2.967823 |    3.024082 |    0.056259 | computing              | simgrid  |
| 2 |    3.024082 |    3.030757 |    0.006675 | computing              | simgrid  |
| 2 |    3.030757 |    3.035777 |     0.00502 | computing              | simgrid  |
| 2 |    3.035777 | 3.035873744 |  9.6744e-05 | action_send             | simgrid  |
| 2 | 3.035873744 | 3.068073744 |      0.0322 | computing              | simgrid  |
| 2 | 3.068073744 | 3.522663744 |     0.45459 | computing              | simgrid  |
| 2 | 3.522663744 | 3.531049744 |    0.008386 | computing              | simgrid  |
| 2 | 3.531049744 | 3.647678744 |    0.116629 | smpi_replay_run_finalize | simgrid  |
| 1 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 1 |           0 |    3.107402 |    3.107402 | computing              | simgrid  |
| 1 |    3.107402 |    3.163076 |    0.055674 | computing              | simgrid  |
| 1 |    3.163076 |    3.169343 |    0.006267 | computing              | simgrid  |
| 1 |    3.169343 |    3.174775 |    0.005432 | computing              | simgrid  |
| 1 |    3.174775 | 3.364988744 | 0.190213744 | action_recv             | simgrid  |
| 1 | 3.364988744 | 3.394507744 |    0.029519 | computing              | simgrid  |
| 1 | 3.394507744 | 3.638671744 |    0.244164 | computing              | simgrid  |
| 1 | 3.638671744 | 3.647194744 |    0.008523 | computing              | simgrid  |
| 1 | 3.647194744 | 3.647678744 |    0.000484 | smpi_replay_run_finalize | simgrid  |
| 0 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 0 |           0 |     3.16019 |     3.16019 | computing              | simgrid  |
| 0 |     3.16019 |    3.353051 |    0.192861 | computing              | simgrid  |
| 0 |    3.353051 |    3.358193 |    0.005142 | computing              | simgrid  |
| 0 |    3.358193 |    3.364892 |    0.006699 | computing              | simgrid  |
| 0 |    3.364892 | 3.364988744 |  9.6744e-05 | action_send             | simgrid  |
| 0 | 3.364988744 | 3.521911744 |    0.156923 | computing              | simgrid  |
| 0 | 3.521911744 | 3.638976744 |    0.117065 | computing              | simgrid  |
| 0 | 3.638976744 | 3.647678744 |    0.008702 | computing              | simgrid  |
| 0 | 3.647678744 | 3.647678744 |           0 | smpi_replay_run_finalize | simgrid  |
| 0 |           0 |  0.00316019 |  0.00316019 | MPI_INIT                | dimemas  |
| 1 |           0 |   1.743e-05 |   1.743e-05 | RUNNING                | dimemas  |
| 2 |           0 | 0.000157444 | 0.000157444 | RUNNING                | dimemas  |
| 3 |           0 | 0.000219304 | 0.000219304 | RUNNING                | dimemas  |
| 1 |   1.743e-05 | 0.003124832 | 0.003107402 | RUNNING                | dimemas  |
| 2 | 0.000157444 | 0.003125267 | 0.002967823 | RUNNING                | dimemas  |
| 3 | 0.000219304 | 0.003125908 | 0.002906604 | RUNNING                | dimemas  |
| 1 | 0.003124832 |  0.00316019 |  3.5358e-05 | MPI_INIT                | dimemas  |
| 2 | 0.003125267 |  0.00316019 |  3.4923e-05 | MPI_INIT                | dimemas  |
| 3 | 0.003125908 |  0.00316019 |  3.4282e-05 | MPI_INIT                | dimemas  |
| 0 |  0.00316019 | 0.003353051 | 0.000192861 | RUNNING                | dimemas  |
| 1 |  0.00316019 | 0.003215864 |  5.5674e-05 | RUNNING                | dimemas  |
| 2 |  0.00316019 | 0.003216449 |  5.6259e-05 | RUNNING                | dimemas  |
| 3 |  0.00316019 | 0.003215941 |  5.5751e-05 | RUNNING                | dimemas  |
| 1 | 0.003215864 | 0.003222131 |   6.267e-06 | RUNNING                | dimemas  |
| 3 | 0.003215941 | 0.003222836 |   6.895e-06 | RUNNING                | dimemas  |
| 2 | 0.003216449 | 0.003223124 |   6.675e-06 | RUNNING                | dimemas  |
| 1 | 0.003222131 | 0.003227563 |   5.432e-06 | RUNNING                | dimemas  |
| 3 | 0.003222836 | 0.003227878 |   5.042e-06 | RUNNING                | dimemas  |
| 2 | 0.003223124 | 0.003228144 |    5.02e-06 | MPI_SEND                | dimemas  |
| 1 | 0.003227563 | 0.003364892 | 0.000137329 | MPI_RECV                | dimemas  |
| 3 | 0.003227878 | 0.003228144 |    2.66e-07 | MPI_RECV                | dimemas  |
| 2 | 0.003228144 | 0.003260344 |    3.22e-05 | MPI_FINALIZE            | dimemas  |
| 3 | 0.003228144 | 0.003259095 |  3.0951e-05 | MPI_FINALIZE            | dimemas  |
| 3 | 0.003259095 | 0.003714393 | 0.000455298 | RUNNING                | dimemas  |
| 2 | 0.003260344 | 0.003714934 |  0.00045459 | RUNNING                | dimemas  |
| 0 | 0.003353051 | 0.003358193 |   5.142e-06 | RUNNING                | dimemas  |
| 0 | 0.003358193 | 0.003364892 |   6.699e-06 | MPI_SEND                | dimemas  |
| 0 | 0.003364892 | 0.003521815 | 0.000156923 | MPI_FINALIZE            | dimemas  |
| 1 | 0.003364892 | 0.003394411 |  2.9519e-05 | MPI_FINALIZE            | dimemas  |
| 1 | 0.003394411 | 0.003638575 | 0.000244164 | RUNNING                | dimemas  |
| 0 | 0.003521815 |  0.00363888 | 0.000117065 | RUNNING                | dimemas  |
| 1 | 0.003638575 | 0.003649941 |  1.1366e-05 | RUNNING                | dimemas  |
| 0 |  0.00363888 | 0.003649005 |  1.0125e-05 | RUNNING                | dimemas  |
| 0 | 0.003649005 | 0.003657463 |   8.458e-06 | RUNNING                | dimemas  |
| 1 | 0.003649941 | 0.003658186 |   8.245e-06 | RUNNING                | dimemas  |
| 0 | 0.003657463 | 0.003657707 |    2.44e-07 | RUNNING                | dimemas  |
| 0 | 0.003657707 | 0.003657708 |       1e-09 | NOT CREATED            | dimemas  |
| 1 | 0.003658186 | 0.003658464 |    2.78e-07 | RUNNING                | dimemas  |
| 1 | 0.003658464 | 0.003658465 |       1e-09 | NOT CREATED            | dimemas  |
| 3 | 0.003714393 | 0.003725446 |  1.1053e-05 | RUNNING                | dimemas  |
| 2 | 0.003714934 | 0.003726998 |  1.2064e-05 | RUNNING                | dimemas  |
| 3 | 0.003725446 | 0.003732523 |   7.077e-06 | RUNNING                | dimemas  |
| 2 | 0.003726998 | 0.003735078 |    8.08e-06 | RUNNING                | dimemas  |
| 3 | 0.003732523 | 0.003732753 |     2.3e-07 | RUNNING                | dimemas  |
| 3 | 0.003732753 | 0.003732754 |       1e-09 | NOT CREATED            | dimemas  |
| 2 | 0.003735078 | 0.003735384 |    3.06e-07 | RUNNING                | dimemas  |
| 2 | 0.003735384 | 0.003735385 |       1e-09 | NOT CREATED            | dimemas  |
| 0 |           0 |  0.00316019 |  0.00316019 | RUNNING                | original |
| 1 |           0 |   1.743e-05 |   1.743e-05 | NOT CREATED            | original |
| 2 |           0 | 0.000157444 | 0.000157444 | NOT CREATED            | original |
| 3 |           0 | 0.000219304 | 0.000219304 | NOT CREATED            | original |
| 1 |   1.743e-05 | 0.003124832 | 0.003107402 | RUNNING                | original |
| 2 | 0.000157444 | 0.003125267 | 0.002967823 | RUNNING                | original |
| 3 | 0.000219304 | 0.003125908 | 0.002906604 | RUNNING                | original |
| 1 | 0.003124832 | 0.003170029 |  4.5197e-05 | MPI_INIT                | original |
| 2 | 0.003125267 | 0.003169742 |  4.4475e-05 | MPI_INIT                | original |
| 3 | 0.003125908 | 0.003170472 |  4.4564e-05 | MPI_INIT                | original |
| 0 |  0.00316019 | 0.003170485 |  1.0295e-05 | MPI_INIT                | original |
| 2 | 0.003169742 | 0.003226001 |  5.6259e-05 | RUNNING                | original |
| 1 | 0.003170029 | 0.003225703 |  5.5674e-05 | RUNNING                | original |
| 3 | 0.003170472 | 0.003226223 |  5.5751e-05 | RUNNING                | original |
| 0 | 0.003170485 | 0.003363346 | 0.000192861 | RUNNING                | original |
| 1 | 0.003225703 | 0.003389114 | 0.000163411 | OTHERS                 | original |
| 2 | 0.003226001 | 0.003388687 | 0.000162686 | OTHERS                 | original |
| 3 | 0.003226223 | 0.003388393 |  0.00016217 | OTHERS                 | original |
| 0 | 0.003363346 | 0.003483197 | 0.000119851 | OTHERS                 | original |
| 3 | 0.003388393 | 0.003395288 |   6.895e-06 | RUNNING                | original |
| 2 | 0.003388687 | 0.003395362 |   6.675e-06 | RUNNING                | original |
| 1 | 0.003389114 | 0.003395381 |   6.267e-06 | RUNNING                | original |
| 3 | 0.003395288 | 0.003432306 |  3.7018e-05 | OTHERS                 | original |
| 2 | 0.003395362 | 0.003432702 |   3.734e-05 | OTHERS                 | original |
| 1 | 0.003395381 | 0.003432431 |   3.705e-05 | OTHERS                 | original |
| 3 | 0.003432306 | 0.003437348 |   5.042e-06 | RUNNING                | original |
| 1 | 0.003432431 | 0.003437863 |   5.432e-06 | RUNNING                | original |
| 2 | 0.003432702 | 0.003437722 |    5.02e-06 | RUNNING                | original |
| 3 | 0.003437348 | 0.003484691 |  4.7343e-05 | MPI_RECV                | original |
| 2 | 0.003437722 |  0.00348182 |  4.4098e-05 | MPI_SEND                | original |
| 1 | 0.003437863 |  0.00369535 | 0.000257487 | MPI_RECV                | original |
| 2 |  0.00348182 |  0.00351402 |    3.22e-05 | RUNNING                | original |
| 0 | 0.003483197 | 0.003488339 |   5.142e-06 | RUNNING                | original |
| 3 | 0.003484691 | 0.003515642 |  3.0951e-05 | RUNNING                | original |
| 0 | 0.003488339 | 0.003645036 | 0.000156697 | OTHERS                 | original |
| 2 |  0.00351402 | 0.003550242 |  3.6222e-05 | MPI_FINALIZE            | original |
| 3 | 0.003515642 |  0.00355024 |  3.4598e-05 | MPI_FINALIZE            | original |
| 3 |  0.00355024 | 0.004005538 | 0.000455298 | RUNNING                | original |
| 2 | 0.003550242 | 0.004004832 |  0.00045459 | RUNNING                | original |
| 0 | 0.003645036 | 0.003651735 |   6.699e-06 | RUNNING                | original |
| 0 | 0.003651735 | 0.003693602 |  4.1867e-05 | MPI_SEND                | original |
| 0 | 0.003693602 | 0.003850525 | 0.000156923 | RUNNING                | original |
| 1 |  0.00369535 | 0.003724869 |  2.9519e-05 | RUNNING                | original |
| 1 | 0.003724869 | 0.003761334 |  3.6465e-05 | MPI_FINALIZE            | original |
| 1 | 0.003761334 | 0.004005498 | 0.000244164 | RUNNING                | original |
| 0 | 0.003850525 | 0.003887338 |  3.6813e-05 | MPI_FINALIZE            | original |
| 0 | 0.003887338 | 0.004004403 | 0.000117065 | RUNNING                | original |
| 0 | 0.004004403 | 0.004014528 |  1.0125e-05 | I/O                    | original |
| 2 | 0.004004832 | 0.004016896 |  1.2064e-05 | I/O                    | original |
| 1 | 0.004005498 | 0.004016864 |  1.1366e-05 | I/O                    | original |
| 3 | 0.004005538 | 0.004016591 |  1.1053e-05 | I/O                    | original |
| 0 | 0.004014528 |  0.00402323 |   8.702e-06 | RUNNING                | original |
| 3 | 0.004016591 | 0.004023898 |   7.307e-06 | RUNNING                | original |
| 1 | 0.004016864 | 0.004025387 |   8.523e-06 | RUNNING                | original |
| 2 | 0.004016896 | 0.004025282 |   8.386e-06 | RUNNING                | original |

  
Now let's plot it.

#+begin_src R :results output graphics :file results/old/img/sendrecv-simgrid-2.png :exports both :width 600 :height 400 :session
library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1, scale="free_x");
#+end_src

#+RESULTS:
[[file:results/old/img/sendrecv-simgrid-2.png]]

 The first state in the Simgrid trace is MPI_init and the last one is MPI_Finalize for all tasks.
The tit trace had computing stages before the MPI_init and after the MPI_Finalize, i will take a look at that now.

* 2016-04-13 Next steps

After talking to professor Lucas, here are the next steps:
1. Repeat the comparison with the big traces we have access.
2. Build identical platform descriptions to be used as input to Simgrid and Dimemas.
3. Simulate the large traces with these identical platform descriptions and compare them.
4. Generate some new paraver traces of benchmarks using EXTRAE.

Step 2 is specially important since we want to compare traces using the same
machine model. In this case, we can disregard the original trace.
* 2016-04-14 Platform descriptions
As discussed earlier, it is important to have identical platform descriptions for
Simgrid and Dimemas for us to compare the traces.
First of all, I made an adjustment in the =prv2pjdump.pl= script. It no longer
replaces the state name with the mpi call that occurs during the state. The reason
for that is that some states may contain more than one mpi call.

The format of the platform description in Simgrid and Dimemas is very different. Parameters
with the same name do not necessarily mean the same thing or work the same way. After reading
the documentation for building a platform description in Simgrid and Dimemas, we made our first
attempt. We made a very simple platform and we will try to approach each parameters individually,
so we can have a better understaing of how it works.

The platform we will build contains 2 nodes, each node contains 2 cores. Each core has a
processing power of =P= (flops). The nodes can communicate with each other with latency =L=
(seconds) and a bandwidth of =B= (MBps). Also, each flow going through the link between the
nodes will be provided with the complete bandwidth =B= (to make it simple). Other parameters
will be discussed later. We will be calling this platform "small".

Here is how it looks in the Simgrid format:
#+begin_src sh :results output :exports both
  cat simgrid/examples/small.xml
#+end_src

#+RESULTS:
#+begin_example
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid.dtd">
<platform version="3">
<AS  id="AS_small"  routing="Full">
     <host id="host1" power="286.087kf" core="2"/>
     <host id="host2" power="286.087kf" core="2"/>
     <link id="link1" bandwidth="10000GBps" latency="0s" sharing_policy="FATPIPE"/>
     <route src="host1" dst="host2"><link_ctn id="link1"/></route>
</AS>
</platform>

#+end_example

Very short, very simple. The values used for bandwidth, latency and power can be ignored,
since we will be changing those for the following tests.

One important note is that the power in Simgrid is expressed as Flops, however, on Dimemas,
the simulated power is relative to the instrumented speed ratio (we say how much times
the simulation should be faster/slower than the original trace). We can easily calculate the
value the we should use in Dimemas by dividing the Simgrid simulation Flops with the
original Flops value.

t_simgrid = ops_original / Flops_simgrid

t_simgrid = (t_original * Flops_original) / Flops_simgrid

t_dimemas = t_original/c

We want t_simgrid = t_dimemas

(t_original * Flops_original) / Flops_simgrid = t_original/c

Flops_original / Flops_simgrid = 1/c

c = Flops_simgrid / Flops_original

Where, t_simgrid = the simulated time on Simgrid, t_dimemas = the simulated time on Dimemas,
c = the relative processor speed (the input we should use on Dimemas)

We made a simple MPI application trace to test the power parameter in
both platform descriptions.
Here is the application
#+begin_src sh :results output :exports both
  cat computation/computation.c
#+end_src

#+RESULTS:
#+begin_example
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <mpi.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, num_procs;
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if(num_procs < 2 || num_procs % 2 != 0) {
      MPI_Finalize();
      return 1;
    }

		int i;
		int computationSize = 1000000;
		float value = 1.12435433421;
     for (i = 0; i < computationSize; i++) {
			value = ((((value * 3.1415985934) / 2.42433453) + 0.00000324234) * 1.002134667345) / 1.00042542214;
		}

    MPI_Finalize();
}
#+end_example

The MPI app creates n tasks and each task has the same amount of
computation. We traced this application with EXTRAE and we executed
the cycle to obtain the simgrid trace and Dimemas trace in the pjdump
format using the platform description that we showed earlier with a
power =P= of 286.087KFlops, =L= of 0 seconds and =B= of 1000 GBps.
The app was traced with 4 tasks, task 0 and 1 were assigned to the
first node, task 2 and 3 to the second.
Here are the final traces for Simgrid and Dimemas:
#+begin_src sh :results output :exports both
  cat computation/computation-simgrid.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, 0, 0, 24.9563, 24.9563, 0
Container, 0, MPI, 0, 24.9563, 24.9563, rank-3
State, rank-3, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-3, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-3, MPI_STATE, 0.000000000, 2.879051000, 2.879051000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.879051000, 2.946167000, 0.067116000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.946167000, 2.952132000, 0.005965000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.952132000, 21.636174000, 18.684042000, 1.000000000, computing
State, rank-3, MPI_STATE, 21.636174000, 24.888875000, 3.252701000, 1.000000000, computing
State, rank-3, MPI_STATE, 24.888875000, 24.894258000, 0.005383000, 1.000000000, computing
State, rank-3, MPI_STATE, 24.894258000, 24.956309000, 0.062051000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 24.9563, 24.9563, rank-2
State, rank-2, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-2, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-2, MPI_STATE, 0.000000000, 2.808296000, 2.808296000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.808296000, 2.875053000, 0.066757000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.875053000, 2.881127000, 0.006074000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.881127000, 20.948396000, 18.067269000, 1.000000000, computing
State, rank-2, MPI_STATE, 20.948396000, 24.823476000, 3.875080000, 1.000000000, computing
State, rank-2, MPI_STATE, 24.823476000, 24.829214000, 0.005738000, 1.000000000, computing
State, rank-2, MPI_STATE, 24.829214000, 24.956309000, 0.127095000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 24.9563, 24.9563, rank-1
State, rank-1, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-1, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-1, MPI_STATE, 0.000000000, 2.557289000, 2.557289000, 1.000000000, computing
State, rank-1, MPI_STATE, 2.557289000, 2.625715000, 0.068426000, 1.000000000, computing
State, rank-1, MPI_STATE, 2.625715000, 2.632033000, 0.006318000, 1.000000000, computing
State, rank-1, MPI_STATE, 2.632033000, 24.347424000, 21.715391000, 1.000000000, computing
State, rank-1, MPI_STATE, 24.347424000, 24.564885000, 0.217461000, 1.000000000, computing
State, rank-1, MPI_STATE, 24.564885000, 24.571351000, 0.006466000, 1.000000000, computing
State, rank-1, MPI_STATE, 24.571351000, 24.956309000, 0.384958000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 24.9563, 24.9563, rank-0
State, rank-0, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-0, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-0, MPI_STATE, 0.000000000, 2.885332000, 2.885332000, 1.000000000, computing
State, rank-0, MPI_STATE, 2.885332000, 3.008576000, 0.123244000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.008576000, 3.014496000, 0.005920000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.014496000, 22.493663000, 19.479167000, 1.000000000, computing
State, rank-0, MPI_STATE, 22.493663000, 24.950293000, 2.456630000, 1.000000000, computing
State, rank-0, MPI_STATE, 24.950293000, 24.956309000, 0.006016000, 1.000000000, computing
State, rank-0, MPI_STATE, 24.956309000, 24.956309000, 0.000000000, 1.000000000, smpi_replay_run_finalize
#+end_example

#+begin_src sh :results output :exports both
  cat computation/computation-dimemas.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, APP, 0, 25.002551001, 25.002551001, 0
Container, 0, TASK, 0, 25.002551001, 25.002551001, 0
Container, 0, TASK, 0, 25.002551001, 25.002551001, 1
Container, 0, TASK, 0, 25.002551001, 25.002551001, 2
Container, 0, TASK, 0, 25.002551001, 25.002551001, 3
State, rank-0, RUNNING, 0, 0.034298, 0.034298, 0, RUNNING
State, rank-1, RUNNING, 0, 0.320785, 0.320785, 0, RUNNING
State, rank-2, RUNNING, 0, 0.069523, 0.069523, 0, RUNNING
State, rank-3, RUNNING, 0, 2.879051, 2.879051, 0, RUNNING
State, rank-0, RUNNING, 0.034298, 2.91963, 2.885332, 0, RUNNING
State, rank-2, RUNNING, 0.069523, 2.877819, 2.808296, 0, RUNNING
State, rank-1, RUNNING, 0.320785, 2.878074, 2.557289, 0, RUNNING
State, rank-2, BLOCKED, 2.877819, 2.91963, 0.041811, 0, BLOCKED
State, rank-1, BLOCKED, 2.878074, 2.91963, 0.041556, 0, BLOCKED
State, rank-3, BLOCKED, 2.879051, 2.91963, 0.040579, 0, BLOCKED
State, rank-0, RUNNING, 2.91963, 3.042874, 0.123244, 0, RUNNING
State, rank-1, RUNNING, 2.91963, 2.988056, 0.068426, 0, RUNNING
State, rank-2, RUNNING, 2.91963, 2.986387, 0.066757, 0, RUNNING
State, rank-3, RUNNING, 2.91963, 2.986746, 0.067116, 0, RUNNING
State, rank-2, RUNNING, 2.986387, 2.992461, 0.006074, 0, RUNNING
State, rank-3, RUNNING, 2.986746, 2.992711, 0.005965, 0, RUNNING
State, rank-1, RUNNING, 2.988056, 2.994374, 0.006318, 0, RUNNING
State, rank-2, RUNNING, 2.992461, 21.05973, 18.067269, 0, RUNNING
State, rank-3, RUNNING, 2.992711, 21.676753, 18.684042, 0, RUNNING
State, rank-1, RUNNING, 2.994374, 24.709765, 21.715391, 0, RUNNING
State, rank-0, RUNNING, 3.042874, 3.048794, 0.00592, 0, RUNNING
State, rank-0, RUNNING, 3.048794, 22.527961, 19.479167, 0, RUNNING
State, rank-2, RUNNING, 21.05973, 24.93481, 3.87508, 0, RUNNING
State, rank-3, RUNNING, 21.676753, 24.929454, 3.252701, 0, RUNNING
State, rank-0, RUNNING, 22.527961, 24.984591, 2.45663, 0, RUNNING
State, rank-1, RUNNING, 24.709765, 24.927226, 0.217461, 0, RUNNING
State, rank-1, RUNNING, 24.927226, 24.940695, 0.013469, 0, RUNNING
State, rank-3, RUNNING, 24.929454, 24.941998, 0.012544, 0, RUNNING
State, rank-2, RUNNING, 24.93481, 24.947975, 0.013165, 0, RUNNING
State, rank-1, RUNNING, 24.940695, 24.946954, 0.006259, 0, RUNNING
State, rank-3, RUNNING, 24.941998, 24.94715, 0.005152, 0, RUNNING
State, rank-1, RUNNING, 24.946954, 24.947161, 0.000207, 0, RUNNING
State, rank-3, RUNNING, 24.94715, 24.947381, 0.000231, 0, RUNNING
State, rank-1, NOT CREATED, 24.947161, 24.947161001, 0.000000001, 0, NOT CREATED
State, rank-3, NOT CREATED, 24.947381, 24.947381001, 0.000000001, 0, NOT CREATED
State, rank-2, RUNNING, 24.947975, 24.953528, 0.005553, 0, RUNNING
State, rank-2, RUNNING, 24.953528, 24.953713, 0.000185, 0, RUNNING
State, rank-2, NOT CREATED, 24.953713, 24.953713001, 0.000000001, 0, NOT CREATED
State, rank-0, RUNNING, 24.984591, 24.996535, 0.011944, 0, RUNNING
State, rank-0, RUNNING, 24.996535, 25.002277, 0.005742, 0, RUNNING
State, rank-0, RUNNING, 25.002277, 25.002551, 0.000274, 0, RUNNING
State, rank-0, NOT CREATED, 25.002551, 25.002551001, 0.000000001, 0, NOT CREATED
Event, 2, MPI_CALL, 2.877819, MPI_INIT
Event, 1, MPI_CALL, 2.878074, MPI_INIT
Event, 3, MPI_CALL, 2.879051, MPI_INIT
Event, 0, MPI_CALL, 2.91963, MPI_INIT
Event, 2, MPI_CALL, 21.05973, MPI_FINALIZE
Event, 3, MPI_CALL, 21.676753, MPI_FINALIZE
Event, 0, MPI_CALL, 22.527961, MPI_FINALIZE
Event, 1, MPI_CALL, 24.709765, MPI_FINALIZE
#+end_example

We can observe that both simulations end at approximately the same
time.

Let`s change the bandwidth in the platforms to 1MBps and test if the
output of the simulators will be different. For that, we will use
another MPI application:
#+begin_src sh :results output :exports both
  cat sendrecv/sendrecv.c
#+end_src

#+RESULTS:
#+begin_example
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <mpi.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, num_procs;
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if(num_procs < 2 || num_procs % 2 != 0) {
      MPI_Finalize();
      return 1;
    }

		int sending = 0;
		if (rank % 2 == 0) {
			sending = 1;
		}

    int i;
		int *msg;
		int messageSize = 10000000;
    msg = malloc(messageSize * sizeof(int));

		if (sending) {
     	MPI_Send(msg, messageSize, MPI_INT, rank + 1, 1234, MPI_COMM_WORLD);
     	printf("process %d sent message of size %d to process %d\n", rank, messageSize, rank + 1);
		}
		else {
     	MPI_Status status;
     	MPI_Recv(msg, messageSize, MPI_INT, rank - 1, 1234, MPI_COMM_WORLD, &status);
     	printf("process %d received a message of size %d from process %d\n", rank, messageSize, rank - 1);
		}

		free(msg);

    MPI_Finalize();
}
#+end_example
Each even MPI task send a message of 40MB to its odd neighbor.
We traced the application with EXTRAE and we executed
the cycle to obtain the simgrid trace and Dimemas trace in the pjdump
format. The receiving tasks were assigned to the node 1 and the
sending tasks were sent to the node 2.
Here are the final traces for Simgrid and Dimemas:
#+begin_src sh :results output :exports both
  cat sendrecv/sendrecv-simgrid.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, 0, 0, 52.2366, 52.2366, 0
Link, 0, MPI_LINK, 3.083936000, 45.663638737, 42.579702737, PTP, rank-0, rank-1
Link, 0, MPI_LINK, 3.046690000, 45.870591737, 42.823901737, PTP, rank-2, rank-3
Container, 0, MPI, 0, 52.2366, 52.2366, rank-3
State, rank-3, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-3, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-3, MPI_STATE, 0.000000000, 3.058568000, 3.058568000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.058568000, 3.336736000, 0.278168000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.336736000, 3.340047000, 0.003311000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.340047000, 3.348794000, 0.008747000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.348794000, 45.870591737, 42.521797737, 1.000000000, action_recv
State, rank-3, MPI_STATE, 45.870591737, 46.257293737, 0.386702000, 1.000000000, computing
State, rank-3, MPI_STATE, 46.257293737, 52.225005737, 5.967712000, 1.000000000, computing
State, rank-3, MPI_STATE, 52.225005737, 52.236646737, 0.011641000, 1.000000000, computing
State, rank-3, MPI_STATE, 52.236646737, 52.236646737, 0.000000000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 52.2366, 52.2366, rank-2
State, rank-2, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-2, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-2, MPI_STATE, 0.000000000, 2.953014000, 2.953014000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.953014000, 3.027902000, 0.074888000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.027902000, 3.033616000, 0.005714000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.033616000, 3.046690000, 0.013074000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.046690000, 45.870591737, 42.823901737, 1.000000000, action_send
State, rank-2, MPI_STATE, 45.870591737, 45.968918737, 0.098327000, 1.000000000, computing
State, rank-2, MPI_STATE, 45.968918737, 52.212202737, 6.243284000, 1.000000000, computing
State, rank-2, MPI_STATE, 52.212202737, 52.223213737, 0.011011000, 1.000000000, computing
State, rank-2, MPI_STATE, 52.223213737, 52.236646737, 0.013433000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 52.2366, 52.2366, rank-1
State, rank-1, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-1, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-1, MPI_STATE, 0.000000000, 3.050659000, 3.050659000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.050659000, 3.124837000, 0.074178000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.124837000, 3.129655000, 0.004818000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.129655000, 3.141841000, 0.012186000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.141841000, 45.663638737, 42.521797737, 1.000000000, action_recv
State, rank-1, MPI_STATE, 45.663638737, 46.003718737, 0.340080000, 1.000000000, computing
State, rank-1, MPI_STATE, 46.003718737, 46.232323737, 0.228605000, 1.000000000, computing
State, rank-1, MPI_STATE, 46.232323737, 46.240393737, 0.008070000, 1.000000000, computing
State, rank-1, MPI_STATE, 46.240393737, 52.236646737, 5.996253000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 52.2366, 52.2366, rank-0
State, rank-0, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-0, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-0, MPI_STATE, 0.000000000, 2.986926000, 2.986926000, 1.000000000, computing
State, rank-0, MPI_STATE, 2.986926000, 3.069610000, 0.082684000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.069610000, 3.074111000, 0.004501000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.074111000, 3.083936000, 0.009825000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.083936000, 45.663638737, 42.579702737, 1.000000000, action_send
State, rank-0, MPI_STATE, 45.663638737, 45.893635737, 0.229997000, 1.000000000, computing
State, rank-0, MPI_STATE, 45.893635737, 46.226374737, 0.332739000, 1.000000000, computing
State, rank-0, MPI_STATE, 46.226374737, 46.236190737, 0.009816000, 1.000000000, computing
State, rank-0, MPI_STATE, 46.236190737, 52.236646737, 6.000456000, 1.000000000, smpi_replay_run_finalize
#+end_example

#+begin_src sh :results output :exports both
  cat sendrecv/sendrecv-dimemas.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, APP, 0, 47.915151657, 47.915151657, 0
Container, 0, TASK, 0, 47.915151657, 47.915151657, 0
Container, 0, TASK, 0, 47.915151657, 47.915151657, 1
Container, 0, TASK, 0, 47.915151657, 47.915151657, 2
Container, 0, TASK, 0, 47.915151657, 47.915151657, 3
Link, 0, LINK, 3.387382, 41.534354656, 38.146972656, LINK, 2, 3
Link, 0, LINK, 3.194166, 41.341138656, 38.146972656, LINK, 0, 1
State, rank-0, RUNNING, 0, 0.11023, 0.11023, 0, RUNNING
State, rank-1, RUNNING, 0, 0.006585, 0.006585, 0, RUNNING
State, rank-2, RUNNING, 0, 0.105474, 0.105474, 0, RUNNING
State, rank-3, RUNNING, 0, 3.058568, 3.058568, 0, RUNNING
State, rank-1, RUNNING, 0.006585, 3.057244, 3.050659, 0, RUNNING
State, rank-2, RUNNING, 0.105474, 3.058488, 2.953014, 0, RUNNING
State, rank-0, RUNNING, 0.11023, 3.097156, 2.986926, 0, RUNNING
State, rank-1, BLOCKED, 3.057244, 3.097156, 0.039912, 0, BLOCKED
State, rank-2, BLOCKED, 3.058488, 3.097156, 0.038668, 0, BLOCKED
State, rank-3, BLOCKED, 3.058568, 3.097156, 0.038588, 0, BLOCKED
State, rank-0, RUNNING, 3.097156, 3.17984, 0.082684, 0, RUNNING
State, rank-1, RUNNING, 3.097156, 3.171334, 0.074178, 0, RUNNING
State, rank-2, RUNNING, 3.097156, 3.172044, 0.074888, 0, RUNNING
State, rank-3, RUNNING, 3.097156, 3.375324, 0.278168, 0, RUNNING
State, rank-1, RUNNING, 3.171334, 3.176152, 0.004818, 0, RUNNING
State, rank-2, RUNNING, 3.172044, 3.177758, 0.005714, 0, RUNNING
State, rank-1, RUNNING, 3.176152, 3.188338, 0.012186, 0, RUNNING
State, rank-2, RUNNING, 3.177758, 3.190832, 0.013074, 0, RUNNING
State, rank-0, RUNNING, 3.17984, 3.184341, 0.004501, 0, RUNNING
State, rank-0, RUNNING, 3.184341, 3.194166, 0.009825, 0, RUNNING
State, rank-1, WAITING A MESSAGE, 3.188338, 41.341138656, 38.152800656, 0, WAITING A MESSAGE
State, rank-2, WAITING A MESSAGE, 3.190832, 3.387382, 0.19655, 0, WAITING A MESSAGE
State, rank-0, RUNNING, 3.194166, 3.424163, 0.229997, 0, RUNNING
State, rank-3, RUNNING, 3.375324, 3.378635, 0.003311, 0, RUNNING
State, rank-3, RUNNING, 3.378635, 3.387382, 0.008747, 0, RUNNING
State, rank-2, RUNNING, 3.387382, 3.485709, 0.098327, 0, RUNNING
State, rank-3, WAITING A MESSAGE, 3.387382, 41.534354656, 38.146972656, 0, WAITING A MESSAGE
State, rank-0, RUNNING, 3.424163, 3.756902, 0.332739, 0, RUNNING
State, rank-2, RUNNING, 3.485709, 9.728993, 6.243284, 0, RUNNING
State, rank-0, RUNNING, 3.756902, 3.769042, 0.01214, 0, RUNNING
State, rank-0, RUNNING, 3.769042, 3.778641, 0.009599, 0, RUNNING
State, rank-0, RUNNING, 3.778641, 3.778858, 0.000217, 0, RUNNING
State, rank-0, NOT CREATED, 3.778858, 3.778858001, 0.000000001, 0, NOT CREATED
State, rank-2, RUNNING, 9.728993, 9.743934, 0.014941, 0, RUNNING
State, rank-2, RUNNING, 9.743934, 9.754673, 0.010739, 0, RUNNING
State, rank-2, RUNNING, 9.754673, 9.754945, 0.000272, 0, RUNNING
State, rank-2, NOT CREATED, 9.754945, 9.754945001, 0.000000001, 0, NOT CREATED
State, rank-1, RUNNING, 41.341138656, 41.681218656, 0.34008, 0, RUNNING
State, rank-3, RUNNING, 41.534354656, 41.921056656, 0.386702, 0, RUNNING
State, rank-1, RUNNING, 41.681218656, 41.909823656, 0.228605, 0, RUNNING
State, rank-1, RUNNING, 41.909823656, 41.922359656, 0.012536, 0, RUNNING
State, rank-3, RUNNING, 41.921056656, 47.888768656, 5.967712, 0, RUNNING
State, rank-1, RUNNING, 41.922359656, 41.930222656, 0.007863, 0, RUNNING
State, rank-1, RUNNING, 41.930222656, 41.930429656, 0.000207, 0, RUNNING
State, rank-1, NOT CREATED, 41.930429656, 41.930429657, 0.000000001, 0, NOT CREATED
State, rank-3, RUNNING, 47.888768656, 47.903510656, 0.014742, 0, RUNNING
State, rank-3, RUNNING, 47.903510656, 47.914924656, 0.011414, 0, RUNNING
State, rank-3, RUNNING, 47.914924656, 47.915151656, 0.000227, 0, RUNNING
State, rank-3, NOT CREATED, 47.915151656, 47.915151657, 0.000000001, 0, NOT CREATED
Event, 1, MPI_CALL, 3.057244, MPI_INIT
Event, 2, MPI_CALL, 3.058488, MPI_INIT
Event, 3, MPI_CALL, 3.058568, MPI_INIT
Event, 0, MPI_CALL, 3.097156, MPI_INIT
Event, 1, MPI_CALL, 3.188338, MPI_RECV
Event, 2, MPI_CALL, 3.190832, MPI_SEND
Event, 0, MPI_CALL, 3.194166, MPI_SEND
Event, 3, MPI_CALL, 3.387382, MPI_RECV
Event, 0, MPI_CALL, 3.424163, MPI_FINALIZE
Event, 2, MPI_CALL, 3.485709, MPI_FINALIZE
Event, 1, MPI_CALL, 41.681218656, MPI_FINALIZE
Event, 3, MPI_CALL, 41.921056656, MPI_FINALIZE
#+end_example

There are somethings we can note at this stage. The simulation on
Dimemas takes about 4 seconds less than the Simgrid simulation. Also,
os Dimemas, the processes that sent the message were not blocked until the
message was sent (and they finished much earlier).
Let's plot to have a better idea on what is going on.


#+begin_src sh :results output :exports both
cat sendrecv/sendrecv-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-7,8 | sed -n "/^[^,]*,[^,]*,[^,]*,[^,]*, 0/!p" | cut -d, -f1-4,6 > sendrecv/sendrecv-simgrid-filter.pjdump
cat sendrecv/sendrecv-dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > sendrecv/sendrecv-dimemas-filter.pjdump
#+end_src

#+begin_src R :results output graphics :file results/old/img/sendrecv-1.png :exports both :width 1500 :height 400 :session
df_s <- read.csv("sendrecv/sendrecv-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("sendrecv/sendrecv-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df <- rbind(df_s, df_d);
df

library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1);
#+end_src

#+RESULTS:
[[file:results/old/img/sendrecv-1.png]]

Some notes:
On the Dimemas side, the processes that sent the message are not
blocked and, therefore, they finish much earlier. Also, the waiting
time for the message on Dimemas is approximately 4 sencond lower than
the waiting time on Simgrid. 

Something interesting is that process 2 and 3 have a five second computing time after the message was
sent/received (this happens in both simulations). This delay was also
observed in the original trace.


* 2016-04-16 Platform description part 2

The last result on Dimemas showed that the processes that sent
the message were not blocked until the message was sent.
We created a variation on the previous application so that we have
late receivers. 

Let`s do it all over again.

#+begin_src sh :results output :exports both
cat sendrecv2/sendrecv2-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-7,8 | sed -n "/^[^,]*,[^,]*,[^,]*,[^,]*, 0/!p" | cut -d, -f1-4,6 > sendrecv2/sendrecv2-simgrid-filter.pjdump
cat sendrecv2/sendrecv2-dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > sendrecv2/sendrecv2-dimemas-filter.pjdump
#+end_src

#+begin_src R :results output graphics :file results/old/img/sendrecv-2.png :exports both :width 1500 :height 400 :session
df_s <- read.csv("sendrecv2/sendrecv2-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("sendrecv2/sendrecv2-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df <- rbind(df_s, df_d);
df

library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1);
#+end_src

#+RESULTS:
[[file:results/old/img/sendrecv-2.png]]

Now the sending processes are blocked until the receivers start to
receive the data. But they do not wait until the message is sent.

Let`s add a latency of 1s in the link between the nodes and set the
bandwidth to infinity to see how the latency affects the simulation.

#+begin_src sh :results output :exports both
cat latency/sendrecv-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-7,8 | sed -n "/^[^,]*,[^,]*,[^,]*,[^,]*, 0/!p" | cut -d, -f1-4,6 > latency/sendrecv-simgrid-filter.pjdump
cat latency/sendrecv-dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > latency/sendrecv-dimemas-filter.pjdump
#+end_src

#+begin_src R :results output graphics :file results/old/img/sendrecv-3.png :exports both :width 1500 :height 400 :session
df_s <- read.csv("latency/sendrecv-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("latency/sendrecv-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df <- rbind(df_s, df_d);
df

library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1);
#+end_src

#+RESULTS:
[[file:results/old/img/sendrecv-3.png]]

The result of the simulation from Simgrid and Dimemas is very very
different here. On Dimemas, before the actual communication, there is
a startup latency of 2s for each task and during the communication,
there is a delay of 1s. For Simgrid, however, it takes 30s for the
message to be sent. The network model being used for the simulators
are not the same, I will take a look at that.


* 2016-04-18 Script for executing the cycle
There is a new shell script that executes the complete cycle for a prv trace.
It converts a prv trace to the tit format, creates a platform description, a deployment
file and simulate the tit trace on Simgrid. It converts the result to the pjdump format
and filter it for plotting the data later. It also creates a configuration file for Dimemas
that models the same platform and simulate using Dimemas. It converts the resulting prv trace
to pjdump and filter it for plotting the data later.
The script also calls an R script that load the filtered data and plot a chart for us to
compare the result.
Before using it, we need to configure some option.
On the top of the file, you will find these parameters:

#+begin_src sh :results output :exports both
head -14 getpjdump/getpjdump.sh
#+end_src

#+RESULTS:
#+begin_example
#!/bin/bash

TRACE_NAME='sendrecv6'

# general options
NTASKS=4
BANDWIDTH=1000000 # in bytes/s
LATENCY=1 # in seconds
CORES=4
ORIGINAL_FLOPS=286087000.0 # in flops/s
SIMULATED_FLOPS=286087.0 # in flops/s
TASK_MAPPING=(0 1 0 1)
PLATFORM_FILE='small'

#+end_example

These are all the options available for the script:
=TRACE_NAME= is the original prv trace.
=NTASKS= is the number of tasks in the trace.
=BANDWIDTH= is the simulated bandwidth in bytes/s.
=LATENCY= is the simulated latency in seconds.
=CORES= is the number of cores in each node.
=ORIGINAL_FLOPS= is the original flops.
=SIMULATED_FLOPS= is the simulated flops of each core.
=PLATFORM_FILE= is the platform that will be used for the simulation (currently, there is only
one platform available and it is called small).
=TASK_MAPPING= maps the tasks to the nodes.

Set these parameters and run it: =./getpjdump.sh=

* 2016-04-20 Simgrid latency and bandwidth factors

In the last experiments, we were having different timings for all the
messages exchanges between the processes. It turns out that Simgrid
uses a factor to calculate the real bandwidth and latency used for a
certain size of message. These parameters are given by the option
=smpi/bw_factor= and =smpi/lat_factor=. Below, we are plotting the
simulation result using Dimemas and Simgrid of the same application
that was used in the previous experiment.  This time, using a
bandwidth of 1Mbps, a latency of 1s and a processing power of
286KFlops.

We will be using the script =getpjdump.sh= that executes the complete
cycle and plot the chart.  We configures the script with these values:

#+begin_src sh :results output :exports both
head -14 getpjdump/getpjdump.sh
#+end_src

#+RESULTS:
#+begin_example
#!/bin/bash

TRACE_NAME='sendrecv6'

# general options
NTASKS=4
BANDWIDTH=1000000 # in bytes/s
LATENCY=1 # in seconds
CORES=4
ORIGINAL_FLOPS=286087000.0 # in flops/s
SIMULATED_FLOPS=286087.0 # in flops/s
TASK_MAPPING=(0 1 0 1)
PLATFORM_FILE='small'

#+end_example

And we run it.

#+begin_src sh :results output :exports both
cd getpjdump
./getpjdump.sh
#+end_src

#+RESULTS:
#+begin_example
prv trace converted to the dim format
created dimemas config file
-> Simulator configuration to be read /tmp/tmp.Hkps6FjiQC/small-dimemas.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration

50.000000000: END SIMULATION

Output Paraver trace "sendrecv6-dimemas.prv" generated

dimemas simulation is done
got dimemas pjdump trace
prv trace converted to the tit format
created simgrid platform file
create simgrid deployment file
[rank 0] -> host0
[rank 1] -> host1
[rank 2] -> host0
[rank 3] -> host1
simgrid simulation is done
got simgrid pjdump trace
pjdumps filtered
#+end_example

Magnificent.
Here is the chart.

[[file:results/old/img/sendrecv-4.pdf]]

Note that the processes that received the message and the processes
the sent the message (on Simgrid only) are blocked for the same period
of time since we are now effectively using the same values for
bandwidth and latency.

* 2016-04-28 Meeting with Tiago                                 :Lucas:Tiago:

_Creating a Marenostrum Model_

A marenostrum model for Simgrid is needed for quite a while. See this:
- http://simgrid.gforge.inria.fr/contrib/smpi-paraver.php#orgheadline9

We'll ask Judit about this to see if she has one Dimemas model for
Marenostrum available. There is some documentation about it here:
http://www.jorditorres.org/wp-content/uploads/2014/02/16.CaseStudy.marenostrum.pdf

We took a look at this file:
- There are 3048 compute nodes
  - Each compute node is equiped with 2CPUs, 8 cores each (see slide 2)
- It is based on an Infiniband FDR10 non-blocking fat tree network
  - The three has two levels
    - First level is a _bunch of MLX FDR10 36-port_ (only 18 compute
      nodes are connected to one equipement like this)
    - Second level is _six MLX SX6535_ (522 ports - only 507 used on
      each)
    - The problem is understand exactly how the interconnection is
      made between the root level (the six MLX SX6535) and the
      switches connected to the computer nodes. It looks like one
      bottom network swith has 18 optical uplinks, 3 to each top
      switches in the root level.
  - See slide 13 for details

With this information in hand, we are already able to write a rough
sketch of SimGrid platform model for Marenostrum. Modeling Marenostrum
in Simgrid should be relatively easy to do. We shall pay attention to
SimGrid performance modeling all that.

- _Task #1_: Create a SimGrid model of marenostrum.
- _Task #2_: Ask Judit for a Dimemas model of marenostrum.

_Regarding small scale trace file comparison_

Tiago made some comparisons between SimGrid and Dimemas and observed
that sends are asynchronous no matter the message size in Dimemas
until the beginning of the corresponding receive. In SimGrid, they act
as expected being blocked, i.e., so the send keeps blocking the
process until the receive is done on the receiver side. Tiago tested
with messages up to 40 megabytes, but tests were carried out in a
multicore machine instead of a cluster. The comparisons only show
Dimemas and SimGrid, so one task is to represent also the real paraver
trace file, since we think that the behavior of Dimemas might be
influenced by something in the paraver trace file that we disregard
when we convert prv to tit to replay with smpi.

- _Task #3_: Always put the real execution trace represention when
  comparing Dimemas and SimGrid.

_Get a G5K account for Tiago_

We'll contact Arnaud to try to get an account in G5K so Tiago can
control experimental and obtain traces himself.

* 2016-04-30 Organizing the repository

It is important that all the experiments made here can be easily
reproducible.  The repository was a bit disorganized. Some of the
code/traces/results used for or obtained from the previous experiments
are no longer in the repository.

I will be moving all the resources and results of the experiments we
made so far to the =results/old/= folder. For the next experiments,
there will be a folder named with the date in which the experiment was
made under the =results= directory.  On the date folder, each
experiment will be assigned a number so we can uniquely identify
them. The idea is that all the experiments described here in the
Labbook will be reproducible with the files stored in that folder.

