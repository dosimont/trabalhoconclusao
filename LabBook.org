
#+TITLE: tcbozzetti's LabBook
#+AUTHOR: Tiago
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Tiago(T) Arnaud(A) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Issues with =.tit= <2015-09-10 Thu>                                                   :tit:
I was testing the conversion of the =lulesh2.0_p216_n108_t1.chop1= trace
and I came across the following problem: This code snippet opens a
file for each node.
#+BEGIN_SRC perl
my $nb_proc = 0;
foreach my $node (@{$$resource_name{NODE}}) { 
	my $filename = $output."_$nb_proc.tit";
	open($fh[$nb_proc], "> $filename") or die "Cannot open > $filename: $!";
	nb_proc++;
}
#+END_SRC
While this code snippet assumes the number of opened files is equal to the number of tasks.
#+BEGIN_SRC perl
$task = $task - 1;
defined($tit_translate{$sname}) or die "Unknown state '$sname' for tit\n";
if($tit_translate{$sname} ne "") {
	print { $fh[$task] } "$task $tit_translate{$sname} $sname_param\n",
}
#+END_SRC
Currrently, I am assuming that there must be one =.tit= file for each
node, therefore, I modified the last snippet to write on the task's
node =.tit= file.

* Some notes <2015-09-13 Sun>                                                         :notes:
Understanding the performance of parallel applications is a concern in
the HPC community.  When considering platforms that are not available,
it is necessarty to simulate the application. Therefore, it is
possible to determine a cost-effective hardware configuration for that
particular application.  When considering a platform that is
available, simulating is also important since the access to
large-scale platforms can be costly.

Two frameworks for simulating MPI applications:
1. On-line simulations
	* The same amount of hardware is required to simulate.
	* Usually uses a simple network model to calculate the
          communication delays.
	* Can simulate the computational delays, but data-dependent
          application behavior is lost.
2. Off-line simulations
	* Use a trace of the parallel application.
	* Can be performed on a single computer.
	* Computational delays are scaled based on the performance
          differential between the original and the simulated
          platform.
		* If simulator uses time information, target and
                  original platform must be the same.
		* If trace is time independent, the only condition is
                  that the processors of the target platforms must be
                  of the same family as the original platform.
	* Communication delays are computed based on a network
          simulator.
	* Partially addresses the simulation of data-dependent
          application. Some data-dependent behavior can be captured in
          the trace and simulated.

Off-line simulators differ by the simulation models they employ to
compute simulated durations of CPU bursts and communication
operations.

On a time-independent trace, the CPU bursts or communication
operations are logged with its volume (in number of executed
instructions or in number of transferred bytes) instead of the time
when it begins and ends or its duration.  Therefore, the trace can not
be associated with a platform anymore (with the exception of the
processor family).  This imply that the MPI application can not modify
its execution according to the execution platform (AMPI applications).

*For large numbers of processes and/or numbers of actions, it may be
preferable to split the trace so as to obtain one trace file per
process.*

*Table: Time-independent actions corresponding to supported MPI communication operations.*
| MPI actions        | Trace entry                                  |
|--------------------+----------------------------------------------|
| =CPU burst=          | =<rank> compute <volume>=                      |
| =MPI_Send=           | =<rank> send <dst_rank> <volume>=              |
| =MPI_Isend=          | =<rank> Isend <dst_rank> <volume>=             |
| =MPI_Recv=           | =<rank> recv <src_rank> <volume>=              |
| =MPI_Irecv=          | =<rank> Irecv <src_rank> <volume>=             |
| =MPI_Broadcast=      | =<rank> bcast <volume>=                        |
| =MPI_Reduce=         | =<rank> reduce <vcomm> <vcomp>=                |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_counts> <vcomp>=   |
| =MPI_Allreduce=      | =<rank> allReduce <vcomm> <vcomp>=             |
| =MPI_Alltoall=       | =<rank> allToAll <send_volume> <recv_volume>=  |
| =MPI_Alltoallv=      | =<rank> allToAllv <send_volume> <send_counts>= |
|                    | =<recv_volume> <recv_counts>=                  |
| =MPI_Gather=         | =<rank> gather <send_volume> <recv_volume>=    |
| =MPI_Allgatherv=     | =<rank> allGatherV <send_count> <recv_counts>= |
| =MPI_Barrier=        | =<rank> barrier=                               |
| =MPI_Wait=           | =<rank> wait=                                  |
| =MPI_Waitall=        | =<rank> waitAll=                               |
Source: references/ref1.pdf
* Starting LabBook <2015-09-16 Wed> :journal:
Lucas setup the org mode file for the journal.
** Copied the entries from the README file
I have been looking at some org-mode documentation to make this
journal better. It turns out they have a ton of features and I plan
to make good use of them to make this project more organized.
* Testing the off-line simulation on SimGrid <2015-09-17 Thu> :tit:simgrid:
I was trying to simulate the time-independent trace files generated by
the =EXTRAE_Paraver_trace_mpich= trace. There was a problem, however.
More specifically, the issue was the ranks of the processes on the
=.tit= files. The little piece of code that I modified last thursday
assumed that the number of =.tit= files were equivalent to the number
of nodes, however that was only executed for state entries in the
trace. When an event was processed, the old code was executed and
therefore, the resulting =.tit= was inconsistent. I fixed this by
also changing the code that handles the events. Another problem is
that the ranks of the processes on the =.tit= files must start with
zero (I am not completely sure about this). If I set the ranks of
the processes to start with one, there is an exception during the
simulation. When I use the option =-map= on the =smpirun= command I get
the following (even when there is no rank zero on the =.tit= files):
#+BEGIN_SRC shell
[rank 0] -> graphene-1.nancy.grid5000.fr
[rank 1] -> graphene-2.nancy.grid5000.fr
[rank 2] -> graphene-3.nancy.grid5000.fr
[rank 3] -> graphene-4.nancy.grid5000.fr
[rank 4] -> graphene-5.nancy.grid5000.fr
[rank 5] -> graphene-6.nancy.grid5000.fr
[rank 6] -> graphene-7.nancy.grid5000.fr
[rank 7] -> graphene-8.nancy.grid5000.fr
#+END_SRC
This led me to believe that the ranks of the processes must start with
zero. This fact can cause some headache since the Paraver trace files
do not assume that they should start with zero. During the conversion
to the time-independent trace format, we simply compute the rank that
will be written on the =.tit= file by subtracting one. However, this
solution will not work for every case. Imagine if the Paraver trace uses
the rank zero...
* Compilation of all smpi replay operations <2015-09-17 Thu> :trace_replay:simgrid:tit:
I took a look at the source code of the =smpi_replay= (the
=references/ref1.pdf= does not contain all operations and all possible
arguments) and compiled all the smpi trace replay operations and its
arguments. I expect that this can be useful in the future.

| MPI actions        | Trace entry                                     |
|--------------------+-------------------------------------------------|
| =CPU burst=          | =<rank> compute <flops>=                          |
| =MPI_Send=           | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Isend=          | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Recv=           | =<rank> recv <src> <comm_size> [<datatype>]=      |
| =MPI_Irecv=          | =<rank> Irecv <src> <comm_size> [<datatype>]=     |
| =MPI_Broadcast=      | =<rank> bcast <comm_size> [<root> [<datatype>]]=  |
| =MPI_Reduce=         | =<rank> reduce <comm_size> <comp_size>=           |
|                    | =[<root> [<datatype>]]=                           |
| =MPI_AllReduce=      | =<rank> allReduce <comm_size> <comp_size>=        |
|                    | =[<datatype>]=                                    |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_sizes†> <comp_size>=  |
|                    | =[<datatype>]=                                    |
| =MPI_Gather=         | =<rank> gather <send_size> <recv_size> <root>=    |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGather=      | =<rank> allGather <send_size> <recv_size>=        |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoall=       | =<rank> allToAll <send_size> <recv_recv>=         |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoallv=      | =<rank> allToAllV <send_size> <send_sizes†>=      |
|                    | =<recv_size> <recv_sizes†>=                       |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_GatherV=        | =<rank> gatherV <send_size> <recv_sizes†> <root>= |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGatherV=     | =<rank> allGatherV <send_size> <recv_sizes†>=     |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Barrier=        | =<rank> barrier=                                  |
| =MPI_Wait=           | =<rank> wait=                                     |
| =MPI_Waitall=        | =<rank> waitAll=                                  |
| =MPI_Init=           | =<rank> init [<set_default_double>]=              |
| =MPI_Finalize=       | =<rank> finalize=                                 |
| =MPI_Comm_size=      | =<rank> comm_size <size>=                         |
| =MPI_Comm_split=     | =<rank> comm_split=                               |
| =MPI_Comm_dup=       | =<rank> comm_dup=                                 |
† =send_sizes/recv_sizes= is an array, the number of elements must be equal
to the number of processes in the communicator.

* More Paraver traces with more MPI primitives <2015-09-20 Sun> :tracing:paraver:
We need to make sure the conversion script is able to handle any kind of Paraver trace.
So far, our script does not handle some MPI primitives that are less used, such as MPI_AlltoAllv.
I tried to install the Extrae tool to use it to generate some traces but I failed in completing this task.

* New script for converting to =.tit= format <2015-10-01 Thu> :tit:
I created a new script based on the old one so I could have a better ideia of what was being done.
The script started supporting the MPI calls on the =EXTRAE_Paraver_trace_mpich= trace

* Immediate send format on the Paraver trace <2015-10-01 Thu> :paraver:tracing:
The =cgpop.linux_icc_mt.180x120.24tasks.chop1= trace contains immediate
sends and this was one of the reasons the script was failing. I
examined how the trace tool generates the entries in the trace file
when an immediate send happens. First we have a state entry telling us
that the task is in an immediate send state, then we have a event
entry (right after the state entry) with the =MPI_Isend= event. Keep in
mind that the parameters we need for an immediate send is the destiny
and the communication size. The event entry, however, does not provide
any of those. If we continue on the trace, we will find a
communication entry that is associated with the immediate send. This
communication entry is not necessarily after the event entry.  Also,
this entry contains the destiny and the message size. In the example
below the immediate send event entry is the first line while the
communication entry of the immediate send is the last line.
#+BEGIN_SRC shell
2:3:1:3:1:52372287:50000001:3:42000050:14833...
1:6:1:6:1:52372820:52379709:1
2:6:1:6:1:52372820:50000001:0:42000050:8708...
1:6:1:6:1:52379709:52390070:11
2:6:1:6:1:52379709:50000001:4:42000050:2191...
1:3:1:3:1:52389139:52396811:1
2:3:1:3:1:52389139:50000001:0
3:3:1:3:1:52372287:52389139:2:1:2:1:52853369:53050068:9912:103
#+END_SRC

* 2015-09-15 Questions
The list below contains all questions that I came across during this project.
Feel free to contribute with questions or answers :)
1. The use of a time-independent trace collected on a platform X can
   be used to simulate a platform Y under what conditions? Condition
   example, X and Y must contain the same family of processors or same
   number of processors. The reference 1 was not so clear about this.

* 2015-10-02 Using Arnaud's init.org                                  :Lucas:

Please, follow the instructions of:

http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php

* 2015-10-02 Installing Extrae                                  :Lucas:Tiago:

That's really hard, lots of dependencies, problems, log below tries to
handle all that, but several indications are Debian testing-specific.

Download latest Extrae from:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Dyninst dependencies in my box:

#+BEGIN_SRC sh
sudo apt-get install libboost-thread-dev libboost-system-dev libelf-dev
#+END_SRC

Download dyinst, configure and install:

#+begin_src sh :results output :session :exports both
cd ~/misc
wget --quiet http://www.paradyn.org/release9.0.3/DyninstAPI-9.0.3.tgz
tar xfz DyninstAPI-9.0.3.tgz
cd DyninstAPI-9.0.3
mkdir build
cd build
cmake ..
make
make install
#+end_src

Let's install other Extrae's dependencies:

#+begin_src R :results output :session :exports both
sudo apt-get install libopempi-dev libdwarf-dev libpapi-dev libbinutils-dev
#+end_src

I have put in my *misc* directory:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.1.0.tar.bz2
cd extrae-3.1.0
./configure --prefix=$HOME/install/extrae-3.1.0/ \
            --with-mpi=/usr/lib/openmpi/ \
            --with-unwind=/usr \
            --with-dyninst=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/usr/ --with-papi=/usr --with-binutils=/usr
make
make install
#+END_SRC

After the configure and before make, I had to comment the line on the
Makefile that contained -lsymLite, since I was unable to find such
library in my system (or any other solution in the internet). I have
mailed tools@bsc.es to get a proper solution.

* 2015-10-02 Checking if Extrae was correctly installed               :Lucas:

So, I have both Extrae and Dyninst installed.

These are the steps to see if it is working:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR="/home/schnorr/install/stow/"
rm -rf $STOW_DIR/*
mkdir -p $STOW_DIR
echo $STOW_DIR
stow /home/schnorr/install/dyninst-9.0.3/
stow /home/schnorr/install/extrae-3.1.0/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin/
extrae
#+END_SRC

I wasn't able to run the following commands from org-mode.

If it works, you should have something like this:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
extrae
#+END_SRC

You should see the message:

+ *Extrae: You have to provide a binary to instrument*

* 2015-10-02 Checking if Extrae is able to instrument                 :Lucas:

Environment variables:

#+BEGIN_SRC sh :results output :session myses
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
#+END_SRC

#+RESULTS:


#+BEGIN_SRC C :tangle mpi_init_finalize.c
#include <mpi.h>
int main (int argc, char **argv)
{
  MPI_Init(&argc, &argv);
  MPI_Finalize();
}
#+END_SRC

Compile:

#+BEGIN_SRC sh :results output :session myses
mpicc mpi_init_finalize.c
#+END_SRC

#+RESULTS:

Instrument:

#+BEGIN_SRC sh :results output :session myses
export EXTRAE_CONFIG_FILE=/home/schnorr/install/extrae-3.1.0/share/example/MPI/extrae.xml
extrae a.out
#+END_SRC

#+RESULTS:
: 
: Welcome to Extrae 3.1.0 revision 3316 based on extrae/trunk launcher using DynInst 9.0.3
: Extrae: Creating process for image binary a.out
: Extrae: Error creating the target application process

* 2015-10-15 New Paraver traces                                       :Tiago:
After installing EXTRAE, we were able to generate the traces of a few
simple MPI applications.  Those applications can be found in the
directory =applications=, and a script to generate their traces is also
on the directory.  You need to have EXTRAE installed, however.  The
traces generated will be helpful to validade the translation to tit.

* 2015-10-15 New approach                                             :Tiago:
We came to the conclusion that we can not generate the =.tit= entries
one by one while reading the Paraver trace file.  The new script
=prv2tit.pl= uses buffers to store the MPI calls that have missing
parameters.  Those parameters can be obtained in a trace entry that
can be 10 or 20 lines after the event.  The immediate send MPI call is
an example of this situation.  Anyway, the script is able to translate
all MPI calls supported by SIMGRID except for the V operations.  The V
operations require some extra functionality in the script that will be
soon implemented.

* 2015-10-15 How to run it?                                           :Tiago:
To convert a Paraver trace file to the time-independent trace format execute the following command:
#+BEGIN_SRC shell
$ perl prv2tit.pl -i <paraver_trace_file>
#+END_SRC

* 2015-10-16 Meeting with Tiago                                 :Lucas:Tiago:

+ The /MPI_Comm_size/ is a function that receives the size of the
  communicator. It is pretty easy to obtain, Tiago will do it soon.
  An MPI application might have multiple communicators, but luckly the
  paraver trace file keeps track of all this. So we basically need a
  hash mapping the communicator identifier to its size. When the event
  /MPI_Comm_size/ appears for a given process, we only have to lookup in
  that hash table.
+ All operations that receive as parameter the *size* (of something:
  receive or send sizes particular to each process) are going to be
  implemented by Tiago soon. They are not yet translated because they
  are slightly more complicated to get the size parameter for each
  processes. Tiago already has an idea on how to deal with that, he's
  going to implement that soon. These are the concerned functions:
  /MPI_Reduce_scatter/, /MPI_Alltoallv/, /MPI_Allgatherv/, /MPI_GatherV/.
+ All operations that have /comp_size/ are not yet completed (for
  example: allReduce and reduce). This comp size means "computation"
  since they run an operation for the reduce. We have to take into
  account the computation cost of doing so, but as of now, we have no
  idea on how to obtain such metric from the trace. We could take the
  time spent in the reduce operation, but that means it would also
  take the communication time. We intend to discuss this with Arnaud
  to get his opinion.
+ Micro-applications for validation: Tiago has already done four
  micro-applications (in the applications dir). He is going to extend
  that in order to check if every single MPI operation supported by
  tit is being correctly translated.
+ The next step is simulating the tit traces using SimGrid. It would
  be better to use the *git* version since any changes can be considered
  at the moment they are pushed to there. Here's the git you have to
  clone:

  git+ssh://schnorr@scm.gforge.inria.fr//gitroot//simgrid/simgrid.git

+ We have used smpi2pj.sh script, but reading of tit files generated
  by Tiago's script is not working. The current idea of why is that
  ranks are starting at 1, instead of zero. We have looked in Arnaud's
  script and he indeed subtract 1 from task identifiers. We have done
  so rapidly (see commit) and it works.

+ We convert the paraver to tit, we simulate with /smpi_replay/ and we
  got a paje trace file, and then we convert it to CSV using /pj_dump/
  from pajeng package [1]. With that, we have the "SimGrid size".  To
  have the "Dimemas size", we get the paraver, we feed to to dimemas,
  we get another paraver that reflects the simulation that has been
  conducted by dimemas (so timestamps are going to change), and then
  we have to convert this second simulated paraver trace file to Paje,
  and then use /pj_dump/ to CSV, to finally tackle the comparison
  between Simgrid and Dimemas.

  + "SimGrid size" is almost ready.
  + "Dimemas size" depends on paraver2paje (Tiago has to build a
    paraver to paje converter as well, which is going a copy of the
    prv2tit, but much more easy).

[1]: https://github.com/schnorr/pajeng/


* 2015-10-17 SMPI replay and MPI_Comm_* calls                         :Tiago:
The MPI calls =MPI_Comm_size=, =MPI_Comm_split= and =MPI_Comm_dup= are
supported by the smpi replay command. These calls, however, do not
affect the simulation process in any way. Therefore, we will be
ignoring the events in the trace file that contain these MPI calls.


* 2015-10-18 Support of "V" operations                                :Tiago:
In order to support the "v" operations we made a tweak in the script
so it stores the communicators information.  Each "v" operation has a
buffer for storing its events in the communicator data structure.  To
generate a tit entry for that v operation, the mpi call event from all
the tasks in the communicator must be read first.  Currently
=MPI_Gatherv=, =MPI_Allgatherv=, =MPI_Reduce_scatter= are supported.
=MPI_Alltoallv= has a slightly more different implementation and will
be supported soon.  The next step will be to create a few MPI
applications for testing the translation of this MPI calls.


* 2015-10-24 New MPI apps                                             :Tiago:
I created a few more MPI applications on the =applications/=
folder. They contain all MPI calls supported by SIMGRID. The shell
script in the applications folder was used to generate their Paraver
traces, which can be found on the =paraver_traces/= folder. The next
step is use the conversion script to make sure all MPI calls are being
converted correctly.


* 2015-10-25 Validation with the new apps traces                      :Tiago:
I tested the generated application traces with the conversion
scripts. Only a few minor adjustments had to be made (print the
correct task index for example).  All of the MPI calls are correctly
translated into its tit version with one exception.  The MPI call
=MPI_Alltoallv= requires a serie of parameters that apparently can not
be obtained from the Paraver trace.  The tit format of this call
requires:

+ the size of the send buffer
+ an array containing the size of the msg sent to each process in the communicator
+ the size of the receive buffer and 
+ an array with containing the size of the msg to be received from with each process in the communicator.

The Paraver trace, however, only contain the total size of the msg
sent of each process. Therefore, the script is currently not
supporting this call until we find a workaround for this problem.
* 2016-01-18 Dynamic instrumentation only with Dyninst 8.2.1          :Lucas:

Here's the e-mail message I got from German Llort (from BSC).

We've noticed in the output logs that you sent that you're using
Dyninst version 9.0.3.

The last version that we've tested of Dyninst is 8.2.1, and it is very
likely that with the upgrade to version 9.x there's been some major
changes that broke compatibility. While we run some tests with this
last version, I can suggest you two ways to proceed:

- You can try installing Dyninst 8.2.1, which should work.

- If you don't need to instrument user functions, you can trace your
  application with the alternate method based on LD_PRELOAD (this
  mechanism doesn't use Dyninst). You have examples of this mechanism
  in section 7.2 of the user guide, and under the
  "share/examples/MPI/ld-preload" directory Extrae's home directory.

* 2016-01-18 Extrae installation in another system (Sara)             :Lucas:

I skip dyninst installation since I don't think I need it now to trace MPI.

I have download the latest 3.2.1 version from extrae here:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

I have installed *binutils-dev*.

Now I have issued the following commands:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.2.1.tar.bz2
cd extrae-3.2.1
./configure --prefix=$HOME/install/extrae-3.2.1/  \
            --with-mpi=/usr/lib/openmpi/ \
            --without-unwind \
            --without-dyninst --without-papi
make
make install
#+END_SRC

Configure works, but I wans't able to compile because of this
problem. I just e-mail support tools@bsc.es to understand if they can
solve the problem for me. In the meantime, I will try extrae 3.1.0,
which tarball I have at guarani.
