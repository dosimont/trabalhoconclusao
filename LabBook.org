#+TITLE: tcbozzetti's LabBook
#+AUTHOR: Tiago
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Tiago(T) Arnaud(A) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Issues with =.tit= <2015-09-10 Thu>                                                   :tit:
I was testing the conversion of the =lulesh2.0_p216_n108_t1.chop1= trace
and I came across the following problem: This code snippet opens a
file for each node.
#+BEGIN_SRC perl
my $nb_proc = 0;
foreach my $node (@{$$resource_name{NODE}}) { 
	my $filename = $output."_$nb_proc.tit";
	open($fh[$nb_proc], "> $filename") or die "Cannot open > $filename: $!";
	nb_proc++;
}
#+END_SRC
While this code snippet assumes the number of opened files is equal to the number of tasks.
#+BEGIN_SRC perl
$task = $task - 1;
defined($tit_translate{$sname}) or die "Unknown state '$sname' for tit\n";
if($tit_translate{$sname} ne "") {
	print { $fh[$task] } "$task $tit_translate{$sname} $sname_param\n",
}
#+END_SRC
Currrently, I am assuming that there must be one =.tit= file for each
node, therefore, I modified the last snippet to write on the task's
node =.tit= file.

* Some notes <2015-09-13 Sun>                                                         :notes:
Understanding the performance of parallel applications is a concern in
the HPC community.  When considering platforms that are not available,
it is necessarty to simulate the application. Therefore, it is
possible to determine a cost-effective hardware configuration for that
particular application.  When considering a platform that is
available, simulating is also important since the access to
large-scale platforms can be costly.

Two frameworks for simulating MPI applications:
1. On-line simulations
	* The same amount of hardware is required to simulate.
	* Usually uses a simple network model to calculate the
          communication delays.
	* Can simulate the computational delays, but data-dependent
          application behavior is lost.
2. Off-line simulations
	* Use a trace of the parallel application.
	* Can be performed on a single computer.
	* Computational delays are scaled based on the performance
          differential between the original and the simulated
          platform.
		* If simulator uses time information, target and
                  original platform must be the same.
		* If trace is time independent, the only condition is
                  that the processors of the target platforms must be
                  of the same family as the original platform.
	* Communication delays are computed based on a network
          simulator.
	* Partially addresses the simulation of data-dependent
          application. Some data-dependent behavior can be captured in
          the trace and simulated.

Off-line simulators differ by the simulation models they employ to
compute simulated durations of CPU bursts and communication
operations.

On a time-independent trace, the CPU bursts or communication
operations are logged with its volume (in number of executed
instructions or in number of transferred bytes) instead of the time
when it begins and ends or its duration.  Therefore, the trace can not
be associated with a platform anymore (with the exception of the
processor family).  This imply that the MPI application can not modify
its execution according to the execution platform (AMPI applications).

*For large numbers of processes and/or numbers of actions, it may be
preferable to split the trace so as to obtain one trace file per
process.*

*Table: Time-independent actions corresponding to supported MPI communication operations.*
| MPI actions        | Trace entry                                  |
|--------------------+----------------------------------------------|
| =CPU burst=          | =<rank> compute <volume>=                      |
| =MPI_Send=           | =<rank> send <dst_rank> <volume>=              |
| =MPI_Isend=          | =<rank> Isend <dst_rank> <volume>=             |
| =MPI_Recv=           | =<rank> recv <src_rank> <volume>=              |
| =MPI_Irecv=          | =<rank> Irecv <src_rank> <volume>=             |
| =MPI_Broadcast=      | =<rank> bcast <volume>=                        |
| =MPI_Reduce=         | =<rank> reduce <vcomm> <vcomp>=                |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_counts> <vcomp>=   |
| =MPI_Allreduce=      | =<rank> allReduce <vcomm> <vcomp>=             |
| =MPI_Alltoall=       | =<rank> allToAll <send_volume> <recv_volume>=  |
| =MPI_Alltoallv=      | =<rank> allToAllv <send_volume> <send_counts>= |
|                    | =<recv_volume> <recv_counts>=                  |
| =MPI_Gather=         | =<rank> gather <send_volume> <recv_volume>=    |
| =MPI_Allgatherv=     | =<rank> allGatherV <send_count> <recv_counts>= |
| =MPI_Barrier=        | =<rank> barrier=                               |
| =MPI_Wait=           | =<rank> wait=                                  |
| =MPI_Waitall=        | =<rank> waitAll=                               |
Source: references/ref1.pdf
* Starting LabBook <2015-09-16 Wed> :journal:
Lucas setup the org mode file for the journal.
** Copied the entries from the README file
I have been looking at some org-mode documentation to make this
journal better. It turns out they have a ton of features and I plan
to make good use of them to make this project more organized.
* Testing the off-line simulation on SimGrid <2015-09-17 Thu> :tit:simgrid:
I was trying to simulate the time-independent trace files generated by
the =EXTRAE_Paraver_trace_mpich= trace. There was a problem, however.
More specifically, the issue was the ranks of the processes on the
=.tit= files. The little piece of code that I modified last thursday
assumed that the number of =.tit= files were equivalent to the number
of nodes, however that was only executed for state entries in the
trace. When an event was processed, the old code was executed and
therefore, the resulting =.tit= was inconsistent. I fixed this by
also changing the code that handles the events. Another problem is
that the ranks of the processes on the =.tit= files must start with
zero (I am not completely sure about this). If I set the ranks of
the processes to start with one, there is an exception during the
simulation. When I use the option =-map= on the =smpirun= command I get
the following (even when there is no rank zero on the =.tit= files):
#+BEGIN_SRC shell
[rank 0] -> graphene-1.nancy.grid5000.fr
[rank 1] -> graphene-2.nancy.grid5000.fr
[rank 2] -> graphene-3.nancy.grid5000.fr
[rank 3] -> graphene-4.nancy.grid5000.fr
[rank 4] -> graphene-5.nancy.grid5000.fr
[rank 5] -> graphene-6.nancy.grid5000.fr
[rank 6] -> graphene-7.nancy.grid5000.fr
[rank 7] -> graphene-8.nancy.grid5000.fr
#+END_SRC
This led me to believe that the ranks of the processes must start with
zero. This fact can cause some headache since the Paraver trace files
do not assume that they should start with zero. During the conversion
to the time-independent trace format, we simply compute the rank that
will be written on the =.tit= file by subtracting one. However, this
solution will not work for every case. Imagine if the Paraver trace uses
the rank zero...
* Compilation of all smpi replay operations <2015-09-17 Thu> :trace_replay:simgrid:tit:
I took a look at the source code of the =smpi_replay= (the
=references/ref1.pdf= does not contain all operations and all possible
arguments) and compiled all the smpi trace replay operations and its
arguments. I expect that this can be useful in the future.

| MPI actions          | Trace entry                                       |
|----------------------+---------------------------------------------------|
| =CPU burst=          | =<rank> compute <flops>=                          |
| =MPI_Send=           | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Isend=          | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Recv=           | =<rank> recv <src> <comm_size> [<datatype>]=      |
| =MPI_Irecv=          | =<rank> Irecv <src> <comm_size> [<datatype>]=     |
| =MPI_Broadcast=      | =<rank> bcast <comm_size> [<root> [<datatype>]]=  |
| =MPI_Reduce=         | =<rank> reduce <comm_size> <comp_size>=           |
|                      | =[<root> [<datatype>]]=                           |
| =MPI_AllReduce=      | =<rank> allReduce <comm_size> <comp_size>=        |
|                      | =[<datatype>]=                                    |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_sizes†> <comp_size>=   |
|                      | =[<datatype>]=                                    |
| =MPI_Gather=         | =<rank> gather <send_size> <recv_size> <root>=    |
|                      | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGather=      | =<rank> allGather <send_size> <recv_size>=        |
|                      | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoall=       | =<rank> allToAll <send_size> <recv_recv>=         |
|                      | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoallv=      | =<rank> allToAllV <send_size> <send_sizes†>=       |
|                      | =<recv_size> <recv_sizes†>=                        |
|                      | =[<send_datatype> <recv_datatype>]=               |
| =MPI_GatherV=        | =<rank> gatherV <send_size> <recv_sizes†> <root>=  |
|                      | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGatherV=     | =<rank> allGatherV <send_size> <recv_sizes>=      |
|                      | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Barrier=        | =<rank> barrier=                                  |
| =MPI_Wait=           | =<rank> wait=                                     |
| =MPI_Waitall=        | =<rank> waitAll=                                  |
| =MPI_Init=           | =<rank> init [<set_default_double>]=              |
| =MPI_Finalize=       | =<rank> finalize=                                 |
| =MPI_Comm_size=      | =<rank> comm_size <size>=                         |
| =MPI_Comm_split=     | =<rank> comm_split=                               |
| =MPI_Comm_dup=       | =<rank> comm_dup=                                 |
† =send_sizes/recv_sizes= is an array, the number of elements must be equal
to the number of processes in the communicator.

* Questions
The list below contains all questions that I came across during this project.
Feel free to contribute with questions or answers :)
1. The use of a time-independent trace collected on a platform X can
   be used to simulate a platform Y under what conditions? Condition
   example, X and Y must contain the same family of processors or same
   number of processors. The reference 1 was not so clear about this.

* Roadmap
The list below summarizes what is not yet done.
Please, feel free to create items and subitems if necessary.
** TODO Make sure conversion script is capable of translating all MPI primitives accepted by the SMPI trace replay.
- State "TODO"       from ""           [2015-09-18 Sex 10:25]
** TODO Look into why the two other traces are not working how they should.
- State "TODO"       from ""           [2015-09-18 Sex 10:26]
