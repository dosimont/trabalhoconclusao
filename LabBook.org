#+TITLE: tcbozzetti's LabBook
#+AUTHOR: Tiago
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Tiago(T) Arnaud(A) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Issues with =.tit= <2015-09-10 Thu>                                                   :tit:
I was testing the conversion of the =lulesh2.0_p216_n108_t1.chop1= trace
and I came across the following problem: This code snippet opens a
file for each node.
#+BEGIN_SRC perl
my $nb_proc = 0;
foreach my $node (@{$$resource_name{NODE}}) { 
	my $filename = $output."_$nb_proc.tit";
	open($fh[$nb_proc], "> $filename") or die "Cannot open > $filename: $!";
	nb_proc++;
}
#+END_SRC
While this code snippet assumes the number of opened files is equal to the number of tasks.
#+BEGIN_SRC perl
$task = $task - 1;
defined($tit_translate{$sname}) or die "Unknown state '$sname' for tit\n";
if($tit_translate{$sname} ne "") {
	print { $fh[$task] } "$task $tit_translate{$sname} $sname_param\n",
}
#+END_SRC
Currrently, I am assuming that there must be one =.tit= file for each
node, therefore, I modified the last snippet to write on the task's
node =.tit= file.

* Some notes <2015-09-13 Sun>                                                         :notes:
Understanding the performance of parallel applications is a concern in
the HPC community.  When considering platforms that are not available,
it is necessarty to simulate the application. Therefore, it is
possible to determine a cost-effective hardware configuration for that
particular application.  When considering a platform that is
available, simulating is also important since the access to
large-scale platforms can be costly.

Two frameworks for simulating MPI applications:
1. On-line simulations
	* The same amount of hardware is required to simulate.
	* Usually uses a simple network model to calculate the
          communication delays.
	* Can simulate the computational delays, but data-dependent
          application behavior is lost.
2. Off-line simulations
	* Use a trace of the parallel application.
	* Can be performed on a single computer.
	* Computational delays are scaled based on the performance
          differential between the original and the simulated
          platform.
		* If simulator uses time information, target and
                  original platform must be the same.
		* If trace is time independent, the only condition is
                  that the processors of the target platforms must be
                  of the same family as the original platform.
	* Communication delays are computed based on a network
          simulator.
	* Partially addresses the simulation of data-dependent
          application. Some data-dependent behavior can be captured in
          the trace and simulated.

Off-line simulators differ by the simulation models they employ to
compute simulated durations of CPU bursts and communication
operations.

On a time-independent trace, the CPU bursts or communication
operations are logged with its volume (in number of executed
instructions or in number of transferred bytes) instead of the time
when it begins and ends or its duration.  Therefore, the trace can not
be associated with a platform anymore (with the exception of the
processor family).  This imply that the MPI application can not modify
its execution according to the execution platform (AMPI applications).

*For large numbers of processes and/or numbers of actions, it may be
preferable to split the trace so as to obtain one trace file per
process.*

*Table: Time-independent actions corresponding to supported MPI communication operations.*
| MPI actions        | Trace entry                                  |
|--------------------+----------------------------------------------|
| =CPU burst=          | =<rank> compute <volume>=                      |
| =MPI_Send=           | =<rank> send <dst_rank> <volume>=              |
| =MPI_Isend=          | =<rank> Isend <dst_rank> <volume>=             |
| =MPI_Recv=           | =<rank> recv <src_rank> <volume>=              |
| =MPI_Irecv=          | =<rank> Irecv <src_rank> <volume>=             |
| =MPI_Broadcast=      | =<rank> bcast <volume>=                        |
| =MPI_Reduce=         | =<rank> reduce <vcomm> <vcomp>=                |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_counts> <vcomp>=   |
| =MPI_Allreduce=      | =<rank> allReduce <vcomm> <vcomp>=             |
| =MPI_Alltoall=       | =<rank> allToAll <send_volume> <recv_volume>=  |
| =MPI_Alltoallv=      | =<rank> allToAllv <send_volume> <send_counts>= |
|                    | =<recv_volume> <recv_counts>=                  |
| =MPI_Gather=         | =<rank> gather <send_volume> <recv_volume>=    |
| =MPI_Allgatherv=     | =<rank> allGatherV <send_count> <recv_counts>= |
| =MPI_Barrier=        | =<rank> barrier=                               |
| =MPI_Wait=           | =<rank> wait=                                  |
| =MPI_Waitall=        | =<rank> waitAll=                               |
Source: references/ref1.pdf
* Starting LabBook <2015-09-16 Wed> :journal:
Lucas setup the org mode file for the journal.
** Copied the entries from the README file
I have been looking at some org-mode documentation to make this
journal better. It turns out they have a ton of features and I plan
to make good use of them to make this project more organized.
* Testing the off-line simulation on SimGrid <2015-09-17 Thu> :tit:simgrid:
I was trying to simulate the time-independent trace files generated by
the =EXTRAE_Paraver_trace_mpich= trace. There was a problem, however.
More specifically, the issue was the ranks of the processes on the
=.tit= files. The little piece of code that I modified last thursday
assumed that the number of =.tit= files were equivalent to the number
of nodes, however that was only executed for state entries in the
trace. When an event was processed, the old code was executed and
therefore, the resulting =.tit= was inconsistent. I fixed this by
also changing the code that handles the events. Another problem is
that the ranks of the processes on the =.tit= files must start with
zero (I am not completely sure about this). If I set the ranks of
the processes to start with one, there is an exception during the
simulation. When I use the option =-map= on the =smpirun= command I get
the following (even when there is no rank zero on the =.tit= files):
#+BEGIN_SRC shell
[rank 0] -> graphene-1.nancy.grid5000.fr
[rank 1] -> graphene-2.nancy.grid5000.fr
[rank 2] -> graphene-3.nancy.grid5000.fr
[rank 3] -> graphene-4.nancy.grid5000.fr
[rank 4] -> graphene-5.nancy.grid5000.fr
[rank 5] -> graphene-6.nancy.grid5000.fr
[rank 6] -> graphene-7.nancy.grid5000.fr
[rank 7] -> graphene-8.nancy.grid5000.fr
#+END_SRC
This led me to believe that the ranks of the processes must start with
zero. This fact can cause some headache since the Paraver trace files
do not assume that they should start with zero. During the conversion
to the time-independent trace format, we simply compute the rank that
will be written on the =.tit= file by subtracting one. However, this
solution will not work for every case. Imagine if the Paraver trace uses
the rank zero...
* Compilation of all smpi replay operations <2015-09-17 Thu> :trace_replay:simgrid:tit:
I took a look at the source code of the =smpi_replay= (the
=references/ref1.pdf= does not contain all operations and all possible
arguments) and compiled all the smpi trace replay operations and its
arguments. I expect that this can be useful in the future.

| MPI actions        | Trace entry                                     |
|--------------------+-------------------------------------------------|
| =CPU burst=          | =<rank> compute <flops>=                          |
| =MPI_Send=           | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Isend=          | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Recv=           | =<rank> recv <src> <comm_size> [<datatype>]=      |
| =MPI_Irecv=          | =<rank> Irecv <src> <comm_size> [<datatype>]=     |
| =MPI_Broadcast=      | =<rank> bcast <comm_size> [<root> [<datatype>]]=  |
| =MPI_Reduce=         | =<rank> reduce <comm_size> <comp_size>=           |
|                    | =[<root> [<datatype>]]=                           |
| =MPI_AllReduce=      | =<rank> allReduce <comm_size> <comp_size>=        |
|                    | =[<datatype>]=                                    |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_sizes†> <comp_size>=  |
|                    | =[<datatype>]=                                    |
| =MPI_Gather=         | =<rank> gather <send_size> <recv_size> <root>=    |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGather=      | =<rank> allGather <send_size> <recv_size>=        |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoall=       | =<rank> allToAll <send_size> <recv_recv>=         |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoallv=      | =<rank> allToAllV <send_size> <send_sizes†>=      |
|                    | =<recv_size> <recv_sizes†>=                       |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_GatherV=        | =<rank> gatherV <send_size> <recv_sizes†> <root>= |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGatherV=     | =<rank> allGatherV <send_size> <recv_sizes†>=     |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Barrier=        | =<rank> barrier=                                  |
| =MPI_Wait=           | =<rank> wait=                                     |
| =MPI_Waitall=        | =<rank> waitAll=                                  |
| =MPI_Init=           | =<rank> init [<set_default_double>]=              |
| =MPI_Finalize=       | =<rank> finalize=                                 |
| =MPI_Comm_size=      | =<rank> comm_size <size>=                         |
| =MPI_Comm_split=     | =<rank> comm_split=                               |
| =MPI_Comm_dup=       | =<rank> comm_dup=                                 |
† =send_sizes/recv_sizes= is an array, the number of elements must be equal
to the number of processes in the communicator.

* More Paraver traces with more MPI primitives <2015-09-20 Sun> :tracing:paraver:
We need to make sure the conversion script is able to handle any kind of Paraver trace.
So far, our script does not handle some MPI primitives that are less used, such as MPI_AlltoAllv.
I tried to install the Extrae tool to use it to generate some traces but I failed in completing this task.

* New script for converting to =.tit= format <2015-10-01 Thu> :tit:
I created a new script based on the old one so I could have a better ideia of what was being done.
The script started supporting the MPI calls on the =EXTRAE_Paraver_trace_mpich= trace

* Immediate send format on the Paraver trace <2015-10-01 Thu> :paraver:tracing:
The =cgpop.linux_icc_mt.180x120.24tasks.chop1= trace contains immediate
sends and this was one of the reasons the script was failing. I
examined how the trace tool generates the entries in the trace file
when an immediate send happens. First we have a state entry telling us
that the task is in an immediate send state, then we have a event
entry (right after the state entry) with the =MPI_Isend= event. Keep in
mind that the parameters we need for an immediate send is the destiny
and the communication size. The event entry, however, does not provide
any of those. If we continue on the trace, we will find a
communication entry that is associated with the immediate send. This
communication entry is not necessarily after the event entry.  Also,
this entry contains the destiny and the message size. In the example
below the immediate send event entry is the first line while the
communication entry of the immediate send is the last line.
#+BEGIN_SRC shell
2:3:1:3:1:52372287:50000001:3:42000050:14833...
1:6:1:6:1:52372820:52379709:1
2:6:1:6:1:52372820:50000001:0:42000050:8708...
1:6:1:6:1:52379709:52390070:11
2:6:1:6:1:52379709:50000001:4:42000050:2191...
1:3:1:3:1:52389139:52396811:1
2:3:1:3:1:52389139:50000001:0
3:3:1:3:1:52372287:52389139:2:1:2:1:52853369:53050068:9912:103
#+END_SRC

* 2015-09-15 Questions
The list below contains all questions that I came across during this project.
Feel free to contribute with questions or answers :)
1. The use of a time-independent trace collected on a platform X can
   be used to simulate a platform Y under what conditions? Condition
   example, X and Y must contain the same family of processors or same
   number of processors. The reference 1 was not so clear about this.

* 2015-10-02 Using Arnaud's init.org                                  :Lucas:

Please, follow the instructions of:

http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php

* 2015-10-02 Installing Extrae                                  :Lucas:Tiago:

That's really hard, lots of dependencies, problems, log below tries to
handle all that, but several indications are Debian testing-specific.

Download latest Extrae from:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Dyninst dependencies in my box:

#+BEGIN_SRC sh
sudo apt-get install libboost-thread-dev libboost-system-dev libelf-dev
#+END_SRC

Download dyinst, configure and install:

#+begin_src sh :results output :session :exports both
cd ~/misc
wget --quiet http://www.paradyn.org/release9.0.3/DyninstAPI-9.0.3.tgz
tar xfz DyninstAPI-9.0.3.tgz
cd DyninstAPI-9.0.3
mkdir build
cd build
cmake ..
make
make install
#+end_src

Let's install other Extrae's dependencies:

#+begin_src R :results output :session :exports both
sudo apt-get install libopempi-dev libdwarf-dev libpapi-dev libbinutils-dev
#+end_src

I have put in my *misc* directory:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.1.0.tar.bz2
cd extrae-3.1.0
./configure --prefix=$HOME/install/extrae-3.1.0/ \
            --with-mpi=/usr/lib/openmpi/ \
            --with-unwind=/usr \
            --with-dyninst=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/usr/ --with-papi=/usr --with-binutils=/usr
make
make install
#+END_SRC

After the configure and before make, I had to comment the line on the
Makefile that contained -lsymLite, since I was unable to find such
library in my system (or any other solution in the internet). I have
mailed tools@bsc.es to get a proper solution.

* 2015-10-02 Checking if Extrae was correctly installed               :Lucas:

So, I have both Extrae and Dyninst installed.

These are the steps to see if it is working:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR="/home/schnorr/install/stow/"
rm -rf $STOW_DIR/*
mkdir -p $STOW_DIR
echo $STOW_DIR
stow /home/schnorr/install/dyninst-9.0.3/
stow /home/schnorr/install/extrae-3.1.0/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin/
extrae
#+END_SRC

I wasn't able to run the following commands from org-mode.

If it works, you should have something like this:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
extrae
#+END_SRC

You should see the message:

+ *Extrae: You have to provide a binary to instrument*

* 2015-10-02 Checking if Extrae is able to instrument                 :Lucas:

Environment variables:

#+BEGIN_SRC sh :results output :session myses
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
#+END_SRC

#+RESULTS:


#+BEGIN_SRC C :tangle mpi_init_finalize.c
#include <mpi.h>
int main (int argc, char **argv)
{
  MPI_Init(&argc, &argv);
  MPI_Finalize();
}
#+END_SRC

Compile:

#+BEGIN_SRC sh :results output :session myses
mpicc mpi_init_finalize.c
#+END_SRC

#+RESULTS:

Instrument:

#+BEGIN_SRC sh :results output :session myses
export EXTRAE_CONFIG_FILE=/home/schnorr/install/extrae-3.1.0/share/example/MPI/extrae.xml
extrae a.out
#+END_SRC

#+RESULTS:
: 
: Welcome to Extrae 3.1.0 revision 3316 based on extrae/trunk launcher using DynInst 9.0.3
: Extrae: Creating process for image binary a.out
: Extrae: Error creating the target application process

* 2015-10-15 New Paraver traces                                       :Tiago:
After installing EXTRAE, we were able to generate the traces of a few
simple MPI applications.  Those applications can be found in the
directory =applications=, and a script to generate their traces is also
on the directory.  You need to have EXTRAE installed, however.  The
traces generated will be helpful to validade the translation to tit.

* 2015-10-15 New approach                                             :Tiago:
We came to the conclusion that we can not generate the =.tit= entries
one by one while reading the Paraver trace file.  The new script
=prv2tit.pl= uses buffers to store the MPI calls that have missing
parameters.  Those parameters can be obtained in a trace entry that
can be 10 or 20 lines after the event.  The immediate send MPI call is
an example of this situation.  Anyway, the script is able to translate
all MPI calls supported by SIMGRID except for the V operations.  The V
operations require some extra functionality in the script that will be
soon implemented.

* 2015-10-15 How to run it?                                           :Tiago:
To convert a Paraver trace file to the time-independent trace format execute the following command:
#+BEGIN_SRC shell
$ perl prv2tit.pl -i <paraver_trace_file>
#+END_SRC

* 2015-10-16 Meeting with Tiago                                 :Lucas:Tiago:

+ The /MPI_Comm_size/ is a function that receives the size of the
  communicator. It is pretty easy to obtain, Tiago will do it soon.
  An MPI application might have multiple communicators, but luckly the
  paraver trace file keeps track of all this. So we basically need a
  hash mapping the communicator identifier to its size. When the event
  /MPI_Comm_size/ appears for a given process, we only have to lookup in
  that hash table.
+ All operations that receive as parameter the *size* (of something:
  receive or send sizes particular to each process) are going to be
  implemented by Tiago soon. They are not yet translated because they
  are slightly more complicated to get the size parameter for each
  processes. Tiago already has an idea on how to deal with that, he's
  going to implement that soon. These are the concerned functions:
  /MPI_Reduce_scatter/, /MPI_Alltoallv/, /MPI_Allgatherv/, /MPI_GatherV/.
+ All operations that have /comp_size/ are not yet completed (for
  example: allReduce and reduce). This comp size means "computation"
  since they run an operation for the reduce. We have to take into
  account the computation cost of doing so, but as of now, we have no
  idea on how to obtain such metric from the trace. We could take the
  time spent in the reduce operation, but that means it would also
  take the communication time. We intend to discuss this with Arnaud
  to get his opinion.
+ Micro-applications for validation: Tiago has already done four
  micro-applications (in the applications dir). He is going to extend
  that in order to check if every single MPI operation supported by
  tit is being correctly translated.
+ The next step is simulating the tit traces using SimGrid. It would
  be better to use the *git* version since any changes can be considered
  at the moment they are pushed to there. Here's the git you have to
  clone:

  git+ssh://schnorr@scm.gforge.inria.fr//gitroot//simgrid/simgrid.git

+ We have used smpi2pj.sh script, but reading of tit files generated
  by Tiago's script is not working. The current idea of why is that
  ranks are starting at 1, instead of zero. We have looked in Arnaud's
  script and he indeed subtract 1 from task identifiers. We have done
  so rapidly (see commit) and it works.

+ We convert the paraver to tit, we simulate with /smpi_replay/ and we
  got a paje trace file, and then we convert it to CSV using /pj_dump/
  from pajeng package [1]. With that, we have the "SimGrid size".  To
  have the "Dimemas size", we get the paraver, we feed to to dimemas,
  we get another paraver that reflects the simulation that has been
  conducted by dimemas (so timestamps are going to change), and then
  we have to convert this second simulated paraver trace file to Paje,
  and then use /pj_dump/ to CSV, to finally tackle the comparison
  between Simgrid and Dimemas.

  + "SimGrid size" is almost ready.
  + "Dimemas size" depends on paraver2paje (Tiago has to build a
    paraver to paje converter as well, which is going a copy of the
    prv2tit, but much more easy).

[1]: https://github.com/schnorr/pajeng/

* 2015-10-17 SMPI replay and MPI_Comm_* calls                         :Tiago:
The MPI calls =MPI_Comm_size=, =MPI_Comm_split= and =MPI_Comm_dup= are
supported by the smpi replay command. These calls, however, do not
affect the simulation process in any way. Therefore, we will be
ignoring the events in the trace file that contain these MPI calls.

* 2015-10-18 Support of "V" operations                                :Tiago:
In order to support the "v" operations we made a tweak in the script
so it stores the communicators information.  Each "v" operation has a
buffer for storing its events in the communicator data structure.  To
generate a tit entry for that v operation, the mpi call event from all
the tasks in the communicator must be read first.  Currently
=MPI_Gatherv=, =MPI_Allgatherv=, =MPI_Reduce_scatter= are supported.
=MPI_Alltoallv= has a slightly more different implementation and will
be supported soon.  The next step will be to create a few MPI
applications for testing the translation of this MPI calls.

* 2015-10-24 New MPI apps                                             :Tiago:
I created a few more MPI applications on the =applications/=
folder. They contain all MPI calls supported by SIMGRID. The shell
script in the applications folder was used to generate their Paraver
traces, which can be found on the =paraver_traces/= folder. The next
step is use the conversion script to make sure all MPI calls are being
converted correctly.

* 2015-10-25 Validation with the new apps traces                      :Tiago:
I tested the generated application traces with the conversion
scripts. Only a few minor adjustments had to be made (print the
correct task index for example).  All of the MPI calls are correctly
translated into its tit version with one exception.  The MPI call
=MPI_Alltoallv= requires a serie of parameters that apparently can not
be obtained from the Paraver trace.  The tit format of this call
requires:

+ the size of the send buffer
+ an array containing the size of the msg sent to each process in the communicator
+ the size of the receive buffer and 
+ an array with containing the size of the msg to be received from with each process in the communicator.

The Paraver trace, however, only contain the total size of the msg
sent of each process. Therefore, the script is currently not
supporting this call until we find a workaround for this problem.
* 2016-01-18 Dynamic instrumentation only with Dyninst 8.2.1          :Lucas:

Here's the e-mail message I got from German Llort (from BSC).

We've noticed in the output logs that you sent that you're using
Dyninst version 9.0.3.

The last version that we've tested of Dyninst is 8.2.1, and it is very
likely that with the upgrade to version 9.x there's been some major
changes that broke compatibility. While we run some tests with this
last version, I can suggest you two ways to proceed:

- You can try installing Dyninst 8.2.1, which should work.

- If you don't need to instrument user functions, you can trace your
  application with the alternate method based on LD_PRELOAD (this
  mechanism doesn't use Dyninst). You have examples of this mechanism
  in section 7.2 of the user guide, and under the
  "share/examples/MPI/ld-preload" directory Extrae's home directory.

* 2016-01-18 Extrae installation in another system (Sara)             :Lucas:

I skip dyninst installation since I don't think I need it now to trace MPI.

I have download the latest 3.2.1 version from extrae here:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

I have installed *libiberty-dev*.

Now I have issued the following commands:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.2.1.tar.bz2
cd extrae-3.2.1
#I need MPICC on Sara.
MPICC=/usr/bin/mpicc ./configure --prefix=/home/schnorr/install/extrae-3.2.1/ \
   --with-mpi=/usr/lib/openmpi/ \
   --without-unwind \
   --without-dyninst \
   --without-papi \
   --disable-openmp
make
make install
#+END_SRC

Configure, make and make install.
* 2016-01-18 Checking if extrae is able to instrument                 :Lucas:
TODO
* 2016-01-18 Problems of converted traces during replay               :Lucas:

I finally was able to run a replay from a tit file generated by the
conversion script. Here's how to do it. I'm supposing latest simgrid
from git was compiled and installed, all tools including =smpi_replay=
are in the PATH and can be correctly executed (=LD_LIBRARY_PATH= is also
configured).

#+begin_src sh :results output :session :exports both
export PATH=$PATH:$HOME/install/stow/bin/
export LD_LIBRARY_PATH=$HOME/install/stow/lib/
smpirun -keep-temps --log=replay.thresh:critical --log=smpi_replay.thresh:verbose --log=no_loc --cfg=smpi/running_power:1 --cfg=smpi/cpu_threshold:-1 -np 8 -platform griffon.xml -hostfile machine.txt smpi_replay paraver_traces/test.tit
#+end_src

#+RESULTS:

The smpirun command above seg faults, so running with keep-temps I can
run it manually with gdb. I was able to find out that the problem
comes from the gather event.

+ *Problems to be solved*:
  + =<comp_size>= should be replaced by 0 or the computational cost (if
    available, which is not the case for the paraver trace file)
  + =gather= event is not being correctly translated, that's the reason
    it segfaults. Documentation tells that we are not providing all
    parameters necessary for the gather. Here's where one of the
    problems appear:

    #+BEGIN_SRC text
    #4  0x00007ffff7a14188 in action_gather (action=action@entry=0x8280b0) at /home/schnorr/workspace/simgrid/src/smpi/smpi_replay.c:694
    694	    MPI_CURRENT_TYPE2=decode_datatype(action[6]);
    #+END_SRC

    From the documentation in =src/smpi/smpi_replay.c=:

    The structure of the gather action for the rank 0 (total 4
    processes) is the following:

    0 gather 68 68 0 0 0

    where: 
    1) 68 is the sendcounts
    2) 68 is the recvcounts
    3) 0 is the root node
    4) 0 is the send datatype id, see =decode_datatype()=
    5) 0 is the recv datatype id, see =decode_datatype()=

    Fred Suter told me there is something strange in the
    implementation of the gather replay. He just committed a solution
    that solves the problem very quickly.
* 2016-01-18 Current situation of translation                         :Lucas:

The following table is inspired from Table 1 of:
+ https://hal.inria.fr/hal-01064561/document

And also from the SimGrid code (=smpi_replay.c=) as of January 19th, 2016.


|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| *Name of the MPI action* | *TIT Entry*                                                                                                                                         | *Converted*                  |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Init=               | <r> init [default datatype]                                                                                                                       | ok (no datatype)           |
| =MPI_Finalize=           | <r> finalize (*not implemented in* =mpi_replay.c)=.                                                                                                   | ok                         |
| =MPI_Comm_size=          | <r> =comm_size= <double>                                                                                                                            |                            |
| =MPI_Comm_split=         | <r> =comm_split=                                                                                                                                    | ok                         |
| =MPI_Comm_dup=           | <r> =comm_dup=                                                                                                                                      | ok                         |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Send=               | <r> send <dst> <volume> [datatype]                                                                                                                | ok (no datatype)           |
| =MPI_Isend=              | <r> isend <dst> <volume> [datatype]                                                                                                               | ok (no datatype)           |
| =MPI_Recv=               | <r> recv <src> <volume> [datatype]                                                                                                                | ok (no datatype)           |
| =MPI_Irecv=              | <r> irecv <src> <volume> [datatype]                                                                                                               | ok (no datatype)           |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Test=               | <r> test                                                                                                                                          |                            |
| =MPI_Wait=               | <r> wait                                                                                                                                          | ok                         |
| =MPI_Waitall=            | <r> waitall                                                                                                                                       | ok                         |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Barrier=            | <r> barrier                                                                                                                                       | ok                         |
| =MPI_Bcast=              | <r> bcast <volume> [root] [datatype]                                                                                                              | ok (no datatype) / BUG     |
| =MPI_Reduce=             | <r> reduce <vcomm> <vcomp> [root] [datatype]                                                                                                      | ok (no datatype) / BUG     |
| =MPI_Allreduce=          | <r> allreduce <vcomm> <vcomp> [datatype]                                                                                                          | ok (no datatype)           |
| =MPI_Alltoall=           | <r> alltoall <sendvolume> <recvvolume> [datatype send] [datatype recv]                                                                            | ok (no datatype)           |
| =MPI_Alltoallv=          | <r> alltoallv <sendvolume> <sendcounts times communicator size> <recvvolume> <recvcounts times communicator size> [datatype send] [datatype recv] |                            |
| =MPI_Gather=             | <r> gather <sendvolume> <recvcounts> [send datatype] [recv datatype]                                                                              | ok (no datatypes) / BUG    |
| =MPI_Gatherv=            | <r> gatherv <sendvolume> <recvvolume times communicator size> <root> [send datatype] [recv datatype]                                              | ok (no datatypes)          |
| =MPI_Allgather=          | <r> allgather <sendvolume> <recvvolume> [send datatype] [recv datatype]                                                                           | ok (no datatypes)          |
| =MPI_Allgatherv=         | <r> allgatherv <sendvolume> <recvvolume times communicator size> [send datatype] [recv datatype]                                                  | ok (no datatypes)          |
| =MPI_Reduce_scatter=     | <r> reducescatter <recvvolume times communicator size> <vcomp> [datatype]                                                                         | ok (no datatypes)          |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| CPU burst              | <r> compute <volume>                                                                                                                              | ok (check =power_reference=) |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|

** Datatype translation

Check function =decode_datatype= at =smpi_replay.c=.

|----------------------+------------------|
| *Datatype translation* | *Kind*           |
|----------------------+------------------|
|                    0 | =MPI_DOUBLE=       |
|                    1 | =MPI_INT=          |
|                    2 | =MPI_CHAR=         |
|                    3 | =MPI_SHORT=        |
|                    4 | =MPI_LONG=         |
|                    5 | =MPI_FLOAT=        |
|                    6 | =MPI_BYTE=         |
|                    7 | =MPI_DEFAULT_TYPE= |
|----------------------+------------------|

** Action replay in Simgrid

#+BEGIN_SRC C
    xbt_replay_action_register("init",       action_init);
    xbt_replay_action_register("finalize",   action_finalize);
    xbt_replay_action_register("comm_size",  action_comm_size);
    xbt_replay_action_register("comm_split", action_comm_split);
    xbt_replay_action_register("comm_dup",   action_comm_dup);
    xbt_replay_action_register("send",       action_send);
    xbt_replay_action_register("Isend",      action_Isend);
    xbt_replay_action_register("recv",       action_recv);
    xbt_replay_action_register("Irecv",      action_Irecv);
    xbt_replay_action_register("test",       action_test);
    xbt_replay_action_register("wait",       action_wait);
    xbt_replay_action_register("waitAll",    action_waitall);
    xbt_replay_action_register("barrier",    action_barrier);
    xbt_replay_action_register("bcast",      action_bcast);
    xbt_replay_action_register("reduce",     action_reduce);
    xbt_replay_action_register("allReduce",  action_allReduce);
    xbt_replay_action_register("allToAll",   action_allToAll);
    xbt_replay_action_register("allToAllV",  action_allToAllv);
    xbt_replay_action_register("gather",  action_gather);
    xbt_replay_action_register("gatherV",  action_gatherv);
    xbt_replay_action_register("allGather",  action_allgather);
    xbt_replay_action_register("allGatherV",  action_allgatherv);
    xbt_replay_action_register("reduceScatter",  action_reducescatter);
    xbt_replay_action_register("compute",    action_compute);
#+END_SRC
** Comments about the implementation

+ =prv2tit.pl= / =power_reference= / =compute_action=
+ 
* 2016-02-15 DIMEMAS Installation                                     :Lucas:

Download DIMEMAS from

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Current version is:

+ dimemas-5.2.12.tar.gz

There are dependencies, check the README file and install them.

#+BEGIN_SRC sh
cd misc
tar xfz dimemas-5.2.12.tar.gz
cd dimemas-5.2.12
./configure --prefix=$HOME/install/dimemas-5.2.12/
make
make install
#+END_SRC

Here's the content after installation:

#+begin_src sh :results output :session :exports both
find /home/schnorr/install/dimemas-5.2.12/
#+END_SRC

#+RESULTS:
#+begin_example
/home/schnorr/install/dimemas-5.2.12/
/home/schnorr/install/dimemas-5.2.12/lib
/home/schnorr/install/dimemas-5.2.12/lib/GUI
/home/schnorr/install/dimemas-5.2.12/lib/GUI/dimemas-gui-5.2.12.jar
/home/schnorr/install/dimemas-5.2.12/lib/GUI/commons-io-2.4.jar
/home/schnorr/install/dimemas-5.2.12/sendrecv4.dim
/home/schnorr/install/dimemas-5.2.12/bin
/home/schnorr/install/dimemas-5.2.12/bin/trf2dim
/home/schnorr/install/dimemas-5.2.12/bin/DimemasGUI
/home/schnorr/install/dimemas-5.2.12/bin/DimemasUpdateCFG
/home/schnorr/install/dimemas-5.2.12/bin/Dimemas
/home/schnorr/install/dimemas-5.2.12/bin/prv2dim
/home/schnorr/install/dimemas-5.2.12/include
/home/schnorr/install/dimemas-5.2.12/include/extern_comm_model.h
/home/schnorr/install/dimemas-5.2.12/share
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/README
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/extern_comm_model.c
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/Makefile
/home/schnorr/install/dimemas-5.2.12/sendrecv4.pcf
/home/schnorr/install/dimemas-5.2.12/sendrecv4.row
#+end_example

We have a =Dimemas= binary and a =prv2dim= that interest us.

Converting a file from =prv= to =dim= file format.

#+begin_src sh :results output :session :exports both
cd ~/install/dimemas-5.2.12/
./bin/prv2dim ~/svn/bozzetti/paraver_traces2/sendrecv4.prv sendrecv4.dim
#+END_SRC

#+RESULTS:
#+begin_example
INITIALIZING PARSER... OK!

SPLITTING COMMUNICATIONS 000 %
SPLITTING COMMUNICATIONS 002 %
SPLITTING COMMUNICATIONS 003 %
SPLITTING COMMUNICATIONS 004 %
SPLITTING COMMUNICATIONS 005 %
SPLITTING COMMUNICATIONS 006 %
SPLITTING COMMUNICATIONS 007 %
SPLITTING COMMUNICATIONS 008 %
SPLITTING COMMUNICATIONS 009 %
SPLITTING COMMUNICATIONS 010 %
SPLITTING COMMUNICATIONS 011 %
SPLITTING COMMUNICATIONS 012 %
SPLITTING COMMUNICATIONS 013 %
SPLITTING COMMUNICATIONS 014 %
SPLITTING COMMUNICATIONS 015 %
SPLITTING COMMUNICATIONS 016 %
SPLITTING COMMUNICATIONS 017 %
SPLITTING COMMUNICATIONS 018 %
SPLITTING COMMUNICATIONS 019 %
SPLITTING COMMUNICATIONS 020 %
SPLITTING COMMUNICATIONS 021 %
SPLITTING COMMUNICATIONS 022 %
SPLITTING COMMUNICATIONS 023 %
SPLITTING COMMUNICATIONS 024 %
SPLITTING COMMUNICATIONS 025 %
SPLITTING COMMUNICATIONS 026 %
SPLITTING COMMUNICATIONS 027 %
SPLITTING COMMUNICATIONS 028 %
SPLITTING COMMUNICATIONS 029 %
SPLITTING COMMUNICATIONS 030 %
SPLITTING COMMUNICATIONS 031 %
SPLITTING COMMUNICATIONS 032 %
SPLITTING COMMUNICATIONS 033 %
SPLITTING COMMUNICATIONS 034 %
SPLITTING COMMUNICATIONS 035 %
SPLITTING COMMUNICATIONS 036 %
SPLITTING COMMUNICATIONS 037 %
SPLITTING COMMUNICATIONS 038 %
SPLITTING COMMUNICATIONS 039 %
SPLITTING COMMUNICATIONS 040 %
SPLITTING COMMUNICATIONS 041 %
SPLITTING COMMUNICATIONS 042 %
SPLITTING COMMUNICATIONS 043 %
SPLITTING COMMUNICATIONS 044 %
SPLITTING COMMUNICATIONS 045 %
SPLITTING COMMUNICATIONS 046 %
SPLITTING COMMUNICATIONS 047 %
SPLITTING COMMUNICATIONS 048 %
SPLITTING COMMUNICATIONS 049 %
SPLITTING COMMUNICATIONS 050 %
SPLITTING COMMUNICATIONS 051 %
SPLITTING COMMUNICATIONS 052 %
SPLITTING COMMUNICATIONS 053 %
SPLITTING COMMUNICATIONS 054 %
SPLITTING COMMUNICATIONS 055 %
SPLITTING COMMUNICATIONS 056 %
SPLITTING COMMUNICATIONS 057 %
SPLITTING COMMUNICATIONS 058 %
SPLITTING COMMUNICATIONS 059 %
SPLITTING COMMUNICATIONS 060 %
SPLITTING COMMUNICATIONS 061 %
SPLITTING COMMUNICATIONS 062 %
SPLITTING COMMUNICATIONS 063 %
SPLITTING COMMUNICATIONS 064 %
SPLITTING COMMUNICATIONS 065 %
SPLITTING COMMUNICATIONS 066 %
SPLITTING COMMUNICATIONS 067 %
SPLITTING COMMUNICATIONS 068 %
SPLITTING COMMUNICATIONS 069 %
SPLITTING COMMUNICATIONS 070 %
SPLITTING COMMUNICATIONS 071 %
SPLITTING COMMUNICATIONS 072 %
SPLITTING COMMUNICATIONS 073 %
SPLITTING COMMUNICATIONS 074 %
SPLITTING COMMUNICATIONS 075 %
SPLITTING COMMUNICATIONS 076 %
SPLITTING COMMUNICATIONS 077 %
SPLITTING COMMUNICATIONS 078 %
SPLITTING COMMUNICATIONS 079 %
SPLITTING COMMUNICATIONS 080 %
SPLITTING COMMUNICATIONS 081 %
SPLITTING COMMUNICATIONS 082 %
SPLITTING COMMUNICATIONS 083 %
SPLITTING COMMUNICATIONS 084 %
SPLITTING COMMUNICATIONS 085 %
SPLITTING COMMUNICATIONS 086 %
SPLITTING COMMUNICATIONS 087 %
SPLITTING COMMUNICATIONS 088 %
SPLITTING COMMUNICATIONS 089 %
SPLITTING COMMUNICATIONS 090 %
SPLITTING COMMUNICATIONS 091 %
SPLITTING COMMUNICATIONS 092 %
SPLITTING COMMUNICATIONS 093 %
SPLITTING COMMUNICATIONS 094 %
SPLITTING COMMUNICATIONS 095 %
SPLITTING COMMUNICATIONS 096 %
SPLITTING COMMUNICATIONS 097 %
SPLITTING COMMUNICATIONS 098 %
SPLITTING COMMUNICATIONS 099 %
SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          210
   * Splitted communications 30
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK

CREATING TRANSLATION STRUCTURES  001/010
CREATING TRANSLATION STRUCTURES  002/010
CREATING TRANSLATION STRUCTURES  003/010
CREATING TRANSLATION STRUCTURES  004/010
CREATING TRANSLATION STRUCTURES  005/010
CREATING TRANSLATION STRUCTURES  006/010
CREATING TRANSLATION STRUCTURES  007/010
CREATING TRANSLATION STRUCTURES  008/010
CREATING TRANSLATION STRUCTURES  009/010
CREATING TRANSLATION STRUCTURES  010/010
CREATING TRANSLATION STRUCTURES  010/010
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION

TRANSLATING RECORDS 000 %
TRANSLATING RECORDS 002 %
TRANSLATING RECORDS 003 %
TRANSLATING RECORDS 004 %
TRANSLATING RECORDS 005 %
TRANSLATING RECORDS 006 %
TRANSLATING RECORDS 007 %
TRANSLATING RECORDS 008 %
TRANSLATING RECORDS 009 %
TRANSLATING RECORDS 010 %
TRANSLATING RECORDS 011 %
TRANSLATING RECORDS 012 %
TRANSLATING RECORDS 013 %
TRANSLATING RECORDS 014 %
TRANSLATING RECORDS 015 %
TRANSLATING RECORDS 016 %
TRANSLATING RECORDS 017 %
TRANSLATING RECORDS 018 %
TRANSLATING RECORDS 019 %
TRANSLATING RECORDS 020 %
TRANSLATING RECORDS 021 %
TRANSLATING RECORDS 022 %
TRANSLATING RECORDS 023 %
TRANSLATING RECORDS 024 %
TRANSLATING RECORDS 025 %
TRANSLATING RECORDS 026 %
TRANSLATING RECORDS 027 %
TRANSLATING RECORDS 028 %
TRANSLATING RECORDS 029 %
TRANSLATING RECORDS 030 %
TRANSLATING RECORDS 031 %
TRANSLATING RECORDS 032 %
TRANSLATING RECORDS 033 %
TRANSLATING RECORDS 034 %
TRANSLATING RECORDS 035 %
TRANSLATING RECORDS 036 %
TRANSLATING RECORDS 037 %
TRANSLATING RECORDS 038 %
TRANSLATING RECORDS 039 %
TRANSLATING RECORDS 040 %
TRANSLATING RECORDS 041 %
TRANSLATING RECORDS 042 %
TRANSLATING RECORDS 043 %
TRANSLATING RECORDS 044 %
TRANSLATING RECORDS 045 %
TRANSLATING RECORDS 046 %
TRANSLATING RECORDS 047 %
TRANSLATING RECORDS 048 %
TRANSLATING RECORDS 049 %
TRANSLATING RECORDS 050 %
TRANSLATING RECORDS 051 %
TRANSLATING RECORDS 052 %
TRANSLATING RECORDS 053 %
TRANSLATING RECORDS 054 %
TRANSLATING RECORDS 055 %
TRANSLATING RECORDS 056 %
TRANSLATING RECORDS 057 %
TRANSLATING RECORDS 058 %
TRANSLATING RECORDS 059 %
TRANSLATING RECORDS 060 %
TRANSLATING RECORDS 061 %
TRANSLATING RECORDS 062 %
TRANSLATING RECORDS 063 %
TRANSLATING RECORDS 064 %
TRANSLATING RECORDS 065 %
TRANSLATING RECORDS 066 %
TRANSLATING RECORDS 067 %
TRANSLATING RECORDS 069 %
TRANSLATING RECORDS 070 %
TRANSLATING RECORDS 071 %
TRANSLATING RECORDS 072 %
TRANSLATING RECORDS 073 %
TRANSLATING RECORDS 074 %
TRANSLATING RECORDS 075 %
TRANSLATING RECORDS 076 %
TRANSLATING RECORDS 077 %
TRANSLATING RECORDS 078 %
TRANSLATING RECORDS 079 %
TRANSLATING RECORDS 080 %
TRANSLATING RECORDS 081 %
TRANSLATING RECORDS 082 %
TRANSLATING RECORDS 083 %
TRANSLATING RECORDS 084 %
TRANSLATING RECORDS 085 %
TRANSLATING RECORDS 086 %
TRANSLATING RECORDS 087 %
TRANSLATING RECORDS 088 %
TRANSLATING RECORDS 089 %
TRANSLATING RECORDS 090 %
TRANSLATING RECORDS 091 %
TRANSLATING RECORDS 092 %
TRANSLATING RECORDS 093 %
TRANSLATING RECORDS 094 %
TRANSLATING RECORDS 095 %
TRANSLATING RECORDS 096 %
TRANSLATING RECORDS 097 %
TRANSLATING RECORDS 098 %
TRANSLATING RECORDS 099 %
TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES

   * Merging task    1 100 %
   * Merging task    2 100 %
   * Merging task    3 100 %
   * Merging task    4 100 %
   * Merging task    5 100 %
   * Merging task    6 100 %
   * Merging task    7 100 %
   * Merging task    8 100 %
   * Merging task    9 100 %
   * Merging task   10 100 %   * All task merged!         

********************************************************************************
 *                               WARNING                                        *
********************************************************************************
5 tasks of this application execute 'non-deterministic' communications 
primitives (MPI_Test[*] | MPI_Waitany | MPI_Waitall | MPI_Waitsome)
The simulation of this trace will not capture the possible indeterminism 
********************************************************************************

TRANSLATION FINISHED
GENERATING PCF
   * Input PCF /home/schnorr/svn/bozzetti/paraver_traces2/sendrecv4.pcf correctly copied to sendrecv4.pcf
COPYING ROW FILE
#+end_example

Great, it works.

Dimemas CLI simulator:

#+begin_src sh :results output :session :exports both
cd ~/install/dimemas-5.2.12/
./bin/Dimemas -h
#+end_src

#+RESULTS:
#+begin_example
Dimemas version 5.2.12

Compiled on Mon Feb 15 07:27:46 BRST 2016 
Usage: ./bin/Dimemas [-h] [-v] [-d] [-x[s|e|p]] [-o[l] output-file] [-T time] [-l] [-C]
	[-p[a|b] paraver-file [-y time] -z time]] [-x[s|e]]
	[-e event_type] [-g event_output_info] [-F] [-S sync_size]
	[-w] [-ES] [-Eo eee_network_definition] [-Ef eee_frame_size]
	[--dim input-trace] [--bw bandwidth] [--lat latency]
	[--ppn processors_per_node] [--fill] [--interlvd]
	 config-file

Required arguments:
	config-file	Dimemas configuration file

Supported options:
	-h		Display this help message
	-v		Display Dimemas version information
	-d		Enable debug output
	-xa		Force assertations - Vladimir: check if optimizations caused some errors
	-xs		Enable extra scheduling debug output
	-xe		Enable extra event manager debug output
	-xp		Enable extra Paraver trace generation debug output
	-o[l] file	Set output file (default: stdout) (l:long info) 
	-t		Show only simulation time as output
	-T time		Set simulation stop time
	-l		Enable in-core operation
	-C		Perform critical path analysis
	-p file		Generate Paraver tracefile (ASCII)
	-pc file	Use the given file as output Paraver configuration file
	-y time		Set Paraver tracefile start time
	-z time		Set Paraver tracefile stop time
	-e event_type	Show time distance between events occurrence
	-g event_output	File for output information on events occurrence
	-F		Ignore synchronism send trace field
	-S sync_size	Minimum message size to use Rendez vous
	-w	When generating Paraver trace, output the LOGICAL_RECV times when Wait primitives take place (Default: at IRecv execution time)
	-ES	Enables the EEE network model (you must use '-Eo' and '-Ef'
	-Eo eee_network_definition	Set the filename where the EEE network is defined
	-Ef frame_size	Sets the EEE network frame size (in bytes)
	--dim input-trace	Set input trace (overrides the configuration file)
	--bw  bandwidth	Set inter-node bandwidth (MBps, overrides the configuration file)
	--lat latency	Set inter-node latency for all nodes (seconds, overrides the configuration file)
	--fill	Set node filling task mapping (overrides the configuration file)
	--ppn tasks_per_node	Set 'n' tasks per node mapping (overrides the configuration file)
	--interlvd	Set interleaved node tasks mapping (overrides the configuration file)
#+end_example

Now I have to find out how to use Dimemas.

Looks like a configuration file is essential.

There is some interesting tutorials here:
+ https://www.bsc.es/computer-sciences/performance-tools/documentation

More specifically these ones:
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/2.introduction_to_dimemas.tar.gz
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/3.introduction_to_paraver_and_dimemas_methodology.tar.gz
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/4.methodology_of_analysis.tar.gz

User-guide is here:
+ https://www.bsc.es/media/1324.pdf

Download all this:

#+begin_src sh :results output :session :exports both
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/2.introduction_to_dimemas.tar.gz
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/3.introduction_to_paraver_and_dimemas_methodology.tar.gz
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/4.methodology_of_analysis.tar.gz
wget -q https://www.bsc.es/media/1324.pdf
#+end_src

#+RESULTS:

* 2016-03-29 Plan for next weeks                                      :Lucas:

Here's what I think is missing from the technical side:

- A converter from paraver to =pj_dump=-like. I think Arnaud already did
  that, but we should merge what he did with the latest changes I have
  made in the =prv2tit.pl= script.
- Perhaps some performance improvements in the conversion script
  - Dump rightaway whenever information is complete, release memory
  - Today that's not the case, we buffer everything in memory (slow)
  - This is not strictly necessary (focus on next experimental part)

Them, for the experiments (from a non-partial paraver trace file):

1. _Replay with Dimemas_, get the trace from the replay (in prv format)
   - Understand how Dimemas model the Marenostrum machine
   - Get a complete trace from Barcelonians to start working with
   - Use =prv2pj.pl= script to convert paraver files to CSV (for comparison)
     #+begin_src sh :results output :session :exports both
     perl ./prv2pj.pl -f csv -i  ./paraver_traces2/sendrecv1
     #+end_src
2. _Replay with SimGrid_, using the tit file from original paraver
   - Create a Marenostrum platform file for Simgrid
   - Convert original paraver file to tit with =prv2tit.pl=
   - Get the trace, convert with =pj_dump= to the CSV-like
3. _Find a comparison metric_
   - It might be a simple gantt-chart with ggplot2
   - Then evolve proposing a metric (should discuss this with Arnaud)

You might find some information here.
- http://simgrid.gforge.inria.fr/contrib/smpi-paraver.php
* TODO 2016-03-29 Replaying with Dimemas to get a CSV for comparison

- How Dimemas CLI works?
- Receipt to replay with dimemas
  - With a step by step to get the CSV

* TODO 2016-03-29 Replaying with SimGrid to get a CSV for comparison

- Get inspiration from =smpi2pj.sh=
* 2016-03-29 Mercier's traces                                         :Lucas:

#+begin_src sh :results output :session :exports both
ls mercier_traces | grep .tar.gz
#+end_src

#+RESULTS:
: lu.B.2_prv_24MB.tar.gz
: lu.C.8_prv_377MB.tar.gz

Download here:

https://www.dropbox.com/sh/hnr7rhytapkuwtb/AACvpzo3MvHHm8LkAfklp61Ya?dl=0
* 2016-03-30 From the ground up

In order to install all the tools needed for this project,
we start by installing SimGrid.

#+BEGIN_SRC sh
cd simgrid
wget https://gforge.inria.fr/frs/download.php/file/35215/SimGrid-3.12.tar.gz
tar -xvzf SimGrid-3.12.tar.gz
cd SimGrid-3.12
cmake -Denable_smpi=on ./
make
make install
#+END_SRC
 
SimGrid is installed. We can try the smpi replay tool with one tit trace to check if the
simulation is running fine. The script that is executed separate the tit trace into files
for each task and creates a task mapping for an example machine model =graphene.xml=.

#+BEGIN_SRC sh
cd ..
cd examples
sh simulator.sh
#+END_SRC

This should output a file out.trace in the PAJE format.

Next we will install Dimemas. I created a directory called dimemas to contain the dimemas installation
and some examples. On that directory, copy the Dimemas source and execute the following commands.

#+BEGIN_SRC sh
tar jxf dimemas-5.2.12.tar.bz2
cd dimemas-5.2.12
./configure
make
make install
#+END_SRC

Now let`s try some examples.
First, we will convert a prv trace to the dimemas format.

#+BEGIN_SRC sh
cd ..
cd examples
./../dimemas-5.2.12/prv2dim/prv2dim sendrecv1.prv sendrecv1.dim
#+END_SRC

We should have a file named sendrecv1.dim with the trace in the dimemas format.
Now, we will simulate this trace on Dimemas using an example configuration file.

#+BEGIN_SRC sh
./../dimemas-5.2.12/Simulator/Dimemas -p out.prv 2dc_I_L2mr.cfg
#+END_SRC

Dimemas will create three files with the result of the simulation.

* 2014-04-03 Closing the cycle

So we got Simgrid and Dimemas working and we have some idea of how to use them.
It would be nice to have the simulation results in the same format for comparison.

Let`s start with Simgrid.
We will use the =pj_dump= command available in the PajeNG trace visualization tool.
We will start by installing this tool.

#+BEGIN_SRC sh
sudo apt-get install git cmake build-essential libqt4-dev libqt4-opengl-dev libboost-dev freeglut3-dev asciidoc flex bison;
git clone git://github.com/schnorr/pajeng.git ; mkdir -p pajeng/b ; cd pajeng/b ; cmake .. ; make install
#+END_SRC

Pjdump should be installed, let`s try it out.

#+BEGIN_SRC sh
cd ../../examples
sh simulator
pj_dump out.trace > out.csv
#+END_SRC

Great, we have our Simgrid output in a csv format.

We have to to the same with Dimemas output. First, we will install the prv2pjdump tool,
which will convert the prv file to a csv file in the pjdump format. Then, we will test it
with the prv file that the Dimemas simulator generated earlier.

#+BEGIN_SRC sh
cd ../../dimemas
git clone https://github.com/soctrace-inria/prv2pjdump.git
cd prv2pjdump
make
cd ../examples
./../prv2pjdump/linux_x64/prv2pjdump out.prv
#+END_SRC

The file out.pjdump should contain the result of our conversion.

* 2016-04-07 Trying to replicate latest entries                 :Lucas:Tiago:

_SimGrid_

#+begin_src sh :results output :session :exports both
cd simgrid; rm -rf SimGrid-3.12;
wget https://gforge.inria.fr/frs/download.php/file/35215/SimGrid-3.12.tar.gz
tar xzf SimGrid-3.12.tar.gz; cd SimGrid-3.12; mkdir build; cd build
cmake -Denable_smpi=ON -Denable_tracing=ON -Denable_java=OFF -Denable_documentation=OFF ..
make -j 4
#+end_src

#+RESULTS:
#+begin_example
-- Cmake version 3.4
-- The C compiler identification is GNU 5.3.1
-- The CXX compiler identification is GNU 5.3.1
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test HAVE_C_STACK_CLEANER
-- Performing Test HAVE_C_STACK_CLEANER - Failed
-- Performing Test COMPILER_SUPPORTS_CXX11
-- Performing Test COMPILER_SUPPORTS_CXX11 - Success
-- Performing Test COMPILER_SUPPORTS_C11
-- Performing Test COMPILER_SUPPORTS_C11 - Success
-- Found Perl: /usr/bin/perl (found version "5.22.1") 
-- Looking for sys/types.h
-- Looking for sys/types.h - found
-- Looking for stdint.h
-- Looking for stdint.h - found
-- Looking for stddef.h
-- Looking for stddef.h - found
-- Check size of int
-- Check size of int - done
-- Check size of long
-- Check size of long - done
-- Check size of long long
-- Check size of long long - done
-- Check size of void*
-- Check size of void* - done
-- System processor: x86_64 (x86_64, 64 bits)
-- Check if the system is big endian
-- Searching 16 bit integer
-- Check size of unsigned short
-- Check size of unsigned short - done
-- Using unsigned short
-- Check if the system is big endian - little endian
-- Looking for agraph.h
-- Looking for agraph.h - not found
-- Looking for cgraph.h
-- Looking for cgraph.h - found
-- Looking for graph.h
-- Looking for graph.h - not found
-- Looking for lib agraph
-- Looking for lib agraph - not found
-- Looking for lib cgraph
-- Looking for lib cgraph - found
-- Looking for lib graph
-- Looking for lib graph - not found
-- Looking for lib cdt
-- Looking for lib cdt - found
-- Looking for sigc++/sigc++.h
-- Looking for sigc++/sigc++.h - not found
-- Looking for sigc++config.h
-- Looking for sigc++config.h - not found
-- Looking for libsigc++
-- Looking for libsigc++ - not found
-- Boost version: 1.58.0
-- Boost version: 1.58.0
-- Found the following Boost libraries:
--   context
-- Looking for dlopen in dl
-- Looking for dlopen in dl - found
-- Looking for backtrace in execinfo
-- Looking for backtrace in execinfo - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Looking for sem_init in pthread
-- Looking for sem_init in pthread - found
-- Looking for sem_open in pthread
-- Looking for sem_open in pthread - found
-- Looking for sem_timedwait in pthread
-- Looking for sem_timedwait in pthread - found
-- Looking for pthread_mutex_timedlock in pthread
-- Looking for pthread_mutex_timedlock in pthread - found
-- Looking for clock_gettime in rt
-- Looking for clock_gettime in rt - found
-- Looking for 4 include files stdlib.h, ..., float.h
-- Looking for 4 include files stdlib.h, ..., float.h - found
-- Looking for valgrind/valgrind.h
-- Looking for valgrind/valgrind.h - found
-- Looking for socket.h
-- Looking for socket.h - not found
-- Looking for stat.h
-- Looking for stat.h - not found
-- Looking for sys/stat.h
-- Looking for sys/stat.h - found
-- Looking for windows.h
-- Looking for windows.h - not found
-- Looking for errno.h
-- Looking for errno.h - found
-- Looking for unistd.h
-- Looking for unistd.h - found
-- Looking for execinfo.h
-- Looking for execinfo.h - found
-- Looking for signal.h
-- Looking for signal.h - found
-- Looking for sys/time.h
-- Looking for sys/time.h - found
-- Looking for sys/param.h
-- Looking for sys/param.h - found
-- Looking for sys/sysctl.h
-- Looking for sys/sysctl.h - found
-- Looking for time.h
-- Looking for time.h - found
-- Looking for string.h
-- Looking for string.h - found
-- Looking for ucontext.h
-- Looking for ucontext.h - found
-- Looking for stdio.h
-- Looking for stdio.h - found
-- Looking for linux/futex.h
-- Looking for linux/futex.h - found
-- Looking for gettimeofday
-- Looking for gettimeofday - found
-- Looking for nanosleep
-- Looking for nanosleep - found
-- Looking for getdtablesize
-- Looking for getdtablesize - found
-- Looking for sysconf
-- Looking for sysconf - found
-- Looking for readv
-- Looking for readv - found
-- Looking for popen
-- Looking for popen - found
-- Looking for signal
-- Looking for signal - found
-- Looking for snprintf
-- Looking for snprintf - found
-- Looking for vsnprintf
-- Looking for vsnprintf - found
-- Looking for asprintf
-- Looking for asprintf - found
-- Looking for vasprintf
-- Looking for vasprintf - found
-- Looking for makecontext
-- Looking for makecontext - found
-- Looking for process_vm_readv
-- Looking for process_vm_readv - found
-- Looking for mmap
-- Looking for mmap - found
-- Looking for bin gfortran
-- Found gfortran: /usr/bin/gfortran
-- Looking for dlfcn.h
-- Looking for dlfcn.h - found
-- We are using GNU dynamic linker
-- sem_open is compilable
-- sem_open is executable
-- sem_init is compilable
-- sem_init is executable
-- timedwait is compilable
-- timedlock is compilable
-- #define pth_skaddr_makecontext(skaddr,sksize) ((skaddr))
-- #define pth_sksize_makecontext(skaddr,sksize) ((sksize))
-- Found Doxygen: /usr/bin/doxygen (found version "1.8.11") 
-- Doxygen version: 1.8.11
-- Configuring done
-- Generating done
-- Build files have been written to: /home/schnorr/svn/bozzetti/simgrid/SimGrid-3.12/build
#+end_example

_Dimemas_

- Download it manually (version 5.2.12) and put it in =dimemas= dir.

#+begin_src sh :results output :session :exports both
cd dimemas; rm -rf dimemas-5.2.12
tar xjf dimemas-5.2.12.tar.bz2
cd dimemas-5.2.12
./configure 2>&1 > x
head x
#+end_src

#+RESULTS:
#+begin_example
configure: error: invalid value: boost_major_version=
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking target system type... x86_64-unknown-linux-gnu
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether configure should try to set CFLAGS... yes
#+end_example

I have problem with boost, let's try to solve it.


* 2016-04-07 Let's take a look into the Dimemas and SimGrid traces :Lucas:Tiago:

We have two files that we want to analyze

#+begin_src sh :results output :session :exports both
find simgrid dimemas | grep pjdump$
#+end_src

#+RESULTS:
: simgrid/examples/out-simgrid.pjdump
: dimemas/examples/out-dimemas.pjdump

Let's prepare them to load into R.

#+begin_src sh :results output :session :exports both
cat simgrid/examples/out-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > simgrid/examples/out-simgrid-filter.pjdump
cat simgrid/examples/out-simgrid-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 3, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 3, 0.000051, 0.000117, 0.000066, action_recv
 3, 0.000126, 0.000128, 0.000002, smpi_replay_run_finalize
 2, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 2, 0.000052, 0.000052, 0.000000, action_send
 2, 0.000061, 0.000128, 0.000067, smpi_replay_run_finalize
 1, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 1, 0.000054, 0.000123, 0.000069, action_recv
 1, 0.000128, 0.000128, 0.000000, smpi_replay_run_finalize
 0, 0.000000, 0.000000, 0.000000, smpi_replay_run_init
 0, 0.000058, 0.000058, 0.000000, action_send
 0, 0.000063, 0.000128, 0.000065, smpi_replay_run_finalize
#+end_example

Let's look into the Dimemas converted trace file.

#+begin_src sh :results output :session :exports both
cat dimemas/examples/out-dimemas.pjdump | grep ^State | cut -d, -f2,4-6,8 | sed -e "s/THREAD 1\.//" -e "s/\.1,/,/" -e "s/\t//" -e "s/1R/R/" -e "s/4B/B/" > dimemas/examples/out-dimemas-filter.pjdump
cat dimemas/examples/out-dimemas-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
1,0,3,3,Running
2,0,3,3,Running
2,0,0,0,Running
3,0,3,3,Running
3,0,0,0,Running
4,0,3,3,Running
4,0,0,0,Running
1,3,4,1,Running
1,3,3,0,Running
1,3,3,0,Running
1,3,3,0,Running
2,3,3,0,Blocked
2,3,3,0,a message
2,3,4,1,Running
2,3,3,0,Running
2,3,3,0,Running
2,3,3,0,Running
2,3,3,0,Running
3,3,3,0,Blocked
3,3,4,1,Running
3,3,3,0,Running
3,3,3,0,Running
3,3,3,0,Running
3,3,3,0,Running
4,3,3,0,Blocked
4,3,3,0,a message
4,3,4,1,Running
4,3,3,0,Running
4,3,3,0,Running
4,3,3,0,Running
4,3,3,0,Running
1,4,5,1,created
1,4,4,0,Running
1,4,4,0,Running
1,4,4,0,Running
1,4,4,0,Running
2,4,5,1,created
2,4,4,0,Running
2,4,4,0,Running
2,4,4,0,Running
3,4,5,1,created
3,4,4,0,Running
3,4,4,0,Running
3,4,4,0,Running
4,4,5,1,created
4,4,4,0,Running
4,4,4,0,Running
4,4,4,0,Running
#+end_example

Great, let's load them into R.

#+begin_src R :results output :session :exports both
df_s <- read.csv("simgrid/examples/out-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";


df_d <- read.csv("dimemas/examples/out-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";


df <- rbind(df_s, df_d);
df
#+end_src

#+RESULTS:
#+begin_example
   Thread    Start      End Duration                     State Simulator
1       3 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
2       3 0.000051 0.000117  6.6e-05               action_recv   simgrid
3       3 0.000126 0.000128  2.0e-06  smpi_replay_run_finalize   simgrid
4       2 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
5       2 0.000052 0.000052  0.0e+00               action_send   simgrid
6       2 0.000061 0.000128  6.7e-05  smpi_replay_run_finalize   simgrid
7       1 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
8       1 0.000054 0.000123  6.9e-05               action_recv   simgrid
9       1 0.000128 0.000128  0.0e+00  smpi_replay_run_finalize   simgrid
10      0 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
11      0 0.000058 0.000058  0.0e+00               action_send   simgrid
12      0 0.000063 0.000128  6.5e-05  smpi_replay_run_finalize   simgrid
13      1 0.000000 3.000000  3.0e+00                   Running   dimemas
14      2 0.000000 3.000000  3.0e+00                   Running   dimemas
15      2 0.000000 0.000000  0.0e+00                   Running   dimemas
16      3 0.000000 3.000000  3.0e+00                   Running   dimemas
17      3 0.000000 0.000000  0.0e+00                   Running   dimemas
18      4 0.000000 3.000000  3.0e+00                   Running   dimemas
19      4 0.000000 0.000000  0.0e+00                   Running   dimemas
20      1 3.000000 4.000000  1.0e+00                   Running   dimemas
21      1 3.000000 3.000000  0.0e+00                   Running   dimemas
22      1 3.000000 3.000000  0.0e+00                   Running   dimemas
23      1 3.000000 3.000000  0.0e+00                   Running   dimemas
24      2 3.000000 3.000000  0.0e+00                   Blocked   dimemas
25      2 3.000000 3.000000  0.0e+00                 a message   dimemas
26      2 3.000000 4.000000  1.0e+00                   Running   dimemas
27      2 3.000000 3.000000  0.0e+00                   Running   dimemas
28      2 3.000000 3.000000  0.0e+00                   Running   dimemas
29      2 3.000000 3.000000  0.0e+00                   Running   dimemas
30      2 3.000000 3.000000  0.0e+00                   Running   dimemas
31      3 3.000000 3.000000  0.0e+00                   Blocked   dimemas
32      3 3.000000 4.000000  1.0e+00                   Running   dimemas
33      3 3.000000 3.000000  0.0e+00                   Running   dimemas
34      3 3.000000 3.000000  0.0e+00                   Running   dimemas
35      3 3.000000 3.000000  0.0e+00                   Running   dimemas
36      3 3.000000 3.000000  0.0e+00                   Running   dimemas
37      4 3.000000 3.000000  0.0e+00                   Blocked   dimemas
38      4 3.000000 3.000000  0.0e+00                 a message   dimemas
39      4 3.000000 4.000000  1.0e+00                   Running   dimemas
40      4 3.000000 3.000000  0.0e+00                   Running   dimemas
41      4 3.000000 3.000000  0.0e+00                   Running   dimemas
42      4 3.000000 3.000000  0.0e+00                   Running   dimemas
43      4 3.000000 3.000000  0.0e+00                   Running   dimemas
44      1 4.000000 5.000000  1.0e+00                   created   dimemas
45      1 4.000000 4.000000  0.0e+00                   Running   dimemas
46      1 4.000000 4.000000  0.0e+00                   Running   dimemas
47      1 4.000000 4.000000  0.0e+00                   Running   dimemas
48      1 4.000000 4.000000  0.0e+00                   Running   dimemas
49      2 4.000000 5.000000  1.0e+00                   created   dimemas
50      2 4.000000 4.000000  0.0e+00                   Running   dimemas
51      2 4.000000 4.000000  0.0e+00                   Running   dimemas
52      2 4.000000 4.000000  0.0e+00                   Running   dimemas
53      3 4.000000 5.000000  1.0e+00                   created   dimemas
54      3 4.000000 4.000000  0.0e+00                   Running   dimemas
55      3 4.000000 4.000000  0.0e+00                   Running   dimemas
56      3 4.000000 4.000000  0.0e+00                   Running   dimemas
57      4 4.000000 5.000000  1.0e+00                   created   dimemas
58      4 4.000000 4.000000  0.0e+00                   Running   dimemas
59      4 4.000000 4.000000  0.0e+00                   Running   dimemas
60      4 4.000000 4.000000  0.0e+00                   Running   dimemas
#+end_example

How to select part of this dataframe:

#+begin_src R :results output :session :exports both
df[df$Simulator == "simgrid",];
#+end_src

#+RESULTS:
#+begin_example
   Thread    Start      End Duration                     State Simulator
1       3 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
2       3 0.000051 0.000117  6.6e-05               action_recv   simgrid
3       3 0.000126 0.000128  2.0e-06  smpi_replay_run_finalize   simgrid
4       2 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
5       2 0.000052 0.000052  0.0e+00               action_send   simgrid
6       2 0.000061 0.000128  6.7e-05  smpi_replay_run_finalize   simgrid
7       1 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
8       1 0.000054 0.000123  6.9e-05               action_recv   simgrid
9       1 0.000128 0.000128  0.0e+00  smpi_replay_run_finalize   simgrid
10      0 0.000000 0.000000  0.0e+00      smpi_replay_run_init   simgrid
11      0 0.000058 0.000058  0.0e+00               action_send   simgrid
12      0 0.000063 0.000128  6.5e-05  smpi_replay_run_finalize   simgrid
#+end_example


Let's plot this with a space/time view.

Learn =ggplot2=:
- http://ggplot2.org/

How to install gpplot2:

#+begin_src R :results output :session :exports both
install.packages("ggplot2");
#+end_src

Let's plot only Simgrid.

#+begin_src R :results output graphics :file experiments/old/img/sendrecv-simgrid-0.png :exports both :width 600 :height 200 :session
library(ggplot2);
ggplot(df[df$Simulator == "simgrid",], aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4);
#+end_src

#+RESULTS:
[[file:experiments/old/img/sendrecv-simgrid-0.png]]

First problem detected with Simgrid:
- =action_send= is too fast, should simulator with higher precision
  - See below how to do it

#+begin_src sh :results output :session :exports both
/home/schnorr/workspace/simgrid.git/b/examples/smpi/smpi_replay --help | grep precision
#+end_src

#+RESULTS:
:    maxmin/precision: Numerical precision used when computing resource sharing (hence this value is expressed in ops/sec or bytes/sec)
:    surf/precision: Numerical precision used when updating simulation times (hence this value is expressed in seconds)
:    tracing/precision: Numerical precision used when timestamping events (hence this value is expressed in number of digits after decimal point)

So, you should run =smpi_replay= with =--cfg=tracing/precision:9=.

Let's get back to the plot.

#+begin_src R :results output graphics :file experiments/old/img/sendrecv-simgrid-1.png :exports both :width 600 :height 400 :session
library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1, scale="free_x");
#+end_src

#+RESULTS:
[[file:experiments/old/img/sendrecv-simgrid-1.png]]

The first four states are from simgrid, others from dimemas
(paraver). This is a problem because it makes the comparison
harder. We should check =prv2tit.pl= to try to convert from paraver
(generated by Dimemas) to CSV exporting MPI states instead of simple
high-level states such as =Blocked=, =a message=, =created=, and =Running=. We

I suggest Tiago to create a copy of current =prv2tit.pl= and simplify it
in order to only convert MPI states (and Running) in another perl
script called =prv2pjdump.pl=.

At the end, Tiago will have three facets in a single plot for each application:
- Real (behavior of the original paraver trace file)
- Dimemas (behavior of the trace file after Dimemas simulation)
- SimGrid (behavior of the trace file after SimGrid simulation based on TIT files)
* 2016-04-09 Paraver to Pjdump conversion

It was discussed in the last meeting that we need to convert the prv traces into the pjdump format
in a way that we can compare the result of the simulation using Simgrid and using Dimemas.
I took a look at the pjdump files generated by the =pj_dump= command and the one created by the prv2pjdump
tool we had installed early. It turns out that they are very different (considering the same original prv trace).
We created a perl script that does the conversion of the prv trace to the pjdump format trying to get a
similar output compared to the =pj_dump= command.
The script is very similar to the script that converts prv to tit traces. It is much simpler actually.
Here is how the conversion is made:
1. A container is created for the application.
2. A container is created for each task.
3. Each communication entry generates a Link entry (using the containers of the sending and receiving tasks).
4. A state is created for each prv state. We use the state name as the type and value. If the state is a MPI operation,
we use the name of the MPI call as the value instead.
5. An event is created for each event in the prv trace that is a MPI operation.

That`s it. Let`s try to convert a prv traces and see what the output looks like.
#+begin_src sh :results output :exports both
perl prv2pjdump.pl -i paraver_traces/sendrecv1
#+end_src

#+RESULTS:
#+begin_example
Container, 0, APP, 0, 0.004025387, 0.004025387, 0
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-0
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-1
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-2
Container, 0, TASK, 0, 0.004025387, 0.004025387, rank-3
Link, 0, LINK, 0.00348182, 0.003484691, 0.000002871, LINK, rank-2, rank-3
Link, 0, LINK, 0.003693602, 0.00369535, 0.000001748, LINK, rank-0, rank-1
State, rank-0, RUNNING, 0, 0.00316019, 0.00316019, 0, RUNNING
State, rank-1, NOT CREATED, 0, 0.00001743, 0.00001743, 0, NOT CREATED
State, rank-2, NOT CREATED, 0, 0.000157444, 0.000157444, 0, NOT CREATED
State, rank-3, NOT CREATED, 0, 0.000219304, 0.000219304, 0, NOT CREATED
State, rank-1, RUNNING, 0.00001743, 0.003124832, 0.003107402, 0, RUNNING
State, rank-2, RUNNING, 0.000157444, 0.003125267, 0.002967823, 0, RUNNING
State, rank-3, RUNNING, 0.000219304, 0.003125908, 0.002906604, 0, RUNNING
State, rank-1, OTHERS, 0.003124832, 0.003170029, 0.000045197, 0, MPI_INIT
State, rank-2, OTHERS, 0.003125267, 0.003169742, 0.000044475, 0, MPI_INIT
State, rank-3, OTHERS, 0.003125908, 0.003170472, 0.000044564, 0, MPI_INIT
State, rank-0, OTHERS, 0.00316019, 0.003170485, 0.000010295, 0, MPI_INIT
State, rank-2, RUNNING, 0.003169742, 0.003226001, 0.000056259, 0, RUNNING
State, rank-1, RUNNING, 0.003170029, 0.003225703, 0.000055674, 0, RUNNING
State, rank-3, RUNNING, 0.003170472, 0.003226223, 0.000055751, 0, RUNNING
State, rank-0, RUNNING, 0.003170485, 0.003363346, 0.000192861, 0, RUNNING
State, rank-1, OTHERS, 0.003225703, 0.003389114, 0.000163411, 0, OTHERS
State, rank-2, OTHERS, 0.003226001, 0.003388687, 0.000162686, 0, OTHERS
State, rank-3, OTHERS, 0.003226223, 0.003388393, 0.00016217, 0, OTHERS
State, rank-0, OTHERS, 0.003363346, 0.003483197, 0.000119851, 0, OTHERS
State, rank-3, RUNNING, 0.003388393, 0.003395288, 0.000006895, 0, RUNNING
State, rank-2, RUNNING, 0.003388687, 0.003395362, 0.000006675, 0, RUNNING
State, rank-1, RUNNING, 0.003389114, 0.003395381, 0.000006267, 0, RUNNING
State, rank-3, OTHERS, 0.003395288, 0.003432306, 0.000037018, 0, OTHERS
State, rank-2, OTHERS, 0.003395362, 0.003432702, 0.00003734, 0, OTHERS
State, rank-1, OTHERS, 0.003395381, 0.003432431, 0.00003705, 0, OTHERS
State, rank-3, RUNNING, 0.003432306, 0.003437348, 0.000005042, 0, RUNNING
State, rank-1, RUNNING, 0.003432431, 0.003437863, 0.000005432, 0, RUNNING
State, rank-2, RUNNING, 0.003432702, 0.003437722, 0.00000502, 0, RUNNING
State, rank-3, WAITING A MESSAGE, 0.003437348, 0.003484691, 0.000047343, 0, MPI_RECV
State, rank-2, BLOCKING SEND, 0.003437722, 0.00348182, 0.000044098, 0, MPI_SEND
State, rank-1, WAITING A MESSAGE, 0.003437863, 0.00369535, 0.000257487, 0, MPI_RECV
State, rank-2, RUNNING, 0.00348182, 0.00351402, 0.0000322, 0, RUNNING
State, rank-0, RUNNING, 0.003483197, 0.003488339, 0.000005142, 0, RUNNING
State, rank-3, RUNNING, 0.003484691, 0.003515642, 0.000030951, 0, RUNNING
State, rank-0, OTHERS, 0.003488339, 0.003645036, 0.000156697, 0, OTHERS
State, rank-2, OTHERS, 0.00351402, 0.003550242, 0.000036222, 0, MPI_FINALIZE
State, rank-3, OTHERS, 0.003515642, 0.00355024, 0.000034598, 0, MPI_FINALIZE
State, rank-3, RUNNING, 0.00355024, 0.004005538, 0.000455298, 0, RUNNING
State, rank-2, RUNNING, 0.003550242, 0.004004832, 0.00045459, 0, RUNNING
State, rank-0, RUNNING, 0.003645036, 0.003651735, 0.000006699, 0, RUNNING
State, rank-0, BLOCKING SEND, 0.003651735, 0.003693602, 0.000041867, 0, MPI_SEND
State, rank-0, RUNNING, 0.003693602, 0.003850525, 0.000156923, 0, RUNNING
State, rank-1, RUNNING, 0.00369535, 0.003724869, 0.000029519, 0, RUNNING
State, rank-1, OTHERS, 0.003724869, 0.003761334, 0.000036465, 0, MPI_FINALIZE
State, rank-1, RUNNING, 0.003761334, 0.004005498, 0.000244164, 0, RUNNING
State, rank-0, OTHERS, 0.003850525, 0.003887338, 0.000036813, 0, MPI_FINALIZE
State, rank-0, RUNNING, 0.003887338, 0.004004403, 0.000117065, 0, RUNNING
State, rank-0, I/O, 0.004004403, 0.004014528, 0.000010125, 0, I/O
State, rank-2, I/O, 0.004004832, 0.004016896, 0.000012064, 0, I/O
State, rank-1, I/O, 0.004005498, 0.004016864, 0.000011366, 0, I/O
State, rank-3, I/O, 0.004005538, 0.004016591, 0.000011053, 0, I/O
State, rank-0, RUNNING, 0.004014528, 0.00402323, 0.000008702, 0, RUNNING
State, rank-3, RUNNING, 0.004016591, 0.004023898, 0.000007307, 0, RUNNING
State, rank-1, RUNNING, 0.004016864, 0.004025387, 0.000008523, 0, RUNNING
State, rank-2, RUNNING, 0.004016896, 0.004025282, 0.000008386, 0, RUNNING
Event, rank-1, MPI_CALL, 0.003124832, MPI_INIT
Event, rank-2, MPI_CALL, 0.003125267, MPI_INIT
Event, rank-3, MPI_CALL, 0.003125908, MPI_INIT
Event, rank-0, MPI_CALL, 0.00316019, MPI_INIT
Event, rank-3, MPI_CALL, 0.003437348, MPI_RECV
Event, rank-2, MPI_CALL, 0.003437722, MPI_SEND
Event, rank-1, MPI_CALL, 0.003437863, MPI_RECV
Event, rank-2, MPI_CALL, 0.00351402, MPI_FINALIZE
Event, rank-3, MPI_CALL, 0.003515642, MPI_FINALIZE
Event, rank-0, MPI_CALL, 0.003651735, MPI_SEND
Event, rank-1, MPI_CALL, 0.003724869, MPI_FINALIZE
Event, rank-0, MPI_CALL, 0.003850525, MPI_FINALIZE
#+end_example

Next step is to execute the cycle using this new script and see how we can compare
the traces.



* 2016-04-10 Simulation time precision

As discussed in the last meeting, the duration of the MPI_Send state on the Simgrid trace was zero.
Also, the Dimemas trace contained very small time values for some reason. This made the comparison
of the Simgrid trace with the Dimemas trace very hard.
I took a look at the Dimemas configuration file used to execute the simulations and noted
a parameter named =speed_ratio_instrumented_vs_simulated=. I changed it to 1, so the computing speed
of the simulated app will be the same as the original trace.

On Simgrid, I adjusted the options in order to generate a pjdump trace with the same precision as the
trace generated by Dimemas (ns precision). Using the ==tacing/precision== option with the value of 9
  and using pj_dump option =--float-precision=9= will generate a pjdump trace with ns precision.

  However, these adjustments did not solve the MPI_Send state problem on the Simgrid trace. After looking
  at the Simgrid help command, we set two more options: =smpi/send_is_detached_thres= and =smpi/async_small_thres=.
  The first option, according to the documentation, is the threshold of message size where MPI_Send stops behaving
  like MPI_Isend and becomes MPI_Send. Since our application was using very small message sizes, we set this
  option to 2.
  The second option, according to the documentation, is the maximal size of messages that are to be sent
  asynchronously, without waiting for the receiver. We set this value to 0.

  Additionally, we used the ==tracing/smpi/computing= on =smpi_replay= so the trace will also show the computing
  states.

  Let`s see what the trace looks like.

#+begin_src sh :results output :exports both
  cd simgrid/examples/
  sh simulator.sh
  pj_dump --float-precision=9 out.trace
#+end_src

#+RESULTS:
#+begin_example
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing' to 'yes'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/filename' to 'smpi_simgrid.trace'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/smpi' to 'yes'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/TCP_gamma' to '4194304'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/smpi/computing' to 'yes'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/precision' to '9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/send_is_detached_thres' to '2'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/async_small_thres' to '0'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/cpu_threshold' to '-1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'tracing/filename' to 'out.trace'
[griffon-1.nancy.grid5000.fr:0:(0) 3.647679] [smpi_replay/INFO] Simulation time 0.487489
Container, 0, 0, 0, 3.64768, 3.64768, 0
Link, 0, MPI_LINK, 3.035777000, 3.035873744, 0.000096744, PTP, rank-2, rank-3
Link, 0, MPI_LINK, 3.364892000, 3.364988744, 0.000096744, PTP, rank-0, rank-1
Container, 0, MPI, 0, 3.64768, 3.64768, rank-3
State, rank-3, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-3, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-3, MPI_STATE, 0.000000000, 2.906604000, 2.906604000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.906604000, 2.962355000, 0.055751000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.962355000, 2.969250000, 0.006895000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.969250000, 2.974292000, 0.005042000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.974292000, 3.035873744, 0.061581744, 1.000000000, action_recv
State, rank-3, MPI_STATE, 3.035873744, 3.066824744, 0.030951000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.066824744, 3.522122744, 0.455298000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.522122744, 3.529429744, 0.007307000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.529429744, 3.647678744, 0.118249000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 3.64768, 3.64768, rank-2
State, rank-2, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-2, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-2, MPI_STATE, 0.000000000, 2.967823000, 2.967823000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.967823000, 3.024082000, 0.056259000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.024082000, 3.030757000, 0.006675000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.030757000, 3.035777000, 0.005020000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.035777000, 3.035873744, 0.000096744, 1.000000000, action_send
State, rank-2, MPI_STATE, 3.035873744, 3.068073744, 0.032200000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.068073744, 3.522663744, 0.454590000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.522663744, 3.531049744, 0.008386000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.531049744, 3.647678744, 0.116629000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 3.64768, 3.64768, rank-1
State, rank-1, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-1, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-1, MPI_STATE, 0.000000000, 3.107402000, 3.107402000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.107402000, 3.163076000, 0.055674000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.163076000, 3.169343000, 0.006267000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.169343000, 3.174775000, 0.005432000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.174775000, 3.364988744, 0.190213744, 1.000000000, action_recv
State, rank-1, MPI_STATE, 3.364988744, 3.394507744, 0.029519000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.394507744, 3.638671744, 0.244164000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.638671744, 3.647194744, 0.008523000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.647194744, 3.647678744, 0.000484000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 3.64768, 3.64768, rank-0
State, rank-0, MPI_STATE, 0.000000000, 3.647678744, 3.647678744, 0.000000000, computing
State, rank-0, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-0, MPI_STATE, 0.000000000, 3.160190000, 3.160190000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.160190000, 3.353051000, 0.192861000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.353051000, 3.358193000, 0.005142000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.358193000, 3.364892000, 0.006699000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.364892000, 3.364988744, 0.000096744, 1.000000000, action_send
State, rank-0, MPI_STATE, 3.364988744, 3.521911744, 0.156923000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.521911744, 3.638976744, 0.117065000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.638976744, 3.647678744, 0.008702000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.647678744, 3.647678744, 0.000000000, 1.000000000, smpi_replay_run_finalize
#+end_example

  We can see that the MPI_Send states last longer that "zero" and we got a nanosecond precision in our
  time values. However, there is a little problem: The first state of each task (computing state) starts
  at time "zero" (which is ok) but ends at the highest possible time. All the other states occur while
  the first state is still valid. To properly plot the data, we will have to remove this first state
  for each task.

* 2016-04-10 Comparison between the traces

Once again, we will perform the complete cycle to obtain a =pjdump= file for the Simgrid trace,
the Dimemas trace and the original trace.
We will be using the =paraver_traces/sendrecv1.prv= trace. Let`s begin.

#+begin_src sh :results output :exports both
  cd simgrid/examples/
  sh simulator.sh
  pj_dump --float-precision=9 out.trace > ../../simgrid.pjdump
  cd ../../dimemas/examples/
  ./../dimemas-5.2.12/prv2dim/prv2dim sendrecv1.prv sendrecv1.dim
  ./../dimemas-5.2.12/Simulator/Dimemas -p out.prv 2dc_I_L2mr.cfg
  perl ../../prv2pjdump.pl -i out > ../../dimemas.pjdump
  cd ../../
  perl prv2pjdump.pl -i paraver_traces/sendrecv1 > original.pjdump
#+end_src

Ok. We got our traces.

#+begin_src sh :results output :exports both
  ls *.pjdump
#+end_src

#+RESULTS:
: dimemas.pjdump
: original.pjdump
: simgrid.pjdump

Let`s prepare it to load into R

#+begin_src sh :results output :exports both
cat simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > simgrid-filter.pjdump
cat simgrid-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 3, 0.000000000, 3.647678744, 3.647678744, computing
 3, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 3, 0.000000000, 2.906604000, 2.906604000, computing
 3, 2.906604000, 2.962355000, 0.055751000, computing
 3, 2.962355000, 2.969250000, 0.006895000, computing
 3, 2.969250000, 2.974292000, 0.005042000, computing
 3, 2.974292000, 3.035873744, 0.061581744, action_recv
 3, 3.035873744, 3.066824744, 0.030951000, computing
 3, 3.066824744, 3.522122744, 0.455298000, computing
 3, 3.522122744, 3.529429744, 0.007307000, computing
 3, 3.529429744, 3.647678744, 0.118249000, smpi_replay_run_finalize
 2, 0.000000000, 3.647678744, 3.647678744, computing
 2, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 2, 0.000000000, 2.967823000, 2.967823000, computing
 2, 2.967823000, 3.024082000, 0.056259000, computing
 2, 3.024082000, 3.030757000, 0.006675000, computing
 2, 3.030757000, 3.035777000, 0.005020000, computing
 2, 3.035777000, 3.035873744, 0.000096744, action_send
 2, 3.035873744, 3.068073744, 0.032200000, computing
 2, 3.068073744, 3.522663744, 0.454590000, computing
 2, 3.522663744, 3.531049744, 0.008386000, computing
 2, 3.531049744, 3.647678744, 0.116629000, smpi_replay_run_finalize
 1, 0.000000000, 3.647678744, 3.647678744, computing
 1, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 1, 0.000000000, 3.107402000, 3.107402000, computing
 1, 3.107402000, 3.163076000, 0.055674000, computing
 1, 3.163076000, 3.169343000, 0.006267000, computing
 1, 3.169343000, 3.174775000, 0.005432000, computing
 1, 3.174775000, 3.364988744, 0.190213744, action_recv
 1, 3.364988744, 3.394507744, 0.029519000, computing
 1, 3.394507744, 3.638671744, 0.244164000, computing
 1, 3.638671744, 3.647194744, 0.008523000, computing
 1, 3.647194744, 3.647678744, 0.000484000, smpi_replay_run_finalize
 0, 0.000000000, 3.647678744, 3.647678744, computing
 0, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
 0, 0.000000000, 3.160190000, 3.160190000, computing
 0, 3.160190000, 3.353051000, 0.192861000, computing
 0, 3.353051000, 3.358193000, 0.005142000, computing
 0, 3.358193000, 3.364892000, 0.006699000, computing
 0, 3.364892000, 3.364988744, 0.000096744, action_send
 0, 3.364988744, 3.521911744, 0.156923000, computing
 0, 3.521911744, 3.638976744, 0.117065000, computing
 0, 3.638976744, 3.647678744, 0.008702000, computing
 0, 3.647678744, 3.647678744, 0.000000000, smpi_replay_run_finalize
#+end_example

As noted in the last entry on the Labbook, the first state of every task
should not be there. Let`s manually remove them. We hope to have an easier 
mechanism for doing this in the future.
This is what the file should look like:

 #+begin_src sh :results output :exports both
    cat simgrid-filter.pjdump
   #+end_src

 #+RESULTS:
 #+begin_example
  3, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  3, 0.000000000, 2.906604000, 2.906604000, computing
  3, 2.906604000, 2.962355000, 0.055751000, computing
  3, 2.962355000, 2.969250000, 0.006895000, computing
  3, 2.969250000, 2.974292000, 0.005042000, computing
  3, 2.974292000, 3.035873744, 0.061581744, action_recv
  3, 3.035873744, 3.066824744, 0.030951000, computing
  3, 3.066824744, 3.522122744, 0.455298000, computing
  3, 3.522122744, 3.529429744, 0.007307000, computing
  3, 3.529429744, 3.647678744, 0.118249000, smpi_replay_run_finalize
  2, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  2, 0.000000000, 2.967823000, 2.967823000, computing
  2, 2.967823000, 3.024082000, 0.056259000, computing
  2, 3.024082000, 3.030757000, 0.006675000, computing
  2, 3.030757000, 3.035777000, 0.005020000, computing
  2, 3.035777000, 3.035873744, 0.000096744, action_send
  2, 3.035873744, 3.068073744, 0.032200000, computing
  2, 3.068073744, 3.522663744, 0.454590000, computing
  2, 3.522663744, 3.531049744, 0.008386000, computing
  2, 3.531049744, 3.647678744, 0.116629000, smpi_replay_run_finalize
  1, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  1, 0.000000000, 3.107402000, 3.107402000, computing
  1, 3.107402000, 3.163076000, 0.055674000, computing
  1, 3.163076000, 3.169343000, 0.006267000, computing
  1, 3.169343000, 3.174775000, 0.005432000, computing
  1, 3.174775000, 3.364988744, 0.190213744, action_recv
  1, 3.364988744, 3.394507744, 0.029519000, computing
  1, 3.394507744, 3.638671744, 0.244164000, computing
  1, 3.638671744, 3.647194744, 0.008523000, computing
  1, 3.647194744, 3.647678744, 0.000484000, smpi_replay_run_finalize
  0, 0.000000000, 0.000000000, 0.000000000, smpi_replay_run_init
  0, 0.000000000, 3.160190000, 3.160190000, computing
  0, 3.160190000, 3.353051000, 0.192861000, computing
  0, 3.353051000, 3.358193000, 0.005142000, computing
  0, 3.358193000, 3.364892000, 0.006699000, computing
  0, 3.364892000, 3.364988744, 0.000096744, action_send
  0, 3.364988744, 3.521911744, 0.156923000, computing
  0, 3.521911744, 3.638976744, 0.117065000, computing
  0, 3.638976744, 3.647678744, 0.008702000, computing
  0, 3.647678744, 3.647678744, 0.000000000, smpi_replay_run_finalize
#+end_example


We have to to the same with the Dimemas pjdump

#+begin_src sh :results output :exports both
    cat dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > dimemas-filter.pjdump
    cat dimemas-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 0, 0, 0.00316019, 0.00316019, MPI_INIT
 1, 0, 0.00001743, 0.00001743, RUNNING
 2, 0, 0.000157444, 0.000157444, RUNNING
 3, 0, 0.000219304, 0.000219304, RUNNING
 1, 0.00001743, 0.003124832, 0.003107402, RUNNING
 2, 0.000157444, 0.003125267, 0.002967823, RUNNING
 3, 0.000219304, 0.003125908, 0.002906604, RUNNING
 1, 0.003124832, 0.00316019, 0.000035358, MPI_INIT
 2, 0.003125267, 0.00316019, 0.000034923, MPI_INIT
 3, 0.003125908, 0.00316019, 0.000034282, MPI_INIT
 0, 0.00316019, 0.003353051, 0.000192861, RUNNING
 1, 0.00316019, 0.003215864, 0.000055674, RUNNING
 2, 0.00316019, 0.003216449, 0.000056259, RUNNING
 3, 0.00316019, 0.003215941, 0.000055751, RUNNING
 1, 0.003215864, 0.003222131, 0.000006267, RUNNING
 3, 0.003215941, 0.003222836, 0.000006895, RUNNING
 2, 0.003216449, 0.003223124, 0.000006675, RUNNING
 1, 0.003222131, 0.003227563, 0.000005432, RUNNING
 3, 0.003222836, 0.003227878, 0.000005042, RUNNING
 2, 0.003223124, 0.003228144, 0.00000502, MPI_SEND
 1, 0.003227563, 0.003364892, 0.000137329, MPI_RECV
 3, 0.003227878, 0.003228144, 0.000000266, MPI_RECV
 2, 0.003228144, 0.003260344, 0.0000322, MPI_FINALIZE
 3, 0.003228144, 0.003259095, 0.000030951, MPI_FINALIZE
 3, 0.003259095, 0.003714393, 0.000455298, RUNNING
 2, 0.003260344, 0.003714934, 0.00045459, RUNNING
 0, 0.003353051, 0.003358193, 0.000005142, RUNNING
 0, 0.003358193, 0.003364892, 0.000006699, MPI_SEND
 0, 0.003364892, 0.003521815, 0.000156923, MPI_FINALIZE
 1, 0.003364892, 0.003394411, 0.000029519, MPI_FINALIZE
 1, 0.003394411, 0.003638575, 0.000244164, RUNNING
 0, 0.003521815, 0.00363888, 0.000117065, RUNNING
 1, 0.003638575, 0.003649941, 0.000011366, RUNNING
 0, 0.00363888, 0.003649005, 0.000010125, RUNNING
 0, 0.003649005, 0.003657463, 0.000008458, RUNNING
 1, 0.003649941, 0.003658186, 0.000008245, RUNNING
 0, 0.003657463, 0.003657707, 0.000000244, RUNNING
 0, 0.003657707, 0.003657708, 0.000000001, NOT CREATED
 1, 0.003658186, 0.003658464, 0.000000278, RUNNING
 1, 0.003658464, 0.003658465, 0.000000001, NOT CREATED
 3, 0.003714393, 0.003725446, 0.000011053, RUNNING
 2, 0.003714934, 0.003726998, 0.000012064, RUNNING
 3, 0.003725446, 0.003732523, 0.000007077, RUNNING
 2, 0.003726998, 0.003735078, 0.00000808, RUNNING
 3, 0.003732523, 0.003732753, 0.00000023, RUNNING
 3, 0.003732753, 0.003732754, 0.000000001, NOT CREATED
 2, 0.003735078, 0.003735384, 0.000000306, RUNNING
 2, 0.003735384, 0.003735385, 0.000000001, NOT CREATED
#+end_example

Now, the original trace


#+begin_src sh :results output :exports both
    cat original.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > original-filter.pjdump
cat original-filter.pjdump
#+end_src

#+RESULTS:
#+begin_example
 0, 0, 0.00316019, 0.00316019, RUNNING
 1, 0, 0.00001743, 0.00001743, NOT CREATED
 2, 0, 0.000157444, 0.000157444, NOT CREATED
 3, 0, 0.000219304, 0.000219304, NOT CREATED
 1, 0.00001743, 0.003124832, 0.003107402, RUNNING
 2, 0.000157444, 0.003125267, 0.002967823, RUNNING
 3, 0.000219304, 0.003125908, 0.002906604, RUNNING
 1, 0.003124832, 0.003170029, 0.000045197, MPI_INIT
 2, 0.003125267, 0.003169742, 0.000044475, MPI_INIT
 3, 0.003125908, 0.003170472, 0.000044564, MPI_INIT
 0, 0.00316019, 0.003170485, 0.000010295, MPI_INIT
 2, 0.003169742, 0.003226001, 0.000056259, RUNNING
 1, 0.003170029, 0.003225703, 0.000055674, RUNNING
 3, 0.003170472, 0.003226223, 0.000055751, RUNNING
 0, 0.003170485, 0.003363346, 0.000192861, RUNNING
 1, 0.003225703, 0.003389114, 0.000163411, OTHERS
 2, 0.003226001, 0.003388687, 0.000162686, OTHERS
 3, 0.003226223, 0.003388393, 0.00016217, OTHERS
 0, 0.003363346, 0.003483197, 0.000119851, OTHERS
 3, 0.003388393, 0.003395288, 0.000006895, RUNNING
 2, 0.003388687, 0.003395362, 0.000006675, RUNNING
 1, 0.003389114, 0.003395381, 0.000006267, RUNNING
 3, 0.003395288, 0.003432306, 0.000037018, OTHERS
 2, 0.003395362, 0.003432702, 0.00003734, OTHERS
 1, 0.003395381, 0.003432431, 0.00003705, OTHERS
 3, 0.003432306, 0.003437348, 0.000005042, RUNNING
 1, 0.003432431, 0.003437863, 0.000005432, RUNNING
 2, 0.003432702, 0.003437722, 0.00000502, RUNNING
 3, 0.003437348, 0.003484691, 0.000047343, MPI_RECV
 2, 0.003437722, 0.00348182, 0.000044098, MPI_SEND
 1, 0.003437863, 0.00369535, 0.000257487, MPI_RECV
 2, 0.00348182, 0.00351402, 0.0000322, RUNNING
 0, 0.003483197, 0.003488339, 0.000005142, RUNNING
 3, 0.003484691, 0.003515642, 0.000030951, RUNNING
 0, 0.003488339, 0.003645036, 0.000156697, OTHERS
 2, 0.00351402, 0.003550242, 0.000036222, MPI_FINALIZE
 3, 0.003515642, 0.00355024, 0.000034598, MPI_FINALIZE
 3, 0.00355024, 0.004005538, 0.000455298, RUNNING
 2, 0.003550242, 0.004004832, 0.00045459, RUNNING
 0, 0.003645036, 0.003651735, 0.000006699, RUNNING
 0, 0.003651735, 0.003693602, 0.000041867, MPI_SEND
 0, 0.003693602, 0.003850525, 0.000156923, RUNNING
 1, 0.00369535, 0.003724869, 0.000029519, RUNNING
 1, 0.003724869, 0.003761334, 0.000036465, MPI_FINALIZE
 1, 0.003761334, 0.004005498, 0.000244164, RUNNING
 0, 0.003850525, 0.003887338, 0.000036813, MPI_FINALIZE
 0, 0.003887338, 0.004004403, 0.000117065, RUNNING
 0, 0.004004403, 0.004014528, 0.000010125, I/O
 2, 0.004004832, 0.004016896, 0.000012064, I/O
 1, 0.004005498, 0.004016864, 0.000011366, I/O
 3, 0.004005538, 0.004016591, 0.000011053, I/O
 0, 0.004014528, 0.00402323, 0.000008702, RUNNING
 3, 0.004016591, 0.004023898, 0.000007307, RUNNING
 1, 0.004016864, 0.004025387, 0.000008523, RUNNING
 2, 0.004016896, 0.004025282, 0.000008386, RUNNING
#+end_example

Repeating the steps that we did before.. We will load this data into R.

#+begin_src R 
df_s <- read.csv("simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df_o <- read.csv("original-filter.pjdump", header=F, sep=",");
names(df_o) <- c("Thread", "Start", "End", "Duration", "State");
df_o$Simulator <- "original";

df <- rbind(df_s, df_d, df_o);
df
#+end_src

#+RESULTS:
| 3 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 3 |           0 |    2.906604 |    2.906604 | computing              | simgrid  |
| 3 |    2.906604 |    2.962355 |    0.055751 | computing              | simgrid  |
| 3 |    2.962355 |     2.96925 |    0.006895 | computing              | simgrid  |
| 3 |     2.96925 |    2.974292 |    0.005042 | computing              | simgrid  |
| 3 |    2.974292 | 3.035873744 | 0.061581744 | action_recv             | simgrid  |
| 3 | 3.035873744 | 3.066824744 |    0.030951 | computing              | simgrid  |
| 3 | 3.066824744 | 3.522122744 |    0.455298 | computing              | simgrid  |
| 3 | 3.522122744 | 3.529429744 |    0.007307 | computing              | simgrid  |
| 3 | 3.529429744 | 3.647678744 |    0.118249 | smpi_replay_run_finalize | simgrid  |
| 2 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 2 |           0 |    2.967823 |    2.967823 | computing              | simgrid  |
| 2 |    2.967823 |    3.024082 |    0.056259 | computing              | simgrid  |
| 2 |    3.024082 |    3.030757 |    0.006675 | computing              | simgrid  |
| 2 |    3.030757 |    3.035777 |     0.00502 | computing              | simgrid  |
| 2 |    3.035777 | 3.035873744 |  9.6744e-05 | action_send             | simgrid  |
| 2 | 3.035873744 | 3.068073744 |      0.0322 | computing              | simgrid  |
| 2 | 3.068073744 | 3.522663744 |     0.45459 | computing              | simgrid  |
| 2 | 3.522663744 | 3.531049744 |    0.008386 | computing              | simgrid  |
| 2 | 3.531049744 | 3.647678744 |    0.116629 | smpi_replay_run_finalize | simgrid  |
| 1 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 1 |           0 |    3.107402 |    3.107402 | computing              | simgrid  |
| 1 |    3.107402 |    3.163076 |    0.055674 | computing              | simgrid  |
| 1 |    3.163076 |    3.169343 |    0.006267 | computing              | simgrid  |
| 1 |    3.169343 |    3.174775 |    0.005432 | computing              | simgrid  |
| 1 |    3.174775 | 3.364988744 | 0.190213744 | action_recv             | simgrid  |
| 1 | 3.364988744 | 3.394507744 |    0.029519 | computing              | simgrid  |
| 1 | 3.394507744 | 3.638671744 |    0.244164 | computing              | simgrid  |
| 1 | 3.638671744 | 3.647194744 |    0.008523 | computing              | simgrid  |
| 1 | 3.647194744 | 3.647678744 |    0.000484 | smpi_replay_run_finalize | simgrid  |
| 0 |           0 |           0 |           0 | smpi_replay_run_init     | simgrid  |
| 0 |           0 |     3.16019 |     3.16019 | computing              | simgrid  |
| 0 |     3.16019 |    3.353051 |    0.192861 | computing              | simgrid  |
| 0 |    3.353051 |    3.358193 |    0.005142 | computing              | simgrid  |
| 0 |    3.358193 |    3.364892 |    0.006699 | computing              | simgrid  |
| 0 |    3.364892 | 3.364988744 |  9.6744e-05 | action_send             | simgrid  |
| 0 | 3.364988744 | 3.521911744 |    0.156923 | computing              | simgrid  |
| 0 | 3.521911744 | 3.638976744 |    0.117065 | computing              | simgrid  |
| 0 | 3.638976744 | 3.647678744 |    0.008702 | computing              | simgrid  |
| 0 | 3.647678744 | 3.647678744 |           0 | smpi_replay_run_finalize | simgrid  |
| 0 |           0 |  0.00316019 |  0.00316019 | MPI_INIT                | dimemas  |
| 1 |           0 |   1.743e-05 |   1.743e-05 | RUNNING                | dimemas  |
| 2 |           0 | 0.000157444 | 0.000157444 | RUNNING                | dimemas  |
| 3 |           0 | 0.000219304 | 0.000219304 | RUNNING                | dimemas  |
| 1 |   1.743e-05 | 0.003124832 | 0.003107402 | RUNNING                | dimemas  |
| 2 | 0.000157444 | 0.003125267 | 0.002967823 | RUNNING                | dimemas  |
| 3 | 0.000219304 | 0.003125908 | 0.002906604 | RUNNING                | dimemas  |
| 1 | 0.003124832 |  0.00316019 |  3.5358e-05 | MPI_INIT                | dimemas  |
| 2 | 0.003125267 |  0.00316019 |  3.4923e-05 | MPI_INIT                | dimemas  |
| 3 | 0.003125908 |  0.00316019 |  3.4282e-05 | MPI_INIT                | dimemas  |
| 0 |  0.00316019 | 0.003353051 | 0.000192861 | RUNNING                | dimemas  |
| 1 |  0.00316019 | 0.003215864 |  5.5674e-05 | RUNNING                | dimemas  |
| 2 |  0.00316019 | 0.003216449 |  5.6259e-05 | RUNNING                | dimemas  |
| 3 |  0.00316019 | 0.003215941 |  5.5751e-05 | RUNNING                | dimemas  |
| 1 | 0.003215864 | 0.003222131 |   6.267e-06 | RUNNING                | dimemas  |
| 3 | 0.003215941 | 0.003222836 |   6.895e-06 | RUNNING                | dimemas  |
| 2 | 0.003216449 | 0.003223124 |   6.675e-06 | RUNNING                | dimemas  |
| 1 | 0.003222131 | 0.003227563 |   5.432e-06 | RUNNING                | dimemas  |
| 3 | 0.003222836 | 0.003227878 |   5.042e-06 | RUNNING                | dimemas  |
| 2 | 0.003223124 | 0.003228144 |    5.02e-06 | MPI_SEND                | dimemas  |
| 1 | 0.003227563 | 0.003364892 | 0.000137329 | MPI_RECV                | dimemas  |
| 3 | 0.003227878 | 0.003228144 |    2.66e-07 | MPI_RECV                | dimemas  |
| 2 | 0.003228144 | 0.003260344 |    3.22e-05 | MPI_FINALIZE            | dimemas  |
| 3 | 0.003228144 | 0.003259095 |  3.0951e-05 | MPI_FINALIZE            | dimemas  |
| 3 | 0.003259095 | 0.003714393 | 0.000455298 | RUNNING                | dimemas  |
| 2 | 0.003260344 | 0.003714934 |  0.00045459 | RUNNING                | dimemas  |
| 0 | 0.003353051 | 0.003358193 |   5.142e-06 | RUNNING                | dimemas  |
| 0 | 0.003358193 | 0.003364892 |   6.699e-06 | MPI_SEND                | dimemas  |
| 0 | 0.003364892 | 0.003521815 | 0.000156923 | MPI_FINALIZE            | dimemas  |
| 1 | 0.003364892 | 0.003394411 |  2.9519e-05 | MPI_FINALIZE            | dimemas  |
| 1 | 0.003394411 | 0.003638575 | 0.000244164 | RUNNING                | dimemas  |
| 0 | 0.003521815 |  0.00363888 | 0.000117065 | RUNNING                | dimemas  |
| 1 | 0.003638575 | 0.003649941 |  1.1366e-05 | RUNNING                | dimemas  |
| 0 |  0.00363888 | 0.003649005 |  1.0125e-05 | RUNNING                | dimemas  |
| 0 | 0.003649005 | 0.003657463 |   8.458e-06 | RUNNING                | dimemas  |
| 1 | 0.003649941 | 0.003658186 |   8.245e-06 | RUNNING                | dimemas  |
| 0 | 0.003657463 | 0.003657707 |    2.44e-07 | RUNNING                | dimemas  |
| 0 | 0.003657707 | 0.003657708 |       1e-09 | NOT CREATED            | dimemas  |
| 1 | 0.003658186 | 0.003658464 |    2.78e-07 | RUNNING                | dimemas  |
| 1 | 0.003658464 | 0.003658465 |       1e-09 | NOT CREATED            | dimemas  |
| 3 | 0.003714393 | 0.003725446 |  1.1053e-05 | RUNNING                | dimemas  |
| 2 | 0.003714934 | 0.003726998 |  1.2064e-05 | RUNNING                | dimemas  |
| 3 | 0.003725446 | 0.003732523 |   7.077e-06 | RUNNING                | dimemas  |
| 2 | 0.003726998 | 0.003735078 |    8.08e-06 | RUNNING                | dimemas  |
| 3 | 0.003732523 | 0.003732753 |     2.3e-07 | RUNNING                | dimemas  |
| 3 | 0.003732753 | 0.003732754 |       1e-09 | NOT CREATED            | dimemas  |
| 2 | 0.003735078 | 0.003735384 |    3.06e-07 | RUNNING                | dimemas  |
| 2 | 0.003735384 | 0.003735385 |       1e-09 | NOT CREATED            | dimemas  |
| 0 |           0 |  0.00316019 |  0.00316019 | RUNNING                | original |
| 1 |           0 |   1.743e-05 |   1.743e-05 | NOT CREATED            | original |
| 2 |           0 | 0.000157444 | 0.000157444 | NOT CREATED            | original |
| 3 |           0 | 0.000219304 | 0.000219304 | NOT CREATED            | original |
| 1 |   1.743e-05 | 0.003124832 | 0.003107402 | RUNNING                | original |
| 2 | 0.000157444 | 0.003125267 | 0.002967823 | RUNNING                | original |
| 3 | 0.000219304 | 0.003125908 | 0.002906604 | RUNNING                | original |
| 1 | 0.003124832 | 0.003170029 |  4.5197e-05 | MPI_INIT                | original |
| 2 | 0.003125267 | 0.003169742 |  4.4475e-05 | MPI_INIT                | original |
| 3 | 0.003125908 | 0.003170472 |  4.4564e-05 | MPI_INIT                | original |
| 0 |  0.00316019 | 0.003170485 |  1.0295e-05 | MPI_INIT                | original |
| 2 | 0.003169742 | 0.003226001 |  5.6259e-05 | RUNNING                | original |
| 1 | 0.003170029 | 0.003225703 |  5.5674e-05 | RUNNING                | original |
| 3 | 0.003170472 | 0.003226223 |  5.5751e-05 | RUNNING                | original |
| 0 | 0.003170485 | 0.003363346 | 0.000192861 | RUNNING                | original |
| 1 | 0.003225703 | 0.003389114 | 0.000163411 | OTHERS                 | original |
| 2 | 0.003226001 | 0.003388687 | 0.000162686 | OTHERS                 | original |
| 3 | 0.003226223 | 0.003388393 |  0.00016217 | OTHERS                 | original |
| 0 | 0.003363346 | 0.003483197 | 0.000119851 | OTHERS                 | original |
| 3 | 0.003388393 | 0.003395288 |   6.895e-06 | RUNNING                | original |
| 2 | 0.003388687 | 0.003395362 |   6.675e-06 | RUNNING                | original |
| 1 | 0.003389114 | 0.003395381 |   6.267e-06 | RUNNING                | original |
| 3 | 0.003395288 | 0.003432306 |  3.7018e-05 | OTHERS                 | original |
| 2 | 0.003395362 | 0.003432702 |   3.734e-05 | OTHERS                 | original |
| 1 | 0.003395381 | 0.003432431 |   3.705e-05 | OTHERS                 | original |
| 3 | 0.003432306 | 0.003437348 |   5.042e-06 | RUNNING                | original |
| 1 | 0.003432431 | 0.003437863 |   5.432e-06 | RUNNING                | original |
| 2 | 0.003432702 | 0.003437722 |    5.02e-06 | RUNNING                | original |
| 3 | 0.003437348 | 0.003484691 |  4.7343e-05 | MPI_RECV                | original |
| 2 | 0.003437722 |  0.00348182 |  4.4098e-05 | MPI_SEND                | original |
| 1 | 0.003437863 |  0.00369535 | 0.000257487 | MPI_RECV                | original |
| 2 |  0.00348182 |  0.00351402 |    3.22e-05 | RUNNING                | original |
| 0 | 0.003483197 | 0.003488339 |   5.142e-06 | RUNNING                | original |
| 3 | 0.003484691 | 0.003515642 |  3.0951e-05 | RUNNING                | original |
| 0 | 0.003488339 | 0.003645036 | 0.000156697 | OTHERS                 | original |
| 2 |  0.00351402 | 0.003550242 |  3.6222e-05 | MPI_FINALIZE            | original |
| 3 | 0.003515642 |  0.00355024 |  3.4598e-05 | MPI_FINALIZE            | original |
| 3 |  0.00355024 | 0.004005538 | 0.000455298 | RUNNING                | original |
| 2 | 0.003550242 | 0.004004832 |  0.00045459 | RUNNING                | original |
| 0 | 0.003645036 | 0.003651735 |   6.699e-06 | RUNNING                | original |
| 0 | 0.003651735 | 0.003693602 |  4.1867e-05 | MPI_SEND                | original |
| 0 | 0.003693602 | 0.003850525 | 0.000156923 | RUNNING                | original |
| 1 |  0.00369535 | 0.003724869 |  2.9519e-05 | RUNNING                | original |
| 1 | 0.003724869 | 0.003761334 |  3.6465e-05 | MPI_FINALIZE            | original |
| 1 | 0.003761334 | 0.004005498 | 0.000244164 | RUNNING                | original |
| 0 | 0.003850525 | 0.003887338 |  3.6813e-05 | MPI_FINALIZE            | original |
| 0 | 0.003887338 | 0.004004403 | 0.000117065 | RUNNING                | original |
| 0 | 0.004004403 | 0.004014528 |  1.0125e-05 | I/O                    | original |
| 2 | 0.004004832 | 0.004016896 |  1.2064e-05 | I/O                    | original |
| 1 | 0.004005498 | 0.004016864 |  1.1366e-05 | I/O                    | original |
| 3 | 0.004005538 | 0.004016591 |  1.1053e-05 | I/O                    | original |
| 0 | 0.004014528 |  0.00402323 |   8.702e-06 | RUNNING                | original |
| 3 | 0.004016591 | 0.004023898 |   7.307e-06 | RUNNING                | original |
| 1 | 0.004016864 | 0.004025387 |   8.523e-06 | RUNNING                | original |
| 2 | 0.004016896 | 0.004025282 |   8.386e-06 | RUNNING                | original |

  
Now let's plot it.

#+begin_src R :results output graphics :file experiments/old/img/sendrecv-simgrid-2.png :exports both :width 600 :height 400 :session
library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1, scale="free_x");
#+end_src

#+RESULTS:
[[file:experiments/old/img/sendrecv-simgrid-2.png]]

 The first state in the Simgrid trace is MPI_init and the last one is MPI_Finalize for all tasks.
The tit trace had computing stages before the MPI_init and after the MPI_Finalize, i will take a look at that now.

* 2016-04-13 Next steps

After talking to professor Lucas, here are the next steps:
1. Repeat the comparison with the big traces we have access.
2. Build identical platform descriptions to be used as input to Simgrid and Dimemas.
3. Simulate the large traces with these identical platform descriptions and compare them.
4. Generate some new paraver traces of benchmarks using EXTRAE.

Step 2 is specially important since we want to compare traces using the same
machine model. In this case, we can disregard the original trace.
* 2016-04-14 Platform descriptions
As discussed earlier, it is important to have identical platform descriptions for
Simgrid and Dimemas for us to compare the traces.
First of all, I made an adjustment in the =prv2pjdump.pl= script. It no longer
replaces the state name with the mpi call that occurs during the state. The reason
for that is that some states may contain more than one mpi call.

The format of the platform description in Simgrid and Dimemas is very different. Parameters
with the same name do not necessarily mean the same thing or work the same way. After reading
the documentation for building a platform description in Simgrid and Dimemas, we made our first
attempt. We made a very simple platform and we will try to approach each parameters individually,
so we can have a better understaing of how it works.

The platform we will build contains 2 nodes, each node contains 2 cores. Each core has a
processing power of =P= (flops). The nodes can communicate with each other with latency =L=
(seconds) and a bandwidth of =B= (MBps). Also, each flow going through the link between the
nodes will be provided with the complete bandwidth =B= (to make it simple). Other parameters
will be discussed later. We will be calling this platform "small".

Here is how it looks in the Simgrid format:
#+begin_src sh :results output :exports both
  cat simgrid/examples/small.xml
#+end_src

#+RESULTS:
#+begin_example
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid.dtd">
<platform version="3">
<AS  id="AS_small"  routing="Full">
     <host id="host1" power="286.087kf" core="2"/>
     <host id="host2" power="286.087kf" core="2"/>
     <link id="link1" bandwidth="10000GBps" latency="0s" sharing_policy="FATPIPE"/>
     <route src="host1" dst="host2"><link_ctn id="link1"/></route>
</AS>
</platform>

#+end_example

Very short, very simple. The values used for bandwidth, latency and power can be ignored,
since we will be changing those for the following tests.

One important note is that the power in Simgrid is expressed as Flops, however, on Dimemas,
the simulated power is relative to the instrumented speed ratio (we say how much times
the simulation should be faster/slower than the original trace). We can easily calculate the
value the we should use in Dimemas by dividing the Simgrid simulation Flops with the
original Flops value.

t_simgrid = ops_original / Flops_simgrid

t_simgrid = (t_original * Flops_original) / Flops_simgrid

t_dimemas = t_original/c

We want t_simgrid = t_dimemas

(t_original * Flops_original) / Flops_simgrid = t_original/c

Flops_original / Flops_simgrid = 1/c

c = Flops_simgrid / Flops_original

Where, t_simgrid = the simulated time on Simgrid, t_dimemas = the simulated time on Dimemas,
c = the relative processor speed (the input we should use on Dimemas)

We made a simple MPI application trace to test the power parameter in
both platform descriptions.
Here is the application
#+begin_src sh :results output :exports both
  cat computation/computation.c
#+end_src

#+RESULTS:
#+begin_example
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <mpi.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, num_procs;
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if(num_procs < 2 || num_procs % 2 != 0) {
      MPI_Finalize();
      return 1;
    }

		int i;
		int computationSize = 1000000;
		float value = 1.12435433421;
     for (i = 0; i < computationSize; i++) {
			value = ((((value * 3.1415985934) / 2.42433453) + 0.00000324234) * 1.002134667345) / 1.00042542214;
		}

    MPI_Finalize();
}
#+end_example

The MPI app creates n tasks and each task has the same amount of
computation. We traced this application with EXTRAE and we executed
the cycle to obtain the simgrid trace and Dimemas trace in the pjdump
format using the platform description that we showed earlier with a
power =P= of 286.087KFlops, =L= of 0 seconds and =B= of 1000 GBps.
The app was traced with 4 tasks, task 0 and 1 were assigned to the
first node, task 2 and 3 to the second.
Here are the final traces for Simgrid and Dimemas:
#+begin_src sh :results output :exports both
  cat computation/computation-simgrid.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, 0, 0, 24.9563, 24.9563, 0
Container, 0, MPI, 0, 24.9563, 24.9563, rank-3
State, rank-3, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-3, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-3, MPI_STATE, 0.000000000, 2.879051000, 2.879051000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.879051000, 2.946167000, 0.067116000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.946167000, 2.952132000, 0.005965000, 1.000000000, computing
State, rank-3, MPI_STATE, 2.952132000, 21.636174000, 18.684042000, 1.000000000, computing
State, rank-3, MPI_STATE, 21.636174000, 24.888875000, 3.252701000, 1.000000000, computing
State, rank-3, MPI_STATE, 24.888875000, 24.894258000, 0.005383000, 1.000000000, computing
State, rank-3, MPI_STATE, 24.894258000, 24.956309000, 0.062051000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 24.9563, 24.9563, rank-2
State, rank-2, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-2, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-2, MPI_STATE, 0.000000000, 2.808296000, 2.808296000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.808296000, 2.875053000, 0.066757000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.875053000, 2.881127000, 0.006074000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.881127000, 20.948396000, 18.067269000, 1.000000000, computing
State, rank-2, MPI_STATE, 20.948396000, 24.823476000, 3.875080000, 1.000000000, computing
State, rank-2, MPI_STATE, 24.823476000, 24.829214000, 0.005738000, 1.000000000, computing
State, rank-2, MPI_STATE, 24.829214000, 24.956309000, 0.127095000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 24.9563, 24.9563, rank-1
State, rank-1, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-1, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-1, MPI_STATE, 0.000000000, 2.557289000, 2.557289000, 1.000000000, computing
State, rank-1, MPI_STATE, 2.557289000, 2.625715000, 0.068426000, 1.000000000, computing
State, rank-1, MPI_STATE, 2.625715000, 2.632033000, 0.006318000, 1.000000000, computing
State, rank-1, MPI_STATE, 2.632033000, 24.347424000, 21.715391000, 1.000000000, computing
State, rank-1, MPI_STATE, 24.347424000, 24.564885000, 0.217461000, 1.000000000, computing
State, rank-1, MPI_STATE, 24.564885000, 24.571351000, 0.006466000, 1.000000000, computing
State, rank-1, MPI_STATE, 24.571351000, 24.956309000, 0.384958000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 24.9563, 24.9563, rank-0
State, rank-0, MPI_STATE, 0.000000000, 24.956309000, 24.956309000, 0.000000000, computing
State, rank-0, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-0, MPI_STATE, 0.000000000, 2.885332000, 2.885332000, 1.000000000, computing
State, rank-0, MPI_STATE, 2.885332000, 3.008576000, 0.123244000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.008576000, 3.014496000, 0.005920000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.014496000, 22.493663000, 19.479167000, 1.000000000, computing
State, rank-0, MPI_STATE, 22.493663000, 24.950293000, 2.456630000, 1.000000000, computing
State, rank-0, MPI_STATE, 24.950293000, 24.956309000, 0.006016000, 1.000000000, computing
State, rank-0, MPI_STATE, 24.956309000, 24.956309000, 0.000000000, 1.000000000, smpi_replay_run_finalize
#+end_example

#+begin_src sh :results output :exports both
  cat computation/computation-dimemas.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, APP, 0, 25.002551001, 25.002551001, 0
Container, 0, TASK, 0, 25.002551001, 25.002551001, 0
Container, 0, TASK, 0, 25.002551001, 25.002551001, 1
Container, 0, TASK, 0, 25.002551001, 25.002551001, 2
Container, 0, TASK, 0, 25.002551001, 25.002551001, 3
State, rank-0, RUNNING, 0, 0.034298, 0.034298, 0, RUNNING
State, rank-1, RUNNING, 0, 0.320785, 0.320785, 0, RUNNING
State, rank-2, RUNNING, 0, 0.069523, 0.069523, 0, RUNNING
State, rank-3, RUNNING, 0, 2.879051, 2.879051, 0, RUNNING
State, rank-0, RUNNING, 0.034298, 2.91963, 2.885332, 0, RUNNING
State, rank-2, RUNNING, 0.069523, 2.877819, 2.808296, 0, RUNNING
State, rank-1, RUNNING, 0.320785, 2.878074, 2.557289, 0, RUNNING
State, rank-2, BLOCKED, 2.877819, 2.91963, 0.041811, 0, BLOCKED
State, rank-1, BLOCKED, 2.878074, 2.91963, 0.041556, 0, BLOCKED
State, rank-3, BLOCKED, 2.879051, 2.91963, 0.040579, 0, BLOCKED
State, rank-0, RUNNING, 2.91963, 3.042874, 0.123244, 0, RUNNING
State, rank-1, RUNNING, 2.91963, 2.988056, 0.068426, 0, RUNNING
State, rank-2, RUNNING, 2.91963, 2.986387, 0.066757, 0, RUNNING
State, rank-3, RUNNING, 2.91963, 2.986746, 0.067116, 0, RUNNING
State, rank-2, RUNNING, 2.986387, 2.992461, 0.006074, 0, RUNNING
State, rank-3, RUNNING, 2.986746, 2.992711, 0.005965, 0, RUNNING
State, rank-1, RUNNING, 2.988056, 2.994374, 0.006318, 0, RUNNING
State, rank-2, RUNNING, 2.992461, 21.05973, 18.067269, 0, RUNNING
State, rank-3, RUNNING, 2.992711, 21.676753, 18.684042, 0, RUNNING
State, rank-1, RUNNING, 2.994374, 24.709765, 21.715391, 0, RUNNING
State, rank-0, RUNNING, 3.042874, 3.048794, 0.00592, 0, RUNNING
State, rank-0, RUNNING, 3.048794, 22.527961, 19.479167, 0, RUNNING
State, rank-2, RUNNING, 21.05973, 24.93481, 3.87508, 0, RUNNING
State, rank-3, RUNNING, 21.676753, 24.929454, 3.252701, 0, RUNNING
State, rank-0, RUNNING, 22.527961, 24.984591, 2.45663, 0, RUNNING
State, rank-1, RUNNING, 24.709765, 24.927226, 0.217461, 0, RUNNING
State, rank-1, RUNNING, 24.927226, 24.940695, 0.013469, 0, RUNNING
State, rank-3, RUNNING, 24.929454, 24.941998, 0.012544, 0, RUNNING
State, rank-2, RUNNING, 24.93481, 24.947975, 0.013165, 0, RUNNING
State, rank-1, RUNNING, 24.940695, 24.946954, 0.006259, 0, RUNNING
State, rank-3, RUNNING, 24.941998, 24.94715, 0.005152, 0, RUNNING
State, rank-1, RUNNING, 24.946954, 24.947161, 0.000207, 0, RUNNING
State, rank-3, RUNNING, 24.94715, 24.947381, 0.000231, 0, RUNNING
State, rank-1, NOT CREATED, 24.947161, 24.947161001, 0.000000001, 0, NOT CREATED
State, rank-3, NOT CREATED, 24.947381, 24.947381001, 0.000000001, 0, NOT CREATED
State, rank-2, RUNNING, 24.947975, 24.953528, 0.005553, 0, RUNNING
State, rank-2, RUNNING, 24.953528, 24.953713, 0.000185, 0, RUNNING
State, rank-2, NOT CREATED, 24.953713, 24.953713001, 0.000000001, 0, NOT CREATED
State, rank-0, RUNNING, 24.984591, 24.996535, 0.011944, 0, RUNNING
State, rank-0, RUNNING, 24.996535, 25.002277, 0.005742, 0, RUNNING
State, rank-0, RUNNING, 25.002277, 25.002551, 0.000274, 0, RUNNING
State, rank-0, NOT CREATED, 25.002551, 25.002551001, 0.000000001, 0, NOT CREATED
Event, 2, MPI_CALL, 2.877819, MPI_INIT
Event, 1, MPI_CALL, 2.878074, MPI_INIT
Event, 3, MPI_CALL, 2.879051, MPI_INIT
Event, 0, MPI_CALL, 2.91963, MPI_INIT
Event, 2, MPI_CALL, 21.05973, MPI_FINALIZE
Event, 3, MPI_CALL, 21.676753, MPI_FINALIZE
Event, 0, MPI_CALL, 22.527961, MPI_FINALIZE
Event, 1, MPI_CALL, 24.709765, MPI_FINALIZE
#+end_example

We can observe that both simulations end at approximately the same
time.

Let`s change the bandwidth in the platforms to 1MBps and test if the
output of the simulators will be different. For that, we will use
another MPI application:
#+begin_src sh :results output :exports both
  cat sendrecv/sendrecv.c
#+end_src

#+RESULTS:
#+begin_example
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <mpi.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, num_procs;
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if(num_procs < 2 || num_procs % 2 != 0) {
      MPI_Finalize();
      return 1;
    }

		int sending = 0;
		if (rank % 2 == 0) {
			sending = 1;
		}

    int i;
		int *msg;
		int messageSize = 10000000;
    msg = malloc(messageSize * sizeof(int));

		if (sending) {
     	MPI_Send(msg, messageSize, MPI_INT, rank + 1, 1234, MPI_COMM_WORLD);
     	printf("process %d sent message of size %d to process %d\n", rank, messageSize, rank + 1);
		}
		else {
     	MPI_Status status;
     	MPI_Recv(msg, messageSize, MPI_INT, rank - 1, 1234, MPI_COMM_WORLD, &status);
     	printf("process %d received a message of size %d from process %d\n", rank, messageSize, rank - 1);
		}

		free(msg);

    MPI_Finalize();
}
#+end_example
Each even MPI task send a message of 40MB to its odd neighbor.
We traced the application with EXTRAE and we executed
the cycle to obtain the simgrid trace and Dimemas trace in the pjdump
format. The receiving tasks were assigned to the node 1 and the
sending tasks were sent to the node 2.
Here are the final traces for Simgrid and Dimemas:
#+begin_src sh :results output :exports both
  cat sendrecv/sendrecv-simgrid.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, 0, 0, 52.2366, 52.2366, 0
Link, 0, MPI_LINK, 3.083936000, 45.663638737, 42.579702737, PTP, rank-0, rank-1
Link, 0, MPI_LINK, 3.046690000, 45.870591737, 42.823901737, PTP, rank-2, rank-3
Container, 0, MPI, 0, 52.2366, 52.2366, rank-3
State, rank-3, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-3, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-3, MPI_STATE, 0.000000000, 3.058568000, 3.058568000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.058568000, 3.336736000, 0.278168000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.336736000, 3.340047000, 0.003311000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.340047000, 3.348794000, 0.008747000, 1.000000000, computing
State, rank-3, MPI_STATE, 3.348794000, 45.870591737, 42.521797737, 1.000000000, action_recv
State, rank-3, MPI_STATE, 45.870591737, 46.257293737, 0.386702000, 1.000000000, computing
State, rank-3, MPI_STATE, 46.257293737, 52.225005737, 5.967712000, 1.000000000, computing
State, rank-3, MPI_STATE, 52.225005737, 52.236646737, 0.011641000, 1.000000000, computing
State, rank-3, MPI_STATE, 52.236646737, 52.236646737, 0.000000000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 52.2366, 52.2366, rank-2
State, rank-2, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-2, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-2, MPI_STATE, 0.000000000, 2.953014000, 2.953014000, 1.000000000, computing
State, rank-2, MPI_STATE, 2.953014000, 3.027902000, 0.074888000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.027902000, 3.033616000, 0.005714000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.033616000, 3.046690000, 0.013074000, 1.000000000, computing
State, rank-2, MPI_STATE, 3.046690000, 45.870591737, 42.823901737, 1.000000000, action_send
State, rank-2, MPI_STATE, 45.870591737, 45.968918737, 0.098327000, 1.000000000, computing
State, rank-2, MPI_STATE, 45.968918737, 52.212202737, 6.243284000, 1.000000000, computing
State, rank-2, MPI_STATE, 52.212202737, 52.223213737, 0.011011000, 1.000000000, computing
State, rank-2, MPI_STATE, 52.223213737, 52.236646737, 0.013433000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 52.2366, 52.2366, rank-1
State, rank-1, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-1, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-1, MPI_STATE, 0.000000000, 3.050659000, 3.050659000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.050659000, 3.124837000, 0.074178000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.124837000, 3.129655000, 0.004818000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.129655000, 3.141841000, 0.012186000, 1.000000000, computing
State, rank-1, MPI_STATE, 3.141841000, 45.663638737, 42.521797737, 1.000000000, action_recv
State, rank-1, MPI_STATE, 45.663638737, 46.003718737, 0.340080000, 1.000000000, computing
State, rank-1, MPI_STATE, 46.003718737, 46.232323737, 0.228605000, 1.000000000, computing
State, rank-1, MPI_STATE, 46.232323737, 46.240393737, 0.008070000, 1.000000000, computing
State, rank-1, MPI_STATE, 46.240393737, 52.236646737, 5.996253000, 1.000000000, smpi_replay_run_finalize
Container, 0, MPI, 0, 52.2366, 52.2366, rank-0
State, rank-0, MPI_STATE, 0.000000000, 52.236646737, 52.236646737, 0.000000000, computing
State, rank-0, MPI_STATE, 0.000000000, 0.000000000, 0.000000000, 1.000000000, smpi_replay_run_init
State, rank-0, MPI_STATE, 0.000000000, 2.986926000, 2.986926000, 1.000000000, computing
State, rank-0, MPI_STATE, 2.986926000, 3.069610000, 0.082684000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.069610000, 3.074111000, 0.004501000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.074111000, 3.083936000, 0.009825000, 1.000000000, computing
State, rank-0, MPI_STATE, 3.083936000, 45.663638737, 42.579702737, 1.000000000, action_send
State, rank-0, MPI_STATE, 45.663638737, 45.893635737, 0.229997000, 1.000000000, computing
State, rank-0, MPI_STATE, 45.893635737, 46.226374737, 0.332739000, 1.000000000, computing
State, rank-0, MPI_STATE, 46.226374737, 46.236190737, 0.009816000, 1.000000000, computing
State, rank-0, MPI_STATE, 46.236190737, 52.236646737, 6.000456000, 1.000000000, smpi_replay_run_finalize
#+end_example

#+begin_src sh :results output :exports both
  cat sendrecv/sendrecv-dimemas.pjdump
#+end_src

#+RESULTS:
#+begin_example
Container, 0, APP, 0, 47.915151657, 47.915151657, 0
Container, 0, TASK, 0, 47.915151657, 47.915151657, 0
Container, 0, TASK, 0, 47.915151657, 47.915151657, 1
Container, 0, TASK, 0, 47.915151657, 47.915151657, 2
Container, 0, TASK, 0, 47.915151657, 47.915151657, 3
Link, 0, LINK, 3.387382, 41.534354656, 38.146972656, LINK, 2, 3
Link, 0, LINK, 3.194166, 41.341138656, 38.146972656, LINK, 0, 1
State, rank-0, RUNNING, 0, 0.11023, 0.11023, 0, RUNNING
State, rank-1, RUNNING, 0, 0.006585, 0.006585, 0, RUNNING
State, rank-2, RUNNING, 0, 0.105474, 0.105474, 0, RUNNING
State, rank-3, RUNNING, 0, 3.058568, 3.058568, 0, RUNNING
State, rank-1, RUNNING, 0.006585, 3.057244, 3.050659, 0, RUNNING
State, rank-2, RUNNING, 0.105474, 3.058488, 2.953014, 0, RUNNING
State, rank-0, RUNNING, 0.11023, 3.097156, 2.986926, 0, RUNNING
State, rank-1, BLOCKED, 3.057244, 3.097156, 0.039912, 0, BLOCKED
State, rank-2, BLOCKED, 3.058488, 3.097156, 0.038668, 0, BLOCKED
State, rank-3, BLOCKED, 3.058568, 3.097156, 0.038588, 0, BLOCKED
State, rank-0, RUNNING, 3.097156, 3.17984, 0.082684, 0, RUNNING
State, rank-1, RUNNING, 3.097156, 3.171334, 0.074178, 0, RUNNING
State, rank-2, RUNNING, 3.097156, 3.172044, 0.074888, 0, RUNNING
State, rank-3, RUNNING, 3.097156, 3.375324, 0.278168, 0, RUNNING
State, rank-1, RUNNING, 3.171334, 3.176152, 0.004818, 0, RUNNING
State, rank-2, RUNNING, 3.172044, 3.177758, 0.005714, 0, RUNNING
State, rank-1, RUNNING, 3.176152, 3.188338, 0.012186, 0, RUNNING
State, rank-2, RUNNING, 3.177758, 3.190832, 0.013074, 0, RUNNING
State, rank-0, RUNNING, 3.17984, 3.184341, 0.004501, 0, RUNNING
State, rank-0, RUNNING, 3.184341, 3.194166, 0.009825, 0, RUNNING
State, rank-1, WAITING A MESSAGE, 3.188338, 41.341138656, 38.152800656, 0, WAITING A MESSAGE
State, rank-2, WAITING A MESSAGE, 3.190832, 3.387382, 0.19655, 0, WAITING A MESSAGE
State, rank-0, RUNNING, 3.194166, 3.424163, 0.229997, 0, RUNNING
State, rank-3, RUNNING, 3.375324, 3.378635, 0.003311, 0, RUNNING
State, rank-3, RUNNING, 3.378635, 3.387382, 0.008747, 0, RUNNING
State, rank-2, RUNNING, 3.387382, 3.485709, 0.098327, 0, RUNNING
State, rank-3, WAITING A MESSAGE, 3.387382, 41.534354656, 38.146972656, 0, WAITING A MESSAGE
State, rank-0, RUNNING, 3.424163, 3.756902, 0.332739, 0, RUNNING
State, rank-2, RUNNING, 3.485709, 9.728993, 6.243284, 0, RUNNING
State, rank-0, RUNNING, 3.756902, 3.769042, 0.01214, 0, RUNNING
State, rank-0, RUNNING, 3.769042, 3.778641, 0.009599, 0, RUNNING
State, rank-0, RUNNING, 3.778641, 3.778858, 0.000217, 0, RUNNING
State, rank-0, NOT CREATED, 3.778858, 3.778858001, 0.000000001, 0, NOT CREATED
State, rank-2, RUNNING, 9.728993, 9.743934, 0.014941, 0, RUNNING
State, rank-2, RUNNING, 9.743934, 9.754673, 0.010739, 0, RUNNING
State, rank-2, RUNNING, 9.754673, 9.754945, 0.000272, 0, RUNNING
State, rank-2, NOT CREATED, 9.754945, 9.754945001, 0.000000001, 0, NOT CREATED
State, rank-1, RUNNING, 41.341138656, 41.681218656, 0.34008, 0, RUNNING
State, rank-3, RUNNING, 41.534354656, 41.921056656, 0.386702, 0, RUNNING
State, rank-1, RUNNING, 41.681218656, 41.909823656, 0.228605, 0, RUNNING
State, rank-1, RUNNING, 41.909823656, 41.922359656, 0.012536, 0, RUNNING
State, rank-3, RUNNING, 41.921056656, 47.888768656, 5.967712, 0, RUNNING
State, rank-1, RUNNING, 41.922359656, 41.930222656, 0.007863, 0, RUNNING
State, rank-1, RUNNING, 41.930222656, 41.930429656, 0.000207, 0, RUNNING
State, rank-1, NOT CREATED, 41.930429656, 41.930429657, 0.000000001, 0, NOT CREATED
State, rank-3, RUNNING, 47.888768656, 47.903510656, 0.014742, 0, RUNNING
State, rank-3, RUNNING, 47.903510656, 47.914924656, 0.011414, 0, RUNNING
State, rank-3, RUNNING, 47.914924656, 47.915151656, 0.000227, 0, RUNNING
State, rank-3, NOT CREATED, 47.915151656, 47.915151657, 0.000000001, 0, NOT CREATED
Event, 1, MPI_CALL, 3.057244, MPI_INIT
Event, 2, MPI_CALL, 3.058488, MPI_INIT
Event, 3, MPI_CALL, 3.058568, MPI_INIT
Event, 0, MPI_CALL, 3.097156, MPI_INIT
Event, 1, MPI_CALL, 3.188338, MPI_RECV
Event, 2, MPI_CALL, 3.190832, MPI_SEND
Event, 0, MPI_CALL, 3.194166, MPI_SEND
Event, 3, MPI_CALL, 3.387382, MPI_RECV
Event, 0, MPI_CALL, 3.424163, MPI_FINALIZE
Event, 2, MPI_CALL, 3.485709, MPI_FINALIZE
Event, 1, MPI_CALL, 41.681218656, MPI_FINALIZE
Event, 3, MPI_CALL, 41.921056656, MPI_FINALIZE
#+end_example

There are somethings we can note at this stage. The simulation on
Dimemas takes about 4 seconds less than the Simgrid simulation. Also,
os Dimemas, the processes that sent the message were not blocked until the
message was sent (and they finished much earlier).
Let's plot to have a better idea on what is going on.


#+begin_src sh :results output :exports both
cat sendrecv/sendrecv-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-7,8 | sed -n "/^[^,]*,[^,]*,[^,]*,[^,]*, 0/!p" | cut -d, -f1-4,6 > sendrecv/sendrecv-simgrid-filter.pjdump
cat sendrecv/sendrecv-dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > sendrecv/sendrecv-dimemas-filter.pjdump
#+end_src

#+begin_src R :results output graphics :file experiments/old/img/sendrecv-1.png :exports both :width 1500 :height 400 :session
df_s <- read.csv("sendrecv/sendrecv-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("sendrecv/sendrecv-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df <- rbind(df_s, df_d);
df

library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1);
#+end_src

#+RESULTS:
[[file:experiments/old/img/sendrecv-1.png]]

Some notes:
On the Dimemas side, the processes that sent the message are not
blocked and, therefore, they finish much earlier. Also, the waiting
time for the message on Dimemas is approximately 4 sencond lower than
the waiting time on Simgrid. 

Something interesting is that process 2 and 3 have a five second computing time after the message was
sent/received (this happens in both simulations). This delay was also
observed in the original trace.


* 2016-04-16 Platform description part 2

The last result on Dimemas showed that the processes that sent
the message were not blocked until the message was sent.
We created a variation on the previous application so that we have
late receivers. 

Let`s do it all over again.

#+begin_src sh :results output :exports both
cat sendrecv2/sendrecv2-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-7,8 | sed -n "/^[^,]*,[^,]*,[^,]*,[^,]*, 0/!p" | cut -d, -f1-4,6 > sendrecv2/sendrecv2-simgrid-filter.pjdump
cat sendrecv2/sendrecv2-dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > sendrecv2/sendrecv2-dimemas-filter.pjdump
#+end_src

#+begin_src R :results output graphics :file experiments/old/img/sendrecv-2.png :exports both :width 1500 :height 400 :session
df_s <- read.csv("sendrecv2/sendrecv2-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("sendrecv2/sendrecv2-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df <- rbind(df_s, df_d);
df

library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1);
#+end_src

#+RESULTS:
[[file:experiments/old/img/sendrecv-2.png]]

Now the sending processes are blocked until the receivers start to
receive the data. But they do not wait until the message is sent.

Let`s add a latency of 1s in the link between the nodes and set the
bandwidth to infinity to see how the latency affects the simulation.

#+begin_src sh :results output :exports both
cat latency/sendrecv-simgrid.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-7,8 | sed -n "/^[^,]*,[^,]*,[^,]*,[^,]*, 0/!p" | cut -d, -f1-4,6 > latency/sendrecv-simgrid-filter.pjdump
cat latency/sendrecv-dimemas.pjdump | grep ^State | sed "s/rank-//" | cut -d, -f2,4-6,8 > latency/sendrecv-dimemas-filter.pjdump
#+end_src

#+begin_src R :results output graphics :file experiments/old/img/sendrecv-3.png :exports both :width 1500 :height 400 :session
df_s <- read.csv("latency/sendrecv-simgrid-filter.pjdump", header=FALSE, sep=",");
names(df_s) <- c("Thread", "Start", "End", "Duration", "State");
df_s$Simulator <- "simgrid";

df_d <- read.csv("latency/sendrecv-dimemas-filter.pjdump", header=F, sep=",");
names(df_d) <- c("Thread", "Start", "End", "Duration", "State");
df_d$Simulator <- "dimemas";

df <- rbind(df_s, df_d);
df

library(ggplot2);
ggplot(df, aes(x=Start, y=factor(Thread), color=State)) +
   theme_bw() +
   geom_segment (aes(xend=End, yend=factor(Thread)), size=4) +
   facet_wrap(~Simulator, ncol=1);
#+end_src

#+RESULTS:
[[file:experiments/old/img/sendrecv-3.png]]

The result of the simulation from Simgrid and Dimemas is very very
different here. On Dimemas, before the actual communication, there is
a startup latency of 2s for each task and during the communication,
there is a delay of 1s. For Simgrid, however, it takes 30s for the
message to be sent. The network model being used for the simulators
are not the same, I will take a look at that.


* 2016-04-18 Script for executing the cycle
There is a new shell script that executes the complete cycle for a prv trace.
It converts a prv trace to the tit format, creates a platform description, a deployment
file and simulate the tit trace on Simgrid. It converts the result to the pjdump format
and filter it for plotting the data later. It also creates a configuration file for Dimemas
that models the same platform and simulate using Dimemas. It converts the resulting prv trace
to pjdump and filter it for plotting the data later.
The script also calls an R script that load the filtered data and plot a chart for us to
compare the result.
Before using it, we need to configure some option.
On the top of the file, you will find these parameters:

#+begin_src sh :results output :exports both
head -14 getpjdump/getpjdump.sh
#+end_src

#+RESULTS:
#+begin_example
#!/bin/bash

TRACE_NAME='sendrecv6'

# general options
NTASKS=4
BANDWIDTH=1000000 # in bytes/s
LATENCY=1 # in seconds
CORES=4
ORIGINAL_FLOPS=286087000.0 # in flops/s
SIMULATED_FLOPS=286087.0 # in flops/s
TASK_MAPPING=(0 1 0 1)
PLATFORM_FILE='small'

#+end_example

These are all the options available for the script:
=TRACE_NAME= is the original prv trace.
=NTASKS= is the number of tasks in the trace.
=BANDWIDTH= is the simulated bandwidth in bytes/s.
=LATENCY= is the simulated latency in seconds.
=CORES= is the number of cores in each node.
=ORIGINAL_FLOPS= is the original flops.
=SIMULATED_FLOPS= is the simulated flops of each core.
=PLATFORM_FILE= is the platform that will be used for the simulation (currently, there is only
one platform available and it is called small).
=TASK_MAPPING= maps the tasks to the nodes.

Set these parameters and run it: =./getpjdump.sh=

* 2016-04-20 Simgrid latency and bandwidth factors

In the last experiments, we were having different timings for all the
messages exchanges between the processes. It turns out that Simgrid
uses a factor to calculate the real bandwidth and latency used for a
certain size of message. These parameters are given by the option
=smpi/bw_factor= and =smpi/lat_factor=. Below, we are plotting the
simulation result using Dimemas and Simgrid of the same application
that was used in the previous experiment.  This time, using a
bandwidth of 1Mbps, a latency of 1s and a processing power of
286KFlops.

We will be using the script =getpjdump.sh= that executes the complete
cycle and plot the chart.  We configures the script with these values:

#+begin_src sh :results output :exports both
head -14 getpjdump/getpjdump.sh
#+end_src

#+RESULTS:
#+begin_example
#!/bin/bash

TRACE_NAME='sendrecv6'

# general options
NTASKS=4
BANDWIDTH=1000000 # in bytes/s
LATENCY=1 # in seconds
CORES=4
ORIGINAL_FLOPS=286087000.0 # in flops/s
SIMULATED_FLOPS=286087.0 # in flops/s
TASK_MAPPING=(0 1 0 1)
PLATFORM_FILE='small'

#+end_example

And we run it.

#+begin_src sh :results output :exports both
cd getpjdump
./getpjdump.sh
#+end_src

#+RESULTS:
#+begin_example
prv trace converted to the dim format
created dimemas config file
-> Simulator configuration to be read /tmp/tmp.Hkps6FjiQC/small-dimemas.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration

50.000000000: END SIMULATION

Output Paraver trace "sendrecv6-dimemas.prv" generated

dimemas simulation is done
got dimemas pjdump trace
prv trace converted to the tit format
created simgrid platform file
create simgrid deployment file
[rank 0] -> host0
[rank 1] -> host1
[rank 2] -> host0
[rank 3] -> host1
simgrid simulation is done
got simgrid pjdump trace
pjdumps filtered
#+end_example

Magnificent.
Here is the chart.

[[file:experiments/old/img/sendrecv-4.pdf]]

Note that the processes that received the message and the processes
the sent the message (on Simgrid only) are blocked for the same period
of time since we are now effectively using the same values for
bandwidth and latency.

* 2016-04-28 Meeting with Tiago                                 :Lucas:Tiago:

_Creating a Marenostrum Model_

A marenostrum model for Simgrid is needed for quite a while. See this:
- http://simgrid.gforge.inria.fr/contrib/smpi-paraver.php#orgheadline9

We'll ask Judit about this to see if she has one Dimemas model for
Marenostrum available. There is some documentation about it here:
http://www.jorditorres.org/wp-content/uploads/2014/02/16.CaseStudy.marenostrum.pdf

We took a look at this file:
- There are 3048 compute nodes
  - Each compute node is equiped with 2CPUs, 8 cores each (see slide 2)
- It is based on an Infiniband FDR10 non-blocking fat tree network
  - The three has two levels
    - First level is a _bunch of MLX FDR10 36-port_ (only 18 compute
      nodes are connected to one equipement like this)
    - Second level is _six MLX SX6535_ (522 ports - only 507 used on
      each)
    - The problem is understand exactly how the interconnection is
      made between the root level (the six MLX SX6535) and the
      switches connected to the computer nodes. It looks like one
      bottom network swith has 18 optical uplinks, 3 to each top
      switches in the root level.
  - See slide 13 for details

With this information in hand, we are already able to write a rough
sketch of SimGrid platform model for Marenostrum. Modeling Marenostrum
in Simgrid should be relatively easy to do. We shall pay attention to
SimGrid performance modeling all that.

- _Task #1_: Create a SimGrid model of marenostrum.
- _Task #2_: Ask Judit for a Dimemas model of marenostrum.

_Regarding small scale trace file comparison_

Tiago made some comparisons between SimGrid and Dimemas and observed
that sends are asynchronous no matter the message size in Dimemas
until the beginning of the corresponding receive. In SimGrid, they act
as expected being blocked, i.e., so the send keeps blocking the
process until the receive is done on the receiver side. Tiago tested
with messages up to 40 megabytes, but tests were carried out in a
multicore machine instead of a cluster. The comparisons only show
Dimemas and SimGrid, so one task is to represent also the real paraver
trace file, since we think that the behavior of Dimemas might be
influenced by something in the paraver trace file that we disregard
when we convert prv to tit to replay with smpi.

- _Task #3_: Always put the real execution trace represention when
  comparing Dimemas and SimGrid.

_Get a G5K account for Tiago_

We'll contact Arnaud to try to get an account in G5K so Tiago can
control experimental and obtain traces himself.
<<<<<<< HEAD

* 2016-04-30 Organizing the repository

It is important that all the experiments made here can be easily
reproducible.  The repository was a bit disorganized. Some of the
code/traces/results used for or obtained from the previous experiments
are no longer in the repository.

I will be moving all the resources and results of the experiments we
made so far to the =experiments/old=. For the next experiments,
there will be a folder named with the date in which the experiment was
made under the =experiments/= directory. On the date folder, each
experiment will be assigned a number so we can uniquely identify
them. The idea is that all the experiments described here in the
Labbook will be reproducible with the files stored in that folder.

* 2016-05-01 Dimemas blocking sends

We will perform some trace simulations on Simgrid and Dimemas in order
to help us investigate why the blocking sends are not being simulated on Dimemas.

Let's start an experiment by creating a folder under the experiments directory.

#+begin_src sh
cd experiments
mkdir -p $(date '+%d-%b-%Y')'/sendrecv1'
#+end_src

We will create a MPI application and an EXTRAE configuration file in the experiment dir.
Here is the app:

#+begin_src sh :results output :exports both
cd experiments/11-May-2016/sendrecv1/
cat sendrecv.c
#+end_src

#+RESULTS:
#+begin_example
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);

    int rank, num_procs;
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if(num_procs < 2 || num_procs % 2 != 0) {
      MPI_Finalize();
      return 1;
    }

    int *msg = malloc(1000000*sizeof(int));
    if(rank % 2 == 0) {
      MPI_Send(msg, 1000000, MPI_INT, rank + 1, 1234, MPI_COMM_WORLD);
      printf("process %d sent message to process %d\n", rank, rank + 1);
    }
    else {
      MPI_Status status;
      MPI_Recv(msg, 1000000, MPI_INT, rank - 1, 1234, MPI_COMM_WORLD, &status);
      printf("process %d received a message from process %d\n", rank, rank - 1);
    }
    MPI_Finalize();
}
#+end_example

Each even process send a message of 1MB to its odd neighbor.
Let`s compile the app and trace it with 4 tasks

#+begin_src sh :results output :exports both
cd experiments/11-May-2016/sendrecv1/
mpicc sendrecv.c -o sendrecv
cd extrae
export EXTRAE_CONFIG_FILE=extrae.xml
export LD_PRELOAD=/home/tiago/install/extrae-3.1.0/lib/libmpitrace.so
mpirun -np 4 ./../sendrecv
#+end_src

#+RESULTS:
#+begin_example
Welcome to Extrae 3.1.0 (revision 3316 based on extrae/trunk)
Extrae: Parsing the configuration file (extrae.xml) begins
Extrae: Tracing package is located on /home/tiago/install/extrae-3.1.0
Extrae: Generating intermediate files for Paraver traces.
Extrae: MPI routines will collect HW counters information.
Extrae: Warning! <pthread> tag will be ignored. This library does not support OpenMP.
Extrae: Tracing 3 level(s) of MPI callers: [ 1 2 3 ]
Extrae: <dynamic-memory> tag at <Callers> level will be ignored. This library does not support dynamic memory instrumentation.
Extrae: Warning! change-at-time time units not specified. Using seconds
Extrae: PAPI domain set to ALL for HWC set 1
Extrae: HWC set 1 contains following counters < PAPI_TOT_INS (0x80000032) PAPI_TOT_CYC (0x8000003b) PAPI_L2_DCM (0x80000002) PAPI_L3_TCM (0x80000008) PAPI_BR_MSP (0x8000002e) > - never changes
Extrae: Warning! change-at-time time units not specified. Using seconds
Extrae: PAPI domain set to ALL for HWC set 2
Extrae: HWC set 2 contains following counters < PAPI_TOT_INS (0x80000032) PAPI_TOT_CYC (0x8000003b) PAPI_LD_INS (0x80000035) PAPI_SR_INS (0x80000036) PAPI_BR_UCN (0x8000002a) PAPI_BR_CN (0x8000002b) RESOURCE_STALLS (0x40000028) > - never changes
Extrae: Resource usage is disabled at flush buffer.
Extrae: Memory usage is disabled at flush buffer.
Extrae: Tracing buffer can hold 500000 events
Extrae: Circular buffer disabled.
Extrae: Dynamic memory instrumentation is disabled.
Extrae: Basic I/O memory instrumentation is disabled.
Extrae: Parsing the configuration file (extrae.xml) has ended
Extrae: Intermediate traces will be stored in /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae
Extrae: Temporal directory (/home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae) is shared among processes.
Extrae: Final directory (/home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae) is shared among processes.
Extrae: Tracing mode is set to: Detail.
Extrae: Successfully initiated with 4 tasks

process 0 sent message to process 1
process 1 received a message from process 0
process 2 sent message to process 3
process 3 received a message from process 2
Extrae: Intermediate raw trace file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017664000003000000.mpit
Extrae: Intermediate raw trace file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017663000002000000.mpit
Extrae: Intermediate raw trace file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017662000001000000.mpit
Extrae: Intermediate raw trace file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017661000000000000.mpit
Extrae: Intermediate raw sym file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017664000003000000.sym
Extrae: Intermediate raw sym file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017662000001000000.sym
Extrae: Intermediate raw sym file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017661000000000000.sym
Extrae: Deallocating memory.
Extrae: Intermediate raw sym file created : /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017663000002000000.sym
Extrae: Application has ended. Tracing has been terminated.
Extrae: Proceeding with the merge of the intermediate tracefiles.
Extrae: Waiting for all tasks to reach the checkpoint.
Extrae: Executing the merge process (using /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/TRACE.mpits).
merger: Extrae 3.1.0 (revision 3316 based on extrae/trunk)
mpi2prv: Tree order is set to 16 but is larger that numtasks. Setting tree order to 4
mpi2prv: Assigned nodes < johanna, johanna, johanna, johanna >
mpi2prv: Assigned size per processor < <1 Mbyte, <1 Mbyte, <1 Mbyte, <1 Mbyte >
mpi2prv: File /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017661000000000000.mpit is object 1.1.1 on node johanna assigned to processor 0
mpi2prv: File /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017662000001000000.mpit is object 1.2.1 on node johanna assigned to processor 1
mpi2prv: File /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017663000002000000.mpit is object 1.3.1 on node johanna assigned to processor 2
mpi2prv: File /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/set-0/TRACE.0000017664000003000000.mpit is object 1.4.1 on node johanna assigned to processor 3
mpi2prv: Time synchronization has been turned off
mpi2prv: A total of 10 symbols were imported from /home/tiago/trabalhoconclusao/experiments/11-May-2016/sendrecv1/extrae/TRACE.sym file
mpi2prv: 0 function symbols imported
mpi2prv: 10 HWC counter descriptions imported
mpi2prv: Checking for target directory existance... exists, ok!
mpi2prv: Selected output trace format is Paraver
mpi2prv: Stored trace format is Paraver
mpi2prv: Searching synchronization points... done
mpi2prv: Enabling Time Synchronization (Node).
mpi2prv: Circular buffer enabled at tracing time? NO
mpi2prv: Parsing intermediate files
mpi2prv: Processor 0 succeeded to translate its assigned files
mpi2prv: Processor 3 succeeded to translate its assigned files
mpi2prv: Processor 1 succeeded to translate its assigned files
mpi2prv: Processor 2 succeeded to translate its assigned files
mpi2prv: Elapsed time translating files: 0 hours 0 minutes 0 seconds
mpi2prv: Gathering addresses across processors... done
mpi2prv: Elapsed time broadcasting and sorting addresses: 0 hours 0 minutes 0 seconds
mpi2prv: Starting the distribution of foreign receives.
mpi2prv: Processor 3 is storing 0 foreign receives (0 Kbytes) for the next phase.
mpi2prv: Processor 0 is storing 1 foreign receives (0 Kbytes) for the next phase.
mpi2prv: Ended the distribution of foreign receives.
mpi2prv: Processor 2 is storing 1 foreign receives (0 Kbytes) for the next phase.
mpi2prv: Processor 1 is storing 0 foreign receives (0 Kbytes) for the next phase.
mpi2prv: Sharing information < MPI OpenMP pthread TRT CUDA HWC MISC callers >
mpi2prv: Elapsed time sharing communications: 0 hours 0 minutes 0 seconds
mpi2prv: Sharing thread accounting information for ptask 0 done
mpi2prv: Merge tree depth for 4 tasks is 1 levels using a fan-out of 4 leaves
mpi2prv: Executing merge tree step 1 of 1.
mpi2prv: Generating tracefile (intermediate buffers of 335539 events)
         This process can take a while. Please, be patient.
mpi2prv: Progress ... 5.1% 10.7% 15.2% 20.2% 25.9% 30.9% 35.0% 40.1% 46.5% 50.4% 57.0% 62.1% 65.2% 70.4% 75.5% 80.7% 86.0% 90.7% 95.5% done
mpi2prv: Elapsed time on tree step 1: 0 hours 0 minutes 0 seconds
mpi2prv: Resulting tracefile occupies 8672 bytes
mpi2prv: Removing temporal files... done
mpi2prv: Elapsed time removing temporal files: 0 hours 0 minutes 0 seconds
mpi2prv: Congratulations! sendrecv.prv has been generated.
#+end_example

  Ok, the prv trace is in the extrae directory.
  Now, we will use the =getpjdump.sh= script to simulate it and generate the plots.
This is the configuration file that getpjdump will load:

#+begin_src sh :results output :exports both
  cd experiments/11-May-2016/sendrecv1/
  cat config.conf
#+end_src

#+RESULTS:
#+begin_example
NTASKS=4
BANDWIDTH=1000000 # in bytes/s
LATENCY=0.001 # in seconds
CORES=2
ORIGINAL_FLOPS=286087000.0 # in flops/s
SIMULATED_FLOPS=286087.0 # in flops/s
TASK_MAPPING=(0 1 0 1)
PLATFORM_FILE='small'

DIMEMAS_OPTS='-S 32k'
SIMGRID_OPTS='-map --cfg=tracing/smpi/computing:'yes' --cfg=tracing/precision:9 --cfg=network/model:SMPI --cfg=smpi/bw_factor:'65472:1.0;15424:1.0;9376:1.0;5776:1.0;3484:1.0;1426:1.0;732:1.0;257:1.0;0:1.0' --cfg=smpi/lat_factor:'65472:1.0;15424:1.0;9376:1.0;5776:1.0;3484:1.0;1426:1.0;732:1.0;257:1.0;0:1.0' --cfg=smpi/send_is_detached_thres:2 --cfg=smpi/async_small_thres:0 --cfg=smpi/cpu_threshold:-1'
#+end_example

Note that we will be using the =-S 32K= option for Dimemas.
Finally, lets use getpjdump

#+begin_src sh :results output :exports both
  cd experiments/11-May-2016/sendrecv1/
  ./../../../getpjdump/getpjdump.sh extrae/sendrecv
#+end_src

#+RESULTS:
#+begin_example
INITIALIZING PARSER... OK!

SPLITTING COMMUNICATIONS 000 %
SPLITTING COMMUNICATIONS 003 %
SPLITTING COMMUNICATIONS 004 %
SPLITTING COMMUNICATIONS 006 %
SPLITTING COMMUNICATIONS 007 %
SPLITTING COMMUNICATIONS 008 %
SPLITTING COMMUNICATIONS 009 %
SPLITTING COMMUNICATIONS 011 %
SPLITTING COMMUNICATIONS 013 %
SPLITTING COMMUNICATIONS 016 %
SPLITTING COMMUNICATIONS 018 %
SPLITTING COMMUNICATIONS 019 %
SPLITTING COMMUNICATIONS 021 %
SPLITTING COMMUNICATIONS 022 %
SPLITTING COMMUNICATIONS 024 %
SPLITTING COMMUNICATIONS 026 %
SPLITTING COMMUNICATIONS 028 %
SPLITTING COMMUNICATIONS 031 %
SPLITTING COMMUNICATIONS 033 %
SPLITTING COMMUNICATIONS 034 %
SPLITTING COMMUNICATIONS 036 %
SPLITTING COMMUNICATIONS 038 %
SPLITTING COMMUNICATIONS 039 %
SPLITTING COMMUNICATIONS 042 %
SPLITTING COMMUNICATIONS 043 %
SPLITTING COMMUNICATIONS 045 %
SPLITTING COMMUNICATIONS 047 %
SPLITTING COMMUNICATIONS 049 %
SPLITTING COMMUNICATIONS 050 %
SPLITTING COMMUNICATIONS 052 %
SPLITTING COMMUNICATIONS 053 %
SPLITTING COMMUNICATIONS 055 %
SPLITTING COMMUNICATIONS 057 %
SPLITTING COMMUNICATIONS 060 %
SPLITTING COMMUNICATIONS 062 %
SPLITTING COMMUNICATIONS 063 %
SPLITTING COMMUNICATIONS 064 %
SPLITTING COMMUNICATIONS 066 %
SPLITTING COMMUNICATIONS 067 %
SPLITTING COMMUNICATIONS 068 %
SPLITTING COMMUNICATIONS 070 %
SPLITTING COMMUNICATIONS 072 %
SPLITTING COMMUNICATIONS 074 %
SPLITTING COMMUNICATIONS 077 %
SPLITTING COMMUNICATIONS 078 %
SPLITTING COMMUNICATIONS 079 %
SPLITTING COMMUNICATIONS 082 %
SPLITTING COMMUNICATIONS 083 %
SPLITTING COMMUNICATIONS 085 %
SPLITTING COMMUNICATIONS 086 %
SPLITTING COMMUNICATIONS 088 %
SPLITTING COMMUNICATIONS 089 %
SPLITTING COMMUNICATIONS 091 %
SPLITTING COMMUNICATIONS 093 %
SPLITTING COMMUNICATIONS 095 %
SPLITTING COMMUNICATIONS 096 %
SPLITTING COMMUNICATIONS 097 %
SPLITTING COMMUNICATIONS 099 %
SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          68
   * Splitted communications 6
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK

CREATING TRANSLATION STRUCTURES  001/004
CREATING TRANSLATION STRUCTURES  002/004
CREATING TRANSLATION STRUCTURES  003/004
CREATING TRANSLATION STRUCTURES  004/004
CREATING TRANSLATION STRUCTURES  004/004
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION

TRANSLATING RECORDS 000 %
TRANSLATING RECORDS 003 %
TRANSLATING RECORDS 004 %
TRANSLATING RECORDS 006 %
TRANSLATING RECORDS 007 %
TRANSLATING RECORDS 008 %
TRANSLATING RECORDS 009 %
TRANSLATING RECORDS 011 %
TRANSLATING RECORDS 013 %
TRANSLATING RECORDS 016 %
TRANSLATING RECORDS 018 %
TRANSLATING RECORDS 019 %
TRANSLATING RECORDS 021 %
TRANSLATING RECORDS 022 %
TRANSLATING RECORDS 024 %
TRANSLATING RECORDS 026 %
TRANSLATING RECORDS 028 %
TRANSLATING RECORDS 031 %
TRANSLATING RECORDS 033 %
TRANSLATING RECORDS 034 %
TRANSLATING RECORDS 036 %
TRANSLATING RECORDS 038 %
TRANSLATING RECORDS 039 %
TRANSLATING RECORDS 042 %
TRANSLATING RECORDS 043 %
TRANSLATING RECORDS 045 %
TRANSLATING RECORDS 047 %
TRANSLATING RECORDS 049 %
TRANSLATING RECORDS 050 %
TRANSLATING RECORDS 052 %
TRANSLATING RECORDS 053 %
TRANSLATING RECORDS 055 %
TRANSLATING RECORDS 057 %
TRANSLATING RECORDS 060 %
TRANSLATING RECORDS 062 %
TRANSLATING RECORDS 064 %
TRANSLATING RECORDS 066 %
TRANSLATING RECORDS 067 %
TRANSLATING RECORDS 068 %
TRANSLATING RECORDS 070 %
TRANSLATING RECORDS 072 %
TRANSLATING RECORDS 074 %
TRANSLATING RECORDS 077 %
TRANSLATING RECORDS 078 %
TRANSLATING RECORDS 079 %
TRANSLATING RECORDS 082 %
TRANSLATING RECORDS 083 %
TRANSLATING RECORDS 085 %
TRANSLATING RECORDS 086 %
TRANSLATING RECORDS 088 %
TRANSLATING RECORDS 089 %
TRANSLATING RECORDS 091 %
TRANSLATING RECORDS 093 %
TRANSLATING RECORDS 095 %
TRANSLATING RECORDS 096 %
TRANSLATING RECORDS 097 %
TRANSLATING RECORDS 099 %
TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES

   * Merging task    1 100 %
   * Merging task    2 100 %
   * Merging task    3 100 %
   * Merging task    4 100 %   * All task merged!         
TRANSLATION FINISHED
GENERATING PCF
   * Input PCF original/sendrecv.pcf correctly copied to dimemas/sendrecv.pcf
COPYING ROW FILE
prv trace converted to the dim format
created dimemas config file
-> Simulator configuration to be read small-dimemas.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration

10.000000000: END SIMULATION

**** Application 0 (sendrecv.dim) ****

**** Total Statistics ****

Execution time:	7.098312266
Speedup:	1.80 
CPU Time:	12.801200000

 Id.	Computation	%time	Communication
  0	3.293400000	31.69	0.003000000
  1	3.201243000	31.08	3.897069266
  2	3.144269000	30.70	0.041321000
  3	3.162288000	30.82	3.855702266

 Id.	Mess.sent	Bytes sent	Immediate recv	Waiting recv	Bytes recv	Coll.op.	Block time	Comm. time	Wait link time	Wait buses time	I/O time
  0	1.000000e+00	4.000000e+06	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.002000000	0.000000000	0.000000000	0.000000000
  1	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	4.000000e+06	1.000000e+00	0.037055000	0.002000000	0.000000000	0.000000000	0.000000000
  2	1.000000e+00	4.000000e+06	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.038321000	0.002000000	0.000000000	0.000000000	0.000000000
  3	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	4.000000e+06	1.000000e+00	0.035777000	0.002000000	0.000000000	0.000000000	0.000000000
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total	2.000000e+00	8.000000e+06	0.000000e+00	2.000000e+00	8.000000e+06	4.000000e+00	1.111530e+08	8.000000e+06	0.000000e+00	0.000000e+00	0.000000e+00	
Average	5.000000e-01	2.000000e+06	0.000000e+00	5.000000e-01	2.000000e+06	1.000000e+00	2.778825e+07	2.000000e+06	0.000000e+00	0.000000e+00	0.000000e+00	
Maximum	1.000000e+00	4.000000e+06	0.000000e+00	1.000000e+00	4.000000e+06	1.000000e+00	3.832100e+07	2.000000e+06	0.000000e+00	0.000000e+00	0.000000e+00	
Minimum	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000e+00	2.000000e+06	0.000000e+00	0.000000e+00	0.000000e+00	
Stdev	5.000000e-01	2.000000e+06	0.000000e+00	5.000000e-01	2.000000e+06	0.000000e+00	1.606875e+07	-nan	0.000000e+00	0.000000e+00	0.000000e+00	

Output Paraver trace "sendrecv-dimemas.prv" generated

dimemas simulation is done
got dimemas pjdump trace
prv trace converted to the tit format
created simgrid platform file
create simgrid deployment file
[rank 0] -> host0
[rank 1] -> host1
[rank 2] -> host0
[rank 3] -> host1
simgrid simulation is done
got simgrid pjdump trace
pjdumps filtered
null device 
          1 
#+end_example

Everything went fine. We got the Dimemas simulation result in the
dimemas directory, the simgrid results in the simgrid directory, the
original trace in the original directory and all the plot data and charts in the plot directory.

Here is the plot getpjdump generated:

[[file:experiments/11-May-2016/sendrecv1/plot/plot.png]]

We can see that the processes that sent the message were not blocked
even though the messages had a size of 1MB and the minimum rendez-vouz
message size for Dimemas was 32K.

* 2016-05-06 Installing OpenMPI + Extrae on Draco                     :Lucas:

I have download OpenMPI 1.10.2

#+begin_src sh :results output :session :exports both
wget https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.2.tar.bz2
#+end_src

Configured, compile it this way:

#+begin_src sh :results output :session :exports both
./configure --prefix=/home/schnorr/install/openmpi-1.10.2/ \
   --enable-static \ #so we can statically link mpi apps
   --without-cuda #because of cuda dependencies
make
make install
#+end_src

Then, download latest extrae 3.3.0, configure it this way:

Note that my stow directory has symlinks to the my openmpi installation.

#+begin_src sh :results output :session :exports both
MPICC=/home/schnorr/install/stow/bin/mpicc ./configure --prefix=/home/schnorr/install/extrae-3.3.0/ \
   --with-mpi=/home/schnorr/install/stow/lib/openmpi/ \
   --with-mpi-headers=/home/schnorr/install/stow/include/ \
   --with-mpi-libs=/home/schnorr/install/stow/lib/ \
   --without-unwind \
   --without-dyninst \
   --without-papi \
   --disable-openmp \
   --disable-online
make
make install
#+end_src

It worked! Now I just copy the openmpi and extrae installations to the
other draco nodes manually, since they do not have NFS.

#+begin_src R :results output :session :exports both
rsync --recursive --progress --links install draco2:.
rsync --recursive --progress --links install draco3:.
rsync --recursive --progress --links install draco4:.
#+end_src

I just found out that I have to run the configure/make/make install
steps for openmpi in all machines, without =rsync=. Machines are
configured differently, and it looks like some hardware is also
different. Another option would be to statically compile the
application, but I've found some problems with that also.

I do not have access to draco[5678] since they are reserved for
someone else as of this moment. Should play with four nodes to start.

I have to put the following line to all =.bashrc= files (make sure this
is placed before any decision about interactive or non-interactive
shells, especially in an ubuntu installation), so we can use the
correct MPI installation when compiling MPI codes.

#+BEGIN_EXAMPLE
export PATH=$HOME/install/stow/bin:$PATH
#+END_EXAMPLE

* 2016-05-06 Compiling and instrumenting Ondes3D for Draco            :Lucas:

I have a copy of Ondes3D in MPI, thanks to Fabrice Dupros from BRGM.

#+begin_src R :results output :session :exports both
git clone https://bitbucket.org/schnorr/ondes3d.git
cd ondes3d/Ondes3d-1.0/
#+end_src

In the =Makefile=, I disable the flag =-DOMP= and =-fopenmp= to disable
OpenMP. I keep only the flag =-DMPI= so we have a pure MPI
implementation of Ondes3D. I need to manually copy the input files
=NICE-XML= which is approximately 970 Megabytes.

#+begin_src sh :results output :session :exports both
rsync --progress --recursive NICE-XML draco1:./ondes3d/Ondes3d-1.0/
#+end_src

A dry run proves successfully.

Now I need to trace it with Extrae.

Here's my =extrae.xml= configuration file.

#+begin_src xml :results output :session :exports both :tangle extrae.xml
<?xml version='1.0'?>

<trace enabled="yes"
 home="/home/schnorr/install/extrae-3.3.0"
 initial-mode="detail"
 type="paraver"
 xml-parser-id="Id: xml-parse.c 3918 2016-03-11 14:59:01Z harald $"
>

  <mpi enabled="yes">
    <counters enabled="yes" />
  </mpi>

  <openmp enabled="no"/>
  <pthread enabled="no"/>

  <callers enabled="yes">
    <mpi enabled="yes">1-3</mpi>
    <sampling enabled="no">1-5</sampling>
    <dynamic-memory enabled="no">1-3</dynamic-memory>
  </callers>

  <user-functions enabled="no" list="/home/bsc41/bsc41273/user-functions.dat" exclude-automatic-functions="no">
    <counters enabled="yes" />
  </user-functions>

  <counters enabled="no"/>
  <storage enabled="no"/>
  <buffer enabled="yes">
    <size enabled="yes">500000</size>
    <circular enabled="no" />
  </buffer>

  <trace-control enabled="no">
    <file enabled="no" frequency="5M">/gpfs/scratch/bsc41/bsc41273/control</file>
    <global-ops enabled="no"></global-ops>
  </trace-control>

  <others enabled="yes">
    <minimum-time enabled="no">10M</minimum-time>
    <finalize-on-signal enabled="yes" 
      SIGUSR1="no" SIGUSR2="no" SIGINT="yes"
      SIGQUIT="yes" SIGTERM="yes" SIGXCPU="yes"
      SIGFPE="yes" SIGSEGV="yes" SIGABRT="yes"
    />
    <flush-sampling-buffer-at-instrumentation-point enabled="yes" />
  </others>

  <bursts enabled="no"/>
  <sampling enabled="no" type="default" period="50m" variability="10m" />
  <dynamic-memory enabled="no"/>
  <input-output enabled="no"/>

  <merge enabled="yes" 
    synchronization="default"
    tree-fan-out="16"
    max-memory="512"
    joint-states="yes"
    keep-mpits="yes"
    sort-addresses="yes"
    overwrite="yes"
  />

</trace>
#+end_src

Capturing using =LD_PRELOAD=.

#+begin_src sh :results output :session :exports both
export EXTRAE_CONFIG_FILE=/home/schnorr/ondes3d/Ondes3d-1.0/extrae.xml
export EXTRAE_HOME=/home/schnorr/install/stow/
export LD_PRELOAD=${EXTRAE_HOME}/lib/libmpitrace.so
mpirun -np 4 ./ondes3d
#+end_src

A local run worked with tracing.

Now, let's try a distributed one.

Contents of the machine file:

#+BEGIN_EXAMPLE
draco1 slots=16 max_slots=16
draco2 slots=16 max_slots=16
draco3 slots=16 max_slots=16
draco4 slots=16 max_slots=16
#+END_EXAMPLE

I stumble upon in a bunch of problems. Here's what I did (summary):
- configure and compiled openmpi in each machine
- have to manually copy input (NICE-XML) to all machines
- have to manually create output directory (NICE-OUTPUT)
- manually copy the binary =ondes3d= to $HOME
- then run MPI like this in your $HOME

#+BEGIN_EXAMPLE
mpirun  -mca btl_tcp_if_include em1 --mca btl tcp,self # use tcp, through the =em1= interface
        --machinefile machinefile \
        -np 64 \
        /home/schnorr/ondes3d
#+END_EXAMPLE

Now let's try a remote run with tracing.

I put the env vars below in my =.bashrc=, before the check for
non-interactive shells. Copy that =.bashrc= to all hosts involved in the
execution. Have also to copy the =extrae.xml= file to all hosts
manually.

#+begin_src sh :results output :session :exports both
export EXTRAE_CONFIG_FILE=/home/schnorr/extrae.xml
export EXTRAE_HOME=/home/schnorr/install/extrae-3.3.0/
export LD_PRELOAD=${EXTRAE_HOME}/lib/libmpitrace.so
#+end_src

Since I don't have NFS on this cluster, I have to disable the
intermediate automatic merge so I can do it manually later on after
the execution. My =extrae.xml= reflects that (see =merge= tag below),
with:

#+begin_src xml :results output :session :exports both :tangle extrae-mpi-draco.xml
<?xml version='1.0'?>

<trace enabled="yes"
 home="/home/schnorr/install/extrae-3.3.0"
 initial-mode="detail"
 type="paraver"
 xml-parser-id="Id: xml-parse.c 3918 2016-03-11 14:59:01Z harald $"
>

  <mpi enabled="yes">
    <counters enabled="yes" />
  </mpi>

  <openmp enabled="no"/>
  <pthread enabled="no"/>

  <callers enabled="yes">
    <mpi enabled="yes">1-3</mpi>
    <sampling enabled="no">1-5</sampling>
    <dynamic-memory enabled="no">1-3</dynamic-memory>
  </callers>

  <user-functions enabled="no" list="/home/bsc41/bsc41273/user-functions.dat" exclude-automatic-functions="no">
    <counters enabled="yes" />
  </user-functions>

  <counters enabled="no"/>
  <storage enabled="no"/>
  <buffer enabled="yes">
    <size enabled="yes">500000</size>
    <circular enabled="no" />
  </buffer>

  <trace-control enabled="no">
    <file enabled="no" frequency="5M">/gpfs/scratch/bsc41/bsc41273/control</file>
    <global-ops enabled="no"></global-ops>
  </trace-control>

  <others enabled="yes">
    <minimum-time enabled="no">10M</minimum-time>
    <finalize-on-signal enabled="yes" 
      SIGUSR1="no" SIGUSR2="no" SIGINT="yes"
      SIGQUIT="yes" SIGTERM="yes" SIGXCPU="yes"
      SIGFPE="yes" SIGSEGV="yes" SIGABRT="yes"
    />
    <flush-sampling-buffer-at-instrumentation-point enabled="yes" />
  </others>

  <bursts enabled="no"/>
  <sampling enabled="no"/>
  <dynamic-memory enabled="no"/>
  <input-output enabled="no"/>
  <merge enabled="no"/>

</trace>
#+end_src

Copy the =extrae.xml= and =.bashrc= with the =LD_PRELOAD= to all nodes:

#+begin_src sh :results output :session :exports both
for i in `seq 1 4`; do scp extrae.xml .bashrc  draco$i:./; done
#+end_src

Run the program, I got a bunch of =mpit= files. Put all of them on
=draco1= (run the code below from =draco1=):

#+begin_src sh :results output :session :exports both
for i in `seq 2 4`; do rsync draco$i:./set-0/* ./set-0/; done
#+end_src

Now I need to convert this to a single =prv= file, so like this:

See:
- https://www.bsc.es/computer-sciences/performance-tools/trace-generation/extrae/extrae-user-guide#SECTION00430000000000000000

#+begin_src sh :results output :session :exports both
${EXTRAE_HOME}/bin/mpi2prv -f TRACE*.mpit -o output.prv
#+end_src

Great, I have these three files now:

#+BEGIN_EXAMPLE
schnorr@draco1:~/set-0$ ls -larth output-ondes3d-64p-draco_1-4.*
-rw-rw-r-- 1 schnorr schnorr 112M May  6 17:46 output-ondes3d-64p-draco_1-4.prv
-rw-rw-r-- 1 schnorr schnorr 2.8K May  6 17:46 output-ondes3d-64p-draco_1-4.row
-rw-rw-r-- 1 schnorr schnorr 5.4K May  6 17:46 output-ondes3d-64p-draco_1-4.pcf
#+END_EXAMPLE

I have copied these to:
https://dl.dropboxusercontent.com/u/1119921/output-ondes3d-64p-draco_1-4.tar.gz

Tiago will take a look to see what he can do it.

There are 64 procs; each host got 16 procs (one per physical core not pinned).

Remember next time to pin MPI processes.

Hosts have the same hardware; network is ethernet/tcp. Running =iperf=, got:

From draco2 (client) to draco1 (server)

#+BEGIN_EXAMPLE
schnorr@draco2:~$ iperf -c draco1
------------------------------------------------------------
Client connecting to draco1, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 143.54.12.43 port 47169 connected with 143.54.12.246 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  1.10 GBytes   943 Mbits/sec
#+END_EXAMPLE

From draco1 (client) to draco2 (server)

#+BEGIN_EXAMPLE
schnorr@draco1:~$ iperf -c draco2
------------------------------------------------------------
Client connecting to draco2, TCP port 5001
TCP window size: 85.0 KByte (default)
------------------------------------------------------------
[  3] local 143.54.12.246 port 34310 connected with 143.54.12.43 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  1.10 GBytes   943 Mbits/sec
#+END_EXAMPLE

Result of ping from draco1 to draco2:

#+BEGIN_EXAMPLE
schnorr@draco1:~$ ping draco2
PING draco2.inf.ufrgs.br (143.54.12.43) 56(84) bytes of data.
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=1 ttl=64 time=0.231 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=2 ttl=64 time=0.202 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=3 ttl=64 time=0.216 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=4 ttl=64 time=0.192 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=5 ttl=64 time=0.157 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=6 ttl=64 time=0.197 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=7 ttl=64 time=0.164 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=8 ttl=64 time=0.212 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=9 ttl=64 time=0.198 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=10 ttl=64 time=0.218 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=11 ttl=64 time=0.256 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=12 ttl=64 time=0.178 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=13 ttl=64 time=0.220 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=14 ttl=64 time=0.207 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=15 ttl=64 time=0.200 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=16 ttl=64 time=0.215 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=17 ttl=64 time=0.187 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=18 ttl=64 time=0.210 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=19 ttl=64 time=0.171 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=20 ttl=64 time=0.180 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=21 ttl=64 time=0.186 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=22 ttl=64 time=0.187 ms
64 bytes from draco2.inf.ufrgs.br (143.54.12.43): icmp_seq=23 ttl=64 time=0.177 ms
#+END_EXAMPLE

Results of ping from draco2 to draco1:

#+BEGIN_EXAMPLE
schnorr@draco2:~$ ping draco1
PING draco1.inf.ufrgs.br (143.54.12.246) 56(84) bytes of data.
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=1 ttl=64 time=0.212 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=2 ttl=64 time=0.241 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=3 ttl=64 time=0.206 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=4 ttl=64 time=0.201 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=5 ttl=64 time=0.211 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=6 ttl=64 time=0.173 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=7 ttl=64 time=0.201 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=8 ttl=64 time=0.222 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=9 ttl=64 time=0.183 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=10 ttl=64 time=0.192 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=11 ttl=64 time=0.176 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=12 ttl=64 time=0.176 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=13 ttl=64 time=0.195 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=14 ttl=64 time=0.216 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=15 ttl=64 time=0.195 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=16 ttl=64 time=0.203 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=17 ttl=64 time=0.198 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=18 ttl=64 time=0.173 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=19 ttl=64 time=0.211 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=20 ttl=64 time=0.167 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=21 ttl=64 time=0.176 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=22 ttl=64 time=0.177 ms
64 bytes from draco1.inf.ufrgs.br (143.54.12.246): icmp_seq=23 ttl=64 time=0.183 ms
#+END_EXAMPLE

Consider these to define latency and bandwidth.

If I remember correctly, latency is the time reported by ping divided by 2.
* 2016-05-11 Regarding asynchronous/block message send/recv    :Lucas:Arnaud:

We have written this:

#+BEGIN_EXAMPLE
> Tiago made some comparisons between SimGrid and Dimemas and observed
> that sends are asynchronous no matter the message size in Dimemas
> until the beginning of the corresponding receive. In SimGrid, they
> act as expected being blocked, i.e., so the send keeps blocking the
> process until the receive is done on the receiver side. 
#+END_EXAMPLE

Arnaud just reminded us on how it works in SimGrid's SMPI:

This depends on the configuration of SMPI:
- http://simgrid.gforge.inria.fr/simgrid/latest/doc/options.html#options_smpi
Have a look at:
- http://simgrid.gforge.inria.fr/simgrid/latest/doc/options.html#options_model_smpi_detached

Tiago will have to consider this.

* 2016-05-13 Message by Judit with MN3 Dimemas configuration file + more :Lucas:
* 2016-05-13 Meeting with Tiago                                 :Lucas:Tiago:

_Tasks_
- From the =MN3.cfg= configuration file sent by Judit
  - Create a XML platform file for SimGrid

- Concerning =lulesh2_27p_12ppn_default= traces (just sent by Judit)
  - Replay this with Dimemas using =-S 32K= as Judit suggested (and =MN3.cfg=)
  - Convert the =prv= file to =tit=, so we can replay with SimGrid

- Get yourself _acquainted_ with G5K platform
  - See [[*2016-05-06 Installing OpenMPI + Extrae on Draco][2016-05-06 Installing OpenMPI + Extrae on Draco]] on how to
    install OpenMPI + Extrae there (you may adapt some commands)
- Depending on which cluster you have selected in G5K, you do:
  - SimGrid's XML platform for it
  - Dimemas platform configuration

- Resolve the conversion problem of the trace obtained in =draco=
  - Four machines involved: from draco1 to draco4
  - The problem is this:
    #+BEGIN_EXAMPLE
    $ time perl ../getpjdump/prv2tit.pl  -i output-ondes3d-64p-draco_1-4
Can't use an undefined value as a HASH reference at ../getpjdump/prv2tit.pl line 202, <INPUT> line 1860723.

real	0m49.905s
user	0m49.728s
sys	0m0.160s
    #+END_EXAMPLE
  - Tiago may modified the conversion script to dump all the times we
    have complete =tit= events. As of today, the script keeps everything
    in memory and dumps only in the end. Since we won't reach the end
    because of the error, we loose everything.

- Do =draco= SimGrid's XML platform file and the same for Dimemas
  - Replay =output-ondes3d-64p-draco_1-4= in SimGrid and Dimemas
  - Compare the results with real trace

Here's how to convert from prv to dim in order to replay with Dimemas:

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
~/install/dimemas-5.2.12/bin/prv2dim output-ondes3d-64p-draco_1-4.prv output-ondes3d-64p-draco_1-4.dim
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4
INITIALIZING PARSER... OK!
SPLITTING COMMUNICATIONS 000 %SPLITTING COMMUNICATIONS 001 %SPLITTING COMMUNICATIONS 002 %SPLITTING COMMUNICATIONS 003 %SPLITTING COMMUNICATIONS 004 %SPLITTING COMMUNICATIONS 005 %SPLITTING COMMUNICATIONS 006 %SPLITTING COMMUNICATIONS 007 %SPLITTING COMMUNICATIONS 008 %SPLITTING COMMUNICATIONS 009 %SPLITTING COMMUNICATIONS 010 %SPLITTING COMMUNICATIONS 011 %SPLITTING COMMUNICATIONS 012 %SPLITTING COMMUNICATIONS 013 %SPLITTING COMMUNICATIONS 014 %SPLITTING COMMUNICATIONS 015 %SPLITTING COMMUNICATIONS 016 %SPLITTING COMMUNICATIONS 017 %SPLITTING COMMUNICATIONS 018 %SPLITTING COMMUNICATIONS 019 %SPLITTING COMMUNICATIONS 020 %SPLITTING COMMUNICATIONS 021 %SPLITTING COMMUNICATIONS 022 %SPLITTING COMMUNICATIONS 023 %SPLITTING COMMUNICATIONS 024 %SPLITTING COMMUNICATIONS 025 %SPLITTING COMMUNICATIONS 026 %SPLITTING COMMUNICATIONS 027 %SPLITTING COMMUNICATIONS 028 %SPLITTING COMMUNICATIONS 029 %SPLITTING COMMUNICATIONS 030 %SPLITTING COMMUNICATIONS 031 %SPLITTING COMMUNICATIONS 032 %SPLITTING COMMUNICATIONS 033 %SPLITTING COMMUNICATIONS 034 %SPLITTING COMMUNICATIONS 035 %SPLITTING COMMUNICATIONS 036 %SPLITTING COMMUNICATIONS 037 %SPLITTING COMMUNICATIONS 038 %SPLITTING COMMUNICATIONS 039 %SPLITTING COMMUNICATIONS 040 %SPLITTING COMMUNICATIONS 041 %SPLITTING COMMUNICATIONS 042 %SPLITTING COMMUNICATIONS 043 %SPLITTING COMMUNICATIONS 044 %SPLITTING COMMUNICATIONS 045 %SPLITTING COMMUNICATIONS 046 %SPLITTING COMMUNICATIONS 047 %SPLITTING COMMUNICATIONS 048 %SPLITTING COMMUNICATIONS 049 %SPLITTING COMMUNICATIONS 050 %SPLITTING COMMUNICATIONS 051 %SPLITTING COMMUNICATIONS 052 %SPLITTING COMMUNICATIONS 053 %SPLITTING COMMUNICATIONS 054 %SPLITTING COMMUNICATIONS 055 %SPLITTING COMMUNICATIONS 056 %SPLITTING COMMUNICATIONS 057 %SPLITTING COMMUNICATIONS 058 %SPLITTING COMMUNICATIONS 059 %SPLITTING COMMUNICATIONS 060 %SPLITTING COMMUNICATIONS 061 %SPLITTING COMMUNICATIONS 062 %SPLITTING COMMUNICATIONS 063 %SPLITTING COMMUNICATIONS 064 %SPLITTING COMMUNICATIONS 065 %SPLITTING COMMUNICATIONS 066 %SPLITTING COMMUNICATIONS 067 %SPLITTING COMMUNICATIONS 068 %SPLITTING COMMUNICATIONS 069 %SPLITTING COMMUNICATIONS 070 %SPLITTING COMMUNICATIONS 071 %SPLITTING COMMUNICATIONS 072 %SPLITTING COMMUNICATIONS 073 %SPLITTING COMMUNICATIONS 074 %SPLITTING COMMUNICATIONS 075 %SPLITTING COMMUNICATIONS 076 %SPLITTING COMMUNICATIONS 077 %SPLITTING COMMUNICATIONS 078 %SPLITTING COMMUNICATIONS 079 %SPLITTING COMMUNICATIONS 080 %SPLITTING COMMUNICATIONS 081 %SPLITTING COMMUNICATIONS 082 %SPLITTING COMMUNICATIONS 083 %SPLITTING COMMUNICATIONS 084 %SPLITTING COMMUNICATIONS 085 %SPLITTING COMMUNICATIONS 086 %SPLITTING COMMUNICATIONS 087 %SPLITTING COMMUNICATIONS 088 %SPLITTING COMMUNICATIONS 089 %SPLITTING COMMUNICATIONS 090 %SPLITTING COMMUNICATIONS 091 %SPLITTING COMMUNICATIONS 092 %SPLITTING COMMUNICATIONS 093 %SPLITTING COMMUNICATIONS 094 %SPLITTING COMMUNICATIONS 095 %SPLITTING COMMUNICATIONS 096 %SPLITTING COMMUNICATIONS 097 %SPLITTING COMMUNICATIONS 098 %SPLITTING COMMUNICATIONS 099 %SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          971792
   * Splitted communications 248760
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK
CREATING TRANSLATION STRUCTURES  001/064CREATING TRANSLATION STRUCTURES  002/064CREATING TRANSLATION STRUCTURES  003/064CREATING TRANSLATION STRUCTURES  004/064CREATING TRANSLATION STRUCTURES  005/064CREATING TRANSLATION STRUCTURES  006/064CREATING TRANSLATION STRUCTURES  007/064CREATING TRANSLATION STRUCTURES  008/064CREATING TRANSLATION STRUCTURES  009/064CREATING TRANSLATION STRUCTURES  010/064CREATING TRANSLATION STRUCTURES  011/064CREATING TRANSLATION STRUCTURES  012/064CREATING TRANSLATION STRUCTURES  013/064CREATING TRANSLATION STRUCTURES  014/064CREATING TRANSLATION STRUCTURES  015/064CREATING TRANSLATION STRUCTURES  016/064CREATING TRANSLATION STRUCTURES  017/064CREATING TRANSLATION STRUCTURES  018/064CREATING TRANSLATION STRUCTURES  019/064CREATING TRANSLATION STRUCTURES  020/064CREATING TRANSLATION STRUCTURES  021/064CREATING TRANSLATION STRUCTURES  022/064CREATING TRANSLATION STRUCTURES  023/064CREATING TRANSLATION STRUCTURES  024/064CREATING TRANSLATION STRUCTURES  025/064CREATING TRANSLATION STRUCTURES  026/064CREATING TRANSLATION STRUCTURES  027/064CREATING TRANSLATION STRUCTURES  028/064CREATING TRANSLATION STRUCTURES  029/064CREATING TRANSLATION STRUCTURES  030/064CREATING TRANSLATION STRUCTURES  031/064CREATING TRANSLATION STRUCTURES  032/064CREATING TRANSLATION STRUCTURES  033/064CREATING TRANSLATION STRUCTURES  034/064CREATING TRANSLATION STRUCTURES  035/064CREATING TRANSLATION STRUCTURES  036/064CREATING TRANSLATION STRUCTURES  037/064CREATING TRANSLATION STRUCTURES  038/064CREATING TRANSLATION STRUCTURES  039/064CREATING TRANSLATION STRUCTURES  040/064CREATING TRANSLATION STRUCTURES  041/064CREATING TRANSLATION STRUCTURES  042/064CREATING TRANSLATION STRUCTURES  043/064CREATING TRANSLATION STRUCTURES  044/064CREATING TRANSLATION STRUCTURES  045/064CREATING TRANSLATION STRUCTURES  046/064CREATING TRANSLATION STRUCTURES  047/064CREATING TRANSLATION STRUCTURES  048/064CREATING TRANSLATION STRUCTURES  049/064CREATING TRANSLATION STRUCTURES  050/064CREATING TRANSLATION STRUCTURES  051/064CREATING TRANSLATION STRUCTURES  052/064CREATING TRANSLATION STRUCTURES  053/064CREATING TRANSLATION STRUCTURES  054/064CREATING TRANSLATION STRUCTURES  055/064CREATING TRANSLATION STRUCTURES  056/064CREATING TRANSLATION STRUCTURES  057/064CREATING TRANSLATION STRUCTURES  058/064CREATING TRANSLATION STRUCTURES  059/064CREATING TRANSLATION STRUCTURES  060/064CREATING TRANSLATION STRUCTURES  061/064CREATING TRANSLATION STRUCTURES  062/064CREATING TRANSLATION STRUCTURES  063/064CREATING TRANSLATION STRUCTURES  064/064CREATING TRANSLATION STRUCTURES  064/064
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION
TRANSLATING RECORDS 000 %TRANSLATING RECORDS 001 %TRANSLATING RECORDS 002 %TRANSLATING RECORDS 003 %TRANSLATING RECORDS 004 %TRANSLATING RECORDS 005 %TRANSLATING RECORDS 006 %TRANSLATING RECORDS 007 %TRANSLATING RECORDS 008 %TRANSLATING RECORDS 009 %TRANSLATING RECORDS 010 %TRANSLATING RECORDS 011 %TRANSLATING RECORDS 012 %TRANSLATING RECORDS 013 %TRANSLATING RECORDS 014 %TRANSLATING RECORDS 015 %TRANSLATING RECORDS 016 %TRANSLATING RECORDS 017 %TRANSLATING RECORDS 018 %TRANSLATING RECORDS 019 %TRANSLATING RECORDS 020 %TRANSLATING RECORDS 021 %TRANSLATING RECORDS 022 %TRANSLATING RECORDS 023 %TRANSLATING RECORDS 024 %TRANSLATING RECORDS 025 %TRANSLATING RECORDS 026 %TRANSLATING RECORDS 027 %TRANSLATING RECORDS 028 %TRANSLATING RECORDS 029 %TRANSLATING RECORDS 030 %TRANSLATING RECORDS 031 %TRANSLATING RECORDS 032 %TRANSLATING RECORDS 033 %TRANSLATING RECORDS 034 %TRANSLATING RECORDS 035 %TRANSLATING RECORDS 036 %TRANSLATING RECORDS 037 %TRANSLATING RECORDS 038 %TRANSLATING RECORDS 039 %TRANSLATING RECORDS 040 %TRANSLATING RECORDS 041 %TRANSLATING RECORDS 042 %TRANSLATING RECORDS 043 %TRANSLATING RECORDS 044 %TRANSLATING RECORDS 045 %TRANSLATING RECORDS 046 %TRANSLATING RECORDS 047 %TRANSLATING RECORDS 048 %TRANSLATING RECORDS 049 %TRANSLATING RECORDS 050 %TRANSLATING RECORDS 051 %TRANSLATING RECORDS 052 %TRANSLATING RECORDS 053 %TRANSLATING RECORDS 054 %TRANSLATING RECORDS 055 %TRANSLATING RECORDS 056 %TRANSLATING RECORDS 057 %TRANSLATING RECORDS 058 %TRANSLATING RECORDS 059 %TRANSLATING RECORDS 060 %TRANSLATING RECORDS 061 %TRANSLATING RECORDS 062 %TRANSLATING RECORDS 063 %TRANSLATING RECORDS 064 %TRANSLATING RECORDS 065 %TRANSLATING RECORDS 066 %TRANSLATING RECORDS 067 %TRANSLATING RECORDS 068 %TRANSLATING RECORDS 069 %TRANSLATING RECORDS 070 %TRANSLATING RECORDS 071 %TRANSLATING RECORDS 072 %TRANSLATING RECORDS 073 %TRANSLATING RECORDS 074 %TRANSLATING RECORDS 075 %TRANSLATING RECORDS 076 %TRANSLATING RECORDS 077 %TRANSLATING RECORDS 078 %TRANSLATING RECORDS 079 %TRANSLATING RECORDS 080 %TRANSLATING RECORDS 081 %TRANSLATING RECORDS 082 %TRANSLATING RECORDS 083 %TRANSLATING RECORDS 084 %TRANSLATING RECORDS 085 %TRANSLATING RECORDS 086 %TRANSLATING RECORDS 087 %TRANSLATING RECORDS 088 %TRANSLATING RECORDS 089 %TRANSLATING RECORDS 090 %TRANSLATING RECORDS 091 %TRANSLATING RECORDS 092 %TRANSLATING RECORDS 093 %TRANSLATING RECORDS 094 %TRANSLATING RECORDS 095 %TRANSLATING RECORDS 096 %TRANSLATING RECORDS 097 %TRANSLATING RECORDS 098 %TRANSLATING RECORDS 099 %TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES
   * Merging task    1 026 %   * Merging task    1 053 %   * Merging task    1 079 %   * Merging task    1 100 %   * Merging task    2 064 %   * Merging task    2 100 %   * Merging task    3 064 %   * Merging task    3 100 %   * Merging task    4 064 %   * Merging task    4 100 %   * Merging task    5 065 %   * Merging task    5 100 %   * Merging task    6 064 %   * Merging task    6 100 %   * Merging task    7 065 %   * Merging task    7 100 %   * Merging task    8 085 %   * Merging task    8 100 %   * Merging task    9 068 %   * Merging task    9 100 %   * Merging task   10 051 %   * Merging task   10 100 %   * Merging task   11 049 %   * Merging task   11 099 %   * Merging task   11 100 %   * Merging task   12 049 %   * Merging task   12 099 %   * Merging task   12 100 %   * Merging task   13 049 %   * Merging task   13 099 %   * Merging task   13 100 %   * Merging task   14 049 %   * Merging task   14 098 %   * Merging task   14 100 %   * Merging task   15 049 %   * Merging task   15 099 %   * Merging task   15 100 %   * Merging task   16 062 %   * Merging task   16 100 %   * Merging task   17 100 %   * Merging task   18 100 %   * Merging task   19 097 %   * Merging task   19 100 %   * Merging task   20 097 %   * Merging task   20 100 %   * Merging task   21 097 %   * Merging task   21 100 %   * Merging task   22 096 %   * Merging task   22 100 %   * Merging task   23 097 %   * Merging task   23 100 %   * Merging task   24 100 %   * Merging task   25 100 %   * Merging task   26 091 %   * Merging task   26 100 %   * Merging task   27 095 %   * Merging task   27 100 %   * Merging task   28 096 %   * Merging task   28 100 %   * Merging task   29 097 %   * Merging task   29 100 %   * Merging task   30 096 %   * Merging task   30 100 %   * Merging task   31 097 %   * Merging task   31 100 %   * Merging task   32 100 %   * Merging task   33 062 %   * Merging task   33 100 %   * Merging task   34 047 %   * Merging task   34 094 %   * Merging task   34 100 %   * Merging task   35 046 %   * Merging task   35 091 %   * Merging task   35 100 %   * Merging task   36 048 %   * Merging task   36 096 %   * Merging task   36 100 %   * Merging task   37 049 %   * Merging task   37 098 %   * Merging task   37 100 %   * Merging task   38 049 %   * Merging task   38 098 %   * Merging task   38 100 %   * Merging task   39 049 %   * Merging task   39 099 %   * Merging task   39 100 %   * Merging task   40 062 %   * Merging task   40 100 %   * Merging task   41 060 %   * Merging task   41 100 %   * Merging task   42 045 %   * Merging task   42 091 %   * Merging task   42 100 %   * Merging task   43 046 %   * Merging task   43 092 %   * Merging task   43 100 %   * Merging task   44 047 %   * Merging task   44 095 %   * Merging task   44 100 %   * Merging task   45 046 %   * Merging task   45 093 %   * Merging task   45 100 %   * Merging task   46 047 %   * Merging task   46 093 %   * Merging task   46 100 %   * Merging task   47 048 %   * Merging task   47 096 %   * Merging task   47 100 %   * Merging task   48 060 %   * Merging task   48 100 %   * Merging task   49 060 %   * Merging task   49 100 %   * Merging task   50 047 %   * Merging task   50 095 %   * Merging task   50 100 %   * Merging task   51 049 %   * Merging task   51 097 %   * Merging task   51 100 %   * Merging task   52 048 %   * Merging task   52 096 %   * Merging task   52 100 %   * Merging task   53 048 %   * Merging task   53 096 %   * Merging task   53 100 %   * Merging task   54 046 %   * Merging task   54 092 %   * Merging task   54 100 %   * Merging task   55 047 %   * Merging task   55 095 %   * Merging task   55 100 %   * Merging task   56 060 %   * Merging task   56 100 %   * Merging task   57 078 %   * Merging task   57 100 %   * Merging task   58 059 %   * Merging task   58 100 %   * Merging task   59 060 %   * Merging task   59 100 %   * Merging task   60 060 %   * Merging task   60 100 %   * Merging task   61 059 %   * Merging task   61 100 %   * Merging task   62 056 %   * Merging task   62 100 %   * Merging task   63 058 %   * Merging task   63 100 %   * Merging task   64 078 %   * Merging task   64 100 %   * All task merged!         
********************************************************************************
 *                               ERROR                                          *
********************************************************************************
ERROR: The 'MPI_Init' primitive does not appear at the beggining of the all
ERROR: tasks in this trace. Current translation will fail

ERROR: Please, re-run the translator using the '-s' flags
********************************************************************************

TRANSLATION FINISHED
GENERATING PCF
-> Input PCF and ouput PCF have the same name. Please use it in your simulations
COPYING ROW FILE
-> Input and ouput ROW files have the same name. Please use it in your simulations
#+end_example

There is a problem (see =ERROR= above). So, let's use the =-s= flag as
recommended.

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
~/install/dimemas-5.2.12/bin/prv2dim -s output-ondes3d-64p-draco_1-4.prv output-ondes3d-64p-draco_1-4.dim
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4
INITIALIZING PARSER... OK!
SPLITTING COMMUNICATIONS 000 %SPLITTING COMMUNICATIONS 001 %SPLITTING COMMUNICATIONS 002 %SPLITTING COMMUNICATIONS 003 %SPLITTING COMMUNICATIONS 004 %SPLITTING COMMUNICATIONS 005 %SPLITTING COMMUNICATIONS 006 %SPLITTING COMMUNICATIONS 007 %SPLITTING COMMUNICATIONS 008 %SPLITTING COMMUNICATIONS 009 %SPLITTING COMMUNICATIONS 010 %SPLITTING COMMUNICATIONS 011 %SPLITTING COMMUNICATIONS 012 %SPLITTING COMMUNICATIONS 013 %SPLITTING COMMUNICATIONS 014 %SPLITTING COMMUNICATIONS 015 %SPLITTING COMMUNICATIONS 016 %SPLITTING COMMUNICATIONS 017 %SPLITTING COMMUNICATIONS 018 %SPLITTING COMMUNICATIONS 019 %SPLITTING COMMUNICATIONS 020 %SPLITTING COMMUNICATIONS 021 %SPLITTING COMMUNICATIONS 022 %SPLITTING COMMUNICATIONS 023 %SPLITTING COMMUNICATIONS 024 %SPLITTING COMMUNICATIONS 025 %SPLITTING COMMUNICATIONS 026 %SPLITTING COMMUNICATIONS 027 %SPLITTING COMMUNICATIONS 028 %SPLITTING COMMUNICATIONS 029 %SPLITTING COMMUNICATIONS 030 %SPLITTING COMMUNICATIONS 031 %SPLITTING COMMUNICATIONS 032 %SPLITTING COMMUNICATIONS 033 %SPLITTING COMMUNICATIONS 034 %SPLITTING COMMUNICATIONS 035 %SPLITTING COMMUNICATIONS 036 %SPLITTING COMMUNICATIONS 037 %SPLITTING COMMUNICATIONS 038 %SPLITTING COMMUNICATIONS 039 %SPLITTING COMMUNICATIONS 040 %SPLITTING COMMUNICATIONS 041 %SPLITTING COMMUNICATIONS 042 %SPLITTING COMMUNICATIONS 043 %SPLITTING COMMUNICATIONS 044 %SPLITTING COMMUNICATIONS 045 %SPLITTING COMMUNICATIONS 046 %SPLITTING COMMUNICATIONS 047 %SPLITTING COMMUNICATIONS 048 %SPLITTING COMMUNICATIONS 049 %SPLITTING COMMUNICATIONS 050 %SPLITTING COMMUNICATIONS 051 %SPLITTING COMMUNICATIONS 052 %SPLITTING COMMUNICATIONS 053 %SPLITTING COMMUNICATIONS 054 %SPLITTING COMMUNICATIONS 055 %SPLITTING COMMUNICATIONS 056 %SPLITTING COMMUNICATIONS 057 %SPLITTING COMMUNICATIONS 058 %SPLITTING COMMUNICATIONS 059 %SPLITTING COMMUNICATIONS 060 %SPLITTING COMMUNICATIONS 061 %SPLITTING COMMUNICATIONS 062 %SPLITTING COMMUNICATIONS 063 %SPLITTING COMMUNICATIONS 064 %SPLITTING COMMUNICATIONS 065 %SPLITTING COMMUNICATIONS 066 %SPLITTING COMMUNICATIONS 067 %SPLITTING COMMUNICATIONS 068 %SPLITTING COMMUNICATIONS 069 %SPLITTING COMMUNICATIONS 070 %SPLITTING COMMUNICATIONS 071 %SPLITTING COMMUNICATIONS 072 %SPLITTING COMMUNICATIONS 073 %SPLITTING COMMUNICATIONS 074 %SPLITTING COMMUNICATIONS 075 %SPLITTING COMMUNICATIONS 076 %SPLITTING COMMUNICATIONS 077 %SPLITTING COMMUNICATIONS 078 %SPLITTING COMMUNICATIONS 079 %SPLITTING COMMUNICATIONS 080 %SPLITTING COMMUNICATIONS 081 %SPLITTING COMMUNICATIONS 082 %SPLITTING COMMUNICATIONS 083 %SPLITTING COMMUNICATIONS 084 %SPLITTING COMMUNICATIONS 085 %SPLITTING COMMUNICATIONS 086 %SPLITTING COMMUNICATIONS 087 %SPLITTING COMMUNICATIONS 088 %SPLITTING COMMUNICATIONS 089 %SPLITTING COMMUNICATIONS 090 %SPLITTING COMMUNICATIONS 091 %SPLITTING COMMUNICATIONS 092 %SPLITTING COMMUNICATIONS 093 %SPLITTING COMMUNICATIONS 094 %SPLITTING COMMUNICATIONS 095 %SPLITTING COMMUNICATIONS 096 %SPLITTING COMMUNICATIONS 097 %SPLITTING COMMUNICATIONS 098 %SPLITTING COMMUNICATIONS 099 %SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          971792
   * Splitted communications 248760
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK
CREATING TRANSLATION STRUCTURES  001/064CREATING TRANSLATION STRUCTURES  002/064CREATING TRANSLATION STRUCTURES  003/064CREATING TRANSLATION STRUCTURES  004/064CREATING TRANSLATION STRUCTURES  005/064CREATING TRANSLATION STRUCTURES  006/064CREATING TRANSLATION STRUCTURES  007/064CREATING TRANSLATION STRUCTURES  008/064CREATING TRANSLATION STRUCTURES  009/064CREATING TRANSLATION STRUCTURES  010/064CREATING TRANSLATION STRUCTURES  011/064CREATING TRANSLATION STRUCTURES  012/064CREATING TRANSLATION STRUCTURES  013/064CREATING TRANSLATION STRUCTURES  014/064CREATING TRANSLATION STRUCTURES  015/064CREATING TRANSLATION STRUCTURES  016/064CREATING TRANSLATION STRUCTURES  017/064CREATING TRANSLATION STRUCTURES  018/064CREATING TRANSLATION STRUCTURES  019/064CREATING TRANSLATION STRUCTURES  020/064CREATING TRANSLATION STRUCTURES  021/064CREATING TRANSLATION STRUCTURES  022/064CREATING TRANSLATION STRUCTURES  023/064CREATING TRANSLATION STRUCTURES  024/064CREATING TRANSLATION STRUCTURES  025/064CREATING TRANSLATION STRUCTURES  026/064CREATING TRANSLATION STRUCTURES  027/064CREATING TRANSLATION STRUCTURES  028/064CREATING TRANSLATION STRUCTURES  029/064CREATING TRANSLATION STRUCTURES  030/064CREATING TRANSLATION STRUCTURES  031/064CREATING TRANSLATION STRUCTURES  032/064CREATING TRANSLATION STRUCTURES  033/064CREATING TRANSLATION STRUCTURES  034/064CREATING TRANSLATION STRUCTURES  035/064CREATING TRANSLATION STRUCTURES  036/064CREATING TRANSLATION STRUCTURES  037/064CREATING TRANSLATION STRUCTURES  038/064CREATING TRANSLATION STRUCTURES  039/064CREATING TRANSLATION STRUCTURES  040/064CREATING TRANSLATION STRUCTURES  041/064CREATING TRANSLATION STRUCTURES  042/064CREATING TRANSLATION STRUCTURES  043/064CREATING TRANSLATION STRUCTURES  044/064CREATING TRANSLATION STRUCTURES  045/064CREATING TRANSLATION STRUCTURES  046/064CREATING TRANSLATION STRUCTURES  047/064CREATING TRANSLATION STRUCTURES  048/064CREATING TRANSLATION STRUCTURES  049/064CREATING TRANSLATION STRUCTURES  050/064CREATING TRANSLATION STRUCTURES  051/064CREATING TRANSLATION STRUCTURES  052/064CREATING TRANSLATION STRUCTURES  053/064CREATING TRANSLATION STRUCTURES  054/064CREATING TRANSLATION STRUCTURES  055/064CREATING TRANSLATION STRUCTURES  056/064CREATING TRANSLATION STRUCTURES  057/064CREATING TRANSLATION STRUCTURES  058/064CREATING TRANSLATION STRUCTURES  059/064CREATING TRANSLATION STRUCTURES  060/064CREATING TRANSLATION STRUCTURES  061/064CREATING TRANSLATION STRUCTURES  062/064CREATING TRANSLATION STRUCTURES  063/064CREATING TRANSLATION STRUCTURES  064/064CREATING TRANSLATION STRUCTURES  064/064
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION
TRANSLATING RECORDS 000 %TRANSLATING RECORDS 001 %TRANSLATING RECORDS 002 %TRANSLATING RECORDS 003 %TRANSLATING RECORDS 004 %TRANSLATING RECORDS 005 %TRANSLATING RECORDS 006 %TRANSLATING RECORDS 007 %TRANSLATING RECORDS 008 %TRANSLATING RECORDS 009 %TRANSLATING RECORDS 010 %TRANSLATING RECORDS 011 %TRANSLATING RECORDS 012 %TRANSLATING RECORDS 013 %TRANSLATING RECORDS 014 %TRANSLATING RECORDS 015 %TRANSLATING RECORDS 016 %TRANSLATING RECORDS 017 %TRANSLATING RECORDS 018 %TRANSLATING RECORDS 019 %TRANSLATING RECORDS 020 %TRANSLATING RECORDS 021 %TRANSLATING RECORDS 022 %TRANSLATING RECORDS 023 %TRANSLATING RECORDS 024 %TRANSLATING RECORDS 025 %TRANSLATING RECORDS 026 %TRANSLATING RECORDS 027 %TRANSLATING RECORDS 028 %TRANSLATING RECORDS 029 %TRANSLATING RECORDS 030 %TRANSLATING RECORDS 031 %TRANSLATING RECORDS 032 %TRANSLATING RECORDS 033 %TRANSLATING RECORDS 034 %TRANSLATING RECORDS 035 %TRANSLATING RECORDS 036 %TRANSLATING RECORDS 037 %TRANSLATING RECORDS 038 %TRANSLATING RECORDS 039 %TRANSLATING RECORDS 040 %TRANSLATING RECORDS 041 %TRANSLATING RECORDS 042 %TRANSLATING RECORDS 043 %TRANSLATING RECORDS 044 %TRANSLATING RECORDS 045 %TRANSLATING RECORDS 046 %TRANSLATING RECORDS 047 %TRANSLATING RECORDS 048 %TRANSLATING RECORDS 049 %TRANSLATING RECORDS 050 %TRANSLATING RECORDS 051 %TRANSLATING RECORDS 052 %TRANSLATING RECORDS 053 %TRANSLATING RECORDS 054 %TRANSLATING RECORDS 055 %TRANSLATING RECORDS 056 %TRANSLATING RECORDS 057 %TRANSLATING RECORDS 058 %TRANSLATING RECORDS 059 %TRANSLATING RECORDS 060 %TRANSLATING RECORDS 061 %TRANSLATING RECORDS 062 %TRANSLATING RECORDS 063 %TRANSLATING RECORDS 064 %TRANSLATING RECORDS 065 %TRANSLATING RECORDS 066 %TRANSLATING RECORDS 067 %TRANSLATING RECORDS 068 %TRANSLATING RECORDS 069 %TRANSLATING RECORDS 070 %TRANSLATING RECORDS 071 %TRANSLATING RECORDS 072 %TRANSLATING RECORDS 073 %TRANSLATING RECORDS 074 %TRANSLATING RECORDS 075 %TRANSLATING RECORDS 076 %TRANSLATING RECORDS 077 %TRANSLATING RECORDS 078 %TRANSLATING RECORDS 079 %TRANSLATING RECORDS 080 %TRANSLATING RECORDS 081 %TRANSLATING RECORDS 082 %TRANSLATING RECORDS 083 %TRANSLATING RECORDS 084 %TRANSLATING RECORDS 085 %TRANSLATING RECORDS 086 %TRANSLATING RECORDS 087 %TRANSLATING RECORDS 088 %TRANSLATING RECORDS 089 %TRANSLATING RECORDS 090 %TRANSLATING RECORDS 091 %TRANSLATING RECORDS 092 %TRANSLATING RECORDS 093 %TRANSLATING RECORDS 094 %TRANSLATING RECORDS 095 %TRANSLATING RECORDS 096 %TRANSLATING RECORDS 097 %TRANSLATING RECORDS 098 %TRANSLATING RECORDS 099 %TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES
   * Merging task    1 026 %   * Merging task    1 053 %   * Merging task    1 079 %   * Merging task    1 100 %   * Merging task    2 064 %   * Merging task    2 100 %   * Merging task    3 065 %   * Merging task    3 100 %   * Merging task    4 065 %   * Merging task    4 100 %   * Merging task    5 065 %   * Merging task    5 100 %   * Merging task    6 064 %   * Merging task    6 100 %   * Merging task    7 065 %   * Merging task    7 100 %   * Merging task    8 085 %   * Merging task    8 100 %   * Merging task    9 068 %   * Merging task    9 100 %   * Merging task   10 051 %   * Merging task   10 100 %   * Merging task   11 049 %   * Merging task   11 099 %   * Merging task   11 100 %   * Merging task   12 049 %   * Merging task   12 099 %   * Merging task   12 100 %   * Merging task   13 049 %   * Merging task   13 099 %   * Merging task   13 100 %   * Merging task   14 049 %   * Merging task   14 098 %   * Merging task   14 100 %   * Merging task   15 049 %   * Merging task   15 099 %   * Merging task   15 100 %   * Merging task   16 062 %   * Merging task   16 100 %   * Merging task   17 100 %   * Merging task   18 100 %   * Merging task   19 097 %   * Merging task   19 100 %   * Merging task   20 097 %   * Merging task   20 100 %   * Merging task   21 097 %   * Merging task   21 100 %   * Merging task   22 096 %   * Merging task   22 100 %   * Merging task   23 097 %   * Merging task   23 100 %   * Merging task   24 100 %   * Merging task   25 100 %   * Merging task   26 091 %   * Merging task   26 100 %   * Merging task   27 095 %   * Merging task   27 100 %   * Merging task   28 096 %   * Merging task   28 100 %   * Merging task   29 097 %   * Merging task   29 100 %   * Merging task   30 096 %   * Merging task   30 100 %   * Merging task   31 097 %   * Merging task   31 100 %   * Merging task   32 100 %   * Merging task   33 062 %   * Merging task   33 100 %   * Merging task   34 047 %   * Merging task   34 094 %   * Merging task   34 100 %   * Merging task   35 046 %   * Merging task   35 091 %   * Merging task   35 100 %   * Merging task   36 048 %   * Merging task   36 096 %   * Merging task   36 100 %   * Merging task   37 049 %   * Merging task   37 098 %   * Merging task   37 100 %   * Merging task   38 049 %   * Merging task   38 098 %   * Merging task   38 100 %   * Merging task   39 049 %   * Merging task   39 099 %   * Merging task   39 100 %   * Merging task   40 062 %   * Merging task   40 100 %   * Merging task   41 060 %   * Merging task   41 100 %   * Merging task   42 045 %   * Merging task   42 091 %   * Merging task   42 100 %   * Merging task   43 046 %   * Merging task   43 092 %   * Merging task   43 100 %   * Merging task   44 047 %   * Merging task   44 095 %   * Merging task   44 100 %   * Merging task   45 046 %   * Merging task   45 093 %   * Merging task   45 100 %   * Merging task   46 047 %   * Merging task   46 093 %   * Merging task   46 100 %   * Merging task   47 048 %   * Merging task   47 096 %   * Merging task   47 100 %   * Merging task   48 060 %   * Merging task   48 100 %   * Merging task   49 060 %   * Merging task   49 100 %   * Merging task   50 047 %   * Merging task   50 095 %   * Merging task   50 100 %   * Merging task   51 049 %   * Merging task   51 097 %   * Merging task   51 100 %   * Merging task   52 048 %   * Merging task   52 096 %   * Merging task   52 100 %   * Merging task   53 048 %   * Merging task   53 096 %   * Merging task   53 100 %   * Merging task   54 046 %   * Merging task   54 092 %   * Merging task   54 100 %   * Merging task   55 047 %   * Merging task   55 095 %   * Merging task   55 100 %   * Merging task   56 060 %   * Merging task   56 100 %   * Merging task   57 078 %   * Merging task   57 100 %   * Merging task   58 059 %   * Merging task   58 100 %   * Merging task   59 060 %   * Merging task   59 100 %   * Merging task   60 060 %   * Merging task   60 100 %   * Merging task   61 059 %   * Merging task   61 100 %   * Merging task   62 056 %   * Merging task   62 100 %   * Merging task   63 058 %   * Merging task   63 100 %   * Merging task   64 078 %   * Merging task   64 100 %   * All task merged!         
TRANSLATION FINISHED
GENERATING PCF
-> Input PCF and ouput PCF have the same name. Please use it in your simulations
COPYING ROW FILE
-> Input and ouput ROW files have the same name. Please use it in your simulations
#+end_example

Looks like it worked. Let's see the nature of the generated file.

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
ls -hl output-ondes3d-64p-draco_1-4.prv output-ondes3d-64p-draco_1-4.dim
#+end_src

#+RESULTS:
: /home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4
: -rw-r--r-- 1 schnorr schnorr 106M May 13 12:57 output-ondes3d-64p-draco_1-4.dim
: -rw-r--r-- 1 schnorr schnorr 112M May  6 17:50 output-ondes3d-64p-draco_1-4.prv

We need a =cfg= for Dimemas to replay.

Tiago told me there is a =small.cfg= in the =dimemas= directory, let's
create another one based on this.

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
cp ../dimemas/examples/small.cfg draco-example.cfg
#+end_src

#+RESULTS:
: /home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4

#+begin_src text :results output :session :exports both :tangle "./draco-example.cfg"
SDDFA
/*
 * "Dimemas Configuration Format:" "Version 2.5"
 * "Last update"  "09/18/02 at 12:51"
 */ ;;


#0:
"wide area network information" {
   // "wan_name" "name of the wide area network simulated"
   char "wan_name"[];
   // "number_of_machines" "number of machines in wan"
   int "number_of_machines";
   // "number_dedicated_connections" "number of dedicated connections between machines in the simulated system"
   int "number_dedicated_connections";
   //"function_of_traffic" "function that models influence of traffic in the non dedicated network"
        // "options: 1 EXP, 2 LOG, 3 LIN, 4 CT"
   int "function_of_traffic";
   // Maximal value of traffic in the network
   double "max_traffic_value";
   // "external_net_bandwidth" "external net bandwidth in MB/s"
   double "external_net_bandwidth";
   // "1 Constant, 2 Lineal, 3 Logarithmic"
   int "communication_group_model";
};;



#1:
"environment information" {
   char "machine_name"[];
   int "machine_id";
   // "instrumented_architecture" "Architecture used to instrument"
   char    "instrumented_architecture"[];
   // "number_of_nodes" "Number of nodes on virtual machine"
   int     "number_of_nodes";
   // "network_bandwidth" "Data tranfer rate between nodes in Mbytes/s"
   // "0 means instantaneous communication"
   double  "network_bandwidth";
   // "number_of_buses_on_network" "Maximun number of messages on network"
   // "0 means no limit"
   // "1 means bus contention"
   int   "number_of_buses_on_network";
   // "1 Constant, 2 Lineal, 3 Logarithmic"
   int   "communication_group_model";
};;


#2:
"node information" {
   int "machine_id";
   // "node_id" "Node number"
   int     "node_id";
   // "simulated_architecture" "Architecture node name"
   char    "simulated_architecture"[];
   // "number_of_processors" "Number of processors within node"
   int     "number_of_processors";
	// "speed_ratio_instrumented_vs_simulated" "Relative processor speed"
	double  "speed_ratio_instrumented_vs_simulated";
	// "intra_node_startup" "Startup time (s) of intra-node communications model"
	double  "intra_node_startup";
	// "intra_node_bandwidth" "Bandwidth (MB/s) of intra-node communications model"
	// "0 means instantaneous communication"
	double  "intra_node_bandwidth";
	// "intra_node_buses" "Number of buses of intra-node communications model"
	// "0 means infinite buses"
	int     "intra_node_buses";
	// "intra_node_input_links" "Input links of intra-node communications model"
	int     "intra_node_input_links";
	// "intra_node_input_links" "Output links of intra-node communications model"
	int     "intra_node_output_links";
	// "intra_node_startup" "Startup time (s) of inter-node communications model"
	double  "inter_node_startup";
	// "inter_node_input_links" "Input links of inter-node communications model"
	int     "inter_node_input_links";
	// "inter_node_output_links" "Input links of intra-node communications model"
	int     "inter_node_output_links";
	// "wan_startup" "Startup time (s) of inter-machines (WAN) communications model"
	double  "wan_startup";
};;


#3:
"mapping information" {
   // "tracefile" "Tracefile name of application"
   char    "tracefile"[];
   // "number_of_tasks" "Number of tasks in application"
   int     "number_of_tasks";
   // "mapping_tasks_to_nodes" "List of nodes in application"
   int     "mapping_tasks_to_nodes"[];
};;


#4:
"configuration files" {
   char       "scheduler"[];
   char       "file_system"[];
   char       "communication"[];
   char       "sensitivity"[];
};;


#5:
"modules information" {
// Module type
        int     "type";
        // Module value
        int     "value";
        // Speed ratio for this module, 0 means instantaneous execution
        double  "execution_ratio";
};;


#6:
"file system parameters" {
   double     "disk latency";
   double     "disk bandwidth";
   double     "block size";
   int        "concurrent requests";
   double     "hit ratio";
};;


#7:
"dedicated connection information" {
   // "connection_id" "connection number"
   int "connection_id";
   // "source_machine" "source machine number"
   int "source_machine";
   // "destination_machine" "destination machine number"
   int "destination_machine";
   // "connection_bandwidth" "bandwidth of the connection in Mbytes/s"
   double "connection_bandwidth";
   // "tags_list" "list of tags that will use the connection"
   int "tags_list"[];
   // "first_message_size" "size of messages in bytes" int "first_message_size";
   int "first_message_size";
   // "first_size_condition" "size condition that should meet messages to use the connection"
        // "it can be <, =, > and its is referent to message_size
   char "first_size_condition"[];
   // "operation" "& AND, | OR"
   char "operation"[];
    // "second_message_size" "size of messages in bytes"
   int "second_message_size";
   // "second_size_condition" "size condition that should meet messages to use the connection"
        // "it can be <, =, > and its is referent to message_size"
   char "second_size_condition"[];
   // "list_communicators" "list of communicators of coll. Operations that can use the connection"
   int "list_communicators"[];
   // Latency of dedicated connection in seconds
   double "connection_startup";
   //Latency due to distance in seconds
        double "flight_time";
};;


"wide area network information" {"", 1, 0, 4, 0.0, 0.0, 1};;

"environment information" {"draco", 0, "", 4, 1000.0, 0, 1};;

"node information" {0, 0, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 1, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 2, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 3, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;

"mapping information" {"output-ondes3d-64p-draco_1-4.dim", 64, [64] 
  {
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
  }};;

"configuration files" {"", "", "", ""};;

"file system parameters" {0.0, 0.0, 8.0, 0, 1.0};;
#+end_src

I need to tangle the code above with C-c C-v t

That will create the =draco-example.cfg= in the =./output-ondes3d-64p-draco_1-4/=.

Now let's replay with Dimemas.

We are using =-S 32K= according to Judit's suggestion.

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
~/install/dimemas-5.2.12/bin/Dimemas -S 32K -p output draco-example.cfg
#+end_src

#+RESULTS:

I have not output, emacs have showed in another buffer. I manually
copy below. Here's the error I got:

#+BEGIN_EXAMPLE
UNRECOVERABLE ERROR -> Node id 0 does not belong to any of the created machines
#+END_EXAMPLE

Looks like we badly configured Dimemas. So let's fix above.

I have tangled again: C-c C-v t.

Let's do it again:

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
~/install/dimemas-5.2.12/bin/Dimemas -S 32K -p output draco-example.cfg 2>&1 > output.log
cat output.log
rm output.log
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4
-> Simulator configuration to be read draco-example.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration
Fatal error at time 25210.000000000
Time out of simulation limit
#+end_example

Great, but we have reached the time simulation limit. Tiago told me we
cannot change that in any place (as far as he knows), the only
possible solution is to increase the power computing of each
machine. Let's edit the configuration above, tangle and try again.

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
~/install/dimemas-5.2.12/bin/Dimemas -S 32K -p output.prv draco-example.cfg 2>&1 > output.log
cat output.log
rm output.log
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4
-> Simulator configuration to be read draco-example.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration

780.000000000: END SIMULATION

WARNING: : COMMUNIC_end: Application finished with a pending global operation
WARNING: : * 63 tasks involved
 **** Application 0 (output-ondes3d-64p-draco_1-4.dim) ****

 **** Total Statistics ****

Execution time:	780.000000000
Speedup:	62.81 
CPU Time:	48990.241997581

 Id.	Computation	%time	Communication
  0	777.372615546	49.92	0.000000000
  1	777.372597177	49.92	0.000000000
  2	777.372597219	49.92	0.000000000
  3	777.372597403	49.92	0.000000000
  4	777.372597430	49.92	0.000000000
  5	777.372597549	49.92	0.000000000
  6	777.372597691	49.92	0.000000000
  7	777.372597805	49.92	0.000000000
  8	777.372597849	49.92	0.000000000
  9	777.372597980	49.92	0.000000000
 10	777.372598070	49.92	0.000000000
 11	777.372598191	49.92	0.000000000
 12	777.372598289	49.92	0.000000000
 13	777.372598419	49.92	0.000000000
 14	777.372598517	49.92	0.000000000
 15	777.372598632	49.92	0.000000000
 16	0.000000000	0.00	0.000000000
 17	778.023596091	49.94	0.000000000
 18	778.023596451	49.94	0.000000000
 19	778.023596804	49.94	0.000000000
 20	778.023597181	49.94	0.000000000
 21	778.023597564	49.94	0.000000000
 22	778.023597920	49.94	0.000000000
 23	778.023598272	49.94	0.000000000
 24	778.023598644	49.94	0.000000000
 25	778.023598977	49.94	0.000000000
 26	778.023599341	49.94	0.000000000
 27	778.023599702	49.94	0.000000000
 28	778.023600058	49.94	0.000000000
 29	778.023600603	49.94	0.000000000
 30	778.023601051	49.94	0.000000000
 31	778.023601563	49.94	0.000000000
 32	777.680196397	49.93	0.000000000
 33	777.680196770	49.93	0.000000000
 34	777.680197125	49.93	0.000000000
 35	777.680197467	49.93	0.000000000
 36	777.680197795	49.93	0.000000000
 37	777.680198138	49.93	0.000000000
 38	777.680198458	49.93	0.000000000
 39	777.680198781	49.93	0.000000000
 40	777.680199131	49.93	0.000000000
 41	777.680199513	49.93	0.000000000
 42	777.680199850	49.93	0.000000000
 43	777.680200225	49.93	0.000000000
 44	777.680200563	49.93	0.000000000
 45	777.680200901	49.93	0.000000000
 46	777.680201239	49.93	0.000000000
 47	777.680201592	49.93	0.000000000
 48	777.440200314	49.92	0.000000000
 49	777.440200694	49.92	0.000000000
 50	777.440201090	49.92	0.000000000
 51	777.440201446	49.92	0.000000000
 52	777.440201818	49.92	0.000000000
 53	777.440202213	49.92	0.000000000
 54	777.440202572	49.92	0.000000000
 55	777.440202938	49.92	0.000000000
 56	777.440203301	49.92	0.000000000
 57	777.440203648	49.92	0.000000000
 58	777.440204031	49.92	0.000000000
 59	777.440204383	49.92	0.000000000
 60	777.440204766	49.92	0.000000000
 61	777.440205122	49.92	0.000000000
 62	777.440205472	49.92	0.000000000
 63	777.440205841	49.92	0.000000000

 Id.	Mess.sent	Bytes sent	Immediate recv	Waiting recv	Bytes recv	Coll.op.	Block time	Comm. time	Wait link time	Wait buses time	I/O time
  0	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  1	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  2	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  3	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  4	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  5	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  6	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  7	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  8	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  9	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 10	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 11	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 12	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 13	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 14	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 15	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 16	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 17	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 18	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 19	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 20	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 21	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 22	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 23	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 24	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 25	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 26	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 27	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 28	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 29	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 30	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 31	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 32	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 33	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 34	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 35	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 36	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 37	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 38	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 39	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 40	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 41	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 42	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 43	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 44	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 45	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 46	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 47	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 48	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 49	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 50	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 51	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 52	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 53	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 54	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 55	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 56	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 57	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 58	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 59	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 60	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 61	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 62	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 63	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	6.300000e+01	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Average	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	9.843750e-01	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Maximum	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Minimum	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Stdev	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.240196e-01	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	

Output Paraver trace "output.prv" generated

WARNING: Some task P00 T01 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T02 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T03 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T04 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T05 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T06 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T07 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T08 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T09 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T10 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T11 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T12 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T13 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T14 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T15 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T00 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T48 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T49 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T50 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T51 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T52 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T53 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T54 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T55 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T56 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T57 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T58 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T59 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T60 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T61 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T62 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T63 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T32 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T33 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T34 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T35 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T36 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T37 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T38 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T39 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T40 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T41 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T42 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T43 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T44 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T45 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T46 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T47 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T17 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T18 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T19 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T20 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T21 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T22 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T23 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T24 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T25 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T26 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T27 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T28 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T29 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T30 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T31 (t00) waiting for global operation MPI_Barrier
#+end_example

There is a lot of warnings. We are not sure if we are okay.

Strange things we have noticed:

- Task 16 has no computation at all
  #+BEGIN_EXAMPLE
  16	0.000000000	0.00	0.000000000
  #+END_EXAMPLE
- =MPI_Barrier= at the end 

Lucas used this hostname file to run the experiment:

#+BEGIN_EXAMPLE
draco1 slots=16 max_slots=16
draco2 slots=16 max_slots=16
draco3 slots=16 max_slots=16
draco4 slots=16 max_slots=16
#+END_EXAMPLE

We should do a round-robin mapping.

#+begin_src text :results output :session :exports both :tangle "./draco-example-v2.cfg"
SDDFA
/*
 * "Dimemas Configuration Format:" "Version 2.5"
 * "Last update"  "09/18/02 at 12:51"
 */ ;;


#0:
"wide area network information" {
   // "wan_name" "name of the wide area network simulated"
   char "wan_name"[];
   // "number_of_machines" "number of machines in wan"
   int "number_of_machines";
   // "number_dedicated_connections" "number of dedicated connections between machines in the simulated system"
   int "number_dedicated_connections";
   //"function_of_traffic" "function that models influence of traffic in the non dedicated network"
        // "options: 1 EXP, 2 LOG, 3 LIN, 4 CT"
   int "function_of_traffic";
   // Maximal value of traffic in the network
   double "max_traffic_value";
   // "external_net_bandwidth" "external net bandwidth in MB/s"
   double "external_net_bandwidth";
   // "1 Constant, 2 Lineal, 3 Logarithmic"
   int "communication_group_model";
};;



#1:
"environment information" {
   char "machine_name"[];
   int "machine_id";
   // "instrumented_architecture" "Architecture used to instrument"
   char    "instrumented_architecture"[];
   // "number_of_nodes" "Number of nodes on virtual machine"
   int     "number_of_nodes";
   // "network_bandwidth" "Data tranfer rate between nodes in Mbytes/s"
   // "0 means instantaneous communication"
   double  "network_bandwidth";
   // "number_of_buses_on_network" "Maximun number of messages on network"
   // "0 means no limit"
   // "1 means bus contention"
   int   "number_of_buses_on_network";
   // "1 Constant, 2 Lineal, 3 Logarithmic"
   int   "communication_group_model";
};;


#2:
"node information" {
   int "machine_id";
   // "node_id" "Node number"
   int     "node_id";
   // "simulated_architecture" "Architecture node name"
   char    "simulated_architecture"[];
   // "number_of_processors" "Number of processors within node"
   int     "number_of_processors";
	// "speed_ratio_instrumented_vs_simulated" "Relative processor speed"
	double  "speed_ratio_instrumented_vs_simulated";
	// "intra_node_startup" "Startup time (s) of intra-node communications model"
	double  "intra_node_startup";
	// "intra_node_bandwidth" "Bandwidth (MB/s) of intra-node communications model"
	// "0 means instantaneous communication"
	double  "intra_node_bandwidth";
	// "intra_node_buses" "Number of buses of intra-node communications model"
	// "0 means infinite buses"
	int     "intra_node_buses";
	// "intra_node_input_links" "Input links of intra-node communications model"
	int     "intra_node_input_links";
	// "intra_node_input_links" "Output links of intra-node communications model"
	int     "intra_node_output_links";
	// "intra_node_startup" "Startup time (s) of inter-node communications model"
	double  "inter_node_startup";
	// "inter_node_input_links" "Input links of inter-node communications model"
	int     "inter_node_input_links";
	// "inter_node_output_links" "Input links of intra-node communications model"
	int     "inter_node_output_links";
	// "wan_startup" "Startup time (s) of inter-machines (WAN) communications model"
	double  "wan_startup";
};;


#3:
"mapping information" {
   // "tracefile" "Tracefile name of application"
   char    "tracefile"[];
   // "number_of_tasks" "Number of tasks in application"
   int     "number_of_tasks";
   // "mapping_tasks_to_nodes" "List of nodes in application"
   int     "mapping_tasks_to_nodes"[];
};;


#4:
"configuration files" {
   char       "scheduler"[];
   char       "file_system"[];
   char       "communication"[];
   char       "sensitivity"[];
};;


#5:
"modules information" {
// Module type
        int     "type";
        // Module value
        int     "value";
        // Speed ratio for this module, 0 means instantaneous execution
        double  "execution_ratio";
};;


#6:
"file system parameters" {
   double     "disk latency";
   double     "disk bandwidth";
   double     "block size";
   int        "concurrent requests";
   double     "hit ratio";
};;


#7:
"dedicated connection information" {
   // "connection_id" "connection number"
   int "connection_id";
   // "source_machine" "source machine number"
   int "source_machine";
   // "destination_machine" "destination machine number"
   int "destination_machine";
   // "connection_bandwidth" "bandwidth of the connection in Mbytes/s"
   double "connection_bandwidth";
   // "tags_list" "list of tags that will use the connection"
   int "tags_list"[];
   // "first_message_size" "size of messages in bytes" int "first_message_size";
   int "first_message_size";
   // "first_size_condition" "size condition that should meet messages to use the connection"
        // "it can be <, =, > and its is referent to message_size
   char "first_size_condition"[];
   // "operation" "& AND, | OR"
   char "operation"[];
    // "second_message_size" "size of messages in bytes"
   int "second_message_size";
   // "second_size_condition" "size condition that should meet messages to use the connection"
        // "it can be <, =, > and its is referent to message_size"
   char "second_size_condition"[];
   // "list_communicators" "list of communicators of coll. Operations that can use the connection"
   int "list_communicators"[];
   // Latency of dedicated connection in seconds
   double "connection_startup";
   //Latency due to distance in seconds
        double "flight_time";
};;


"wide area network information" {"", 1, 0, 4, 0.0, 0.0, 1};;

"environment information" {"draco", 0, "", 4, 1000.0, 0, 1};;

"node information" {0, 0, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 1, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 2, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 3, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;

"mapping information" {"output-ondes3d-64p-draco_1-4.dim", 64, [64] 
  {
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3,
    0,1,2,3
  }};;

"configuration files" {"", "", "", ""};;

"file system parameters" {0.0, 0.0, 8.0, 0, 1.0};;
#+end_src

Don't forget to tangle again with C-c C-v t

#+begin_src sh :results output :session :exports both :dir "~/svn/bozzetti/output-ondes3d-64p-draco_1-4/"
pwd
~/install/dimemas-5.2.12/bin/Dimemas -S 32K -p output.prv draco-example-v2.cfg 2>&1 > output.log
cat output.log
rm output.log
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr/svn/bozzetti/output-ondes3d-64p-draco_1-4
-> Simulator configuration to be read draco-example-v2.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration

780.000000000: END SIMULATION

WARNING: : COMMUNIC_end: Application finished with a pending global operation
WARNING: : * 63 tasks involved
 **** Application 0 (output-ondes3d-64p-draco_1-4.dim) ****

 **** Total Statistics ****

Execution time:	780.000000000
Speedup:	62.81 
CPU Time:	48990.241997581

 Id.	Computation	%time	Communication
  0	777.372615546	49.92	0.000000000
  1	777.372597177	49.92	0.000000000
  2	777.372597219	49.92	0.000000000
  3	777.372597403	49.92	0.000000000
  4	777.372597430	49.92	0.000000000
  5	777.372597549	49.92	0.000000000
  6	777.372597691	49.92	0.000000000
  7	777.372597805	49.92	0.000000000
  8	777.372597849	49.92	0.000000000
  9	777.372597980	49.92	0.000000000
 10	777.372598070	49.92	0.000000000
 11	777.372598191	49.92	0.000000000
 12	777.372598289	49.92	0.000000000
 13	777.372598419	49.92	0.000000000
 14	777.372598517	49.92	0.000000000
 15	777.372598632	49.92	0.000000000
 16	0.000000000	0.00	0.000000000
 17	778.023596091	49.94	0.000000000
 18	778.023596451	49.94	0.000000000
 19	778.023596804	49.94	0.000000000
 20	778.023597181	49.94	0.000000000
 21	778.023597564	49.94	0.000000000
 22	778.023597920	49.94	0.000000000
 23	778.023598272	49.94	0.000000000
 24	778.023598644	49.94	0.000000000
 25	778.023598977	49.94	0.000000000
 26	778.023599341	49.94	0.000000000
 27	778.023599702	49.94	0.000000000
 28	778.023600058	49.94	0.000000000
 29	778.023600603	49.94	0.000000000
 30	778.023601051	49.94	0.000000000
 31	778.023601563	49.94	0.000000000
 32	777.680196397	49.93	0.000000000
 33	777.680196770	49.93	0.000000000
 34	777.680197125	49.93	0.000000000
 35	777.680197467	49.93	0.000000000
 36	777.680197795	49.93	0.000000000
 37	777.680198138	49.93	0.000000000
 38	777.680198458	49.93	0.000000000
 39	777.680198781	49.93	0.000000000
 40	777.680199131	49.93	0.000000000
 41	777.680199513	49.93	0.000000000
 42	777.680199850	49.93	0.000000000
 43	777.680200225	49.93	0.000000000
 44	777.680200563	49.93	0.000000000
 45	777.680200901	49.93	0.000000000
 46	777.680201239	49.93	0.000000000
 47	777.680201592	49.93	0.000000000
 48	777.440200314	49.92	0.000000000
 49	777.440200694	49.92	0.000000000
 50	777.440201090	49.92	0.000000000
 51	777.440201446	49.92	0.000000000
 52	777.440201818	49.92	0.000000000
 53	777.440202213	49.92	0.000000000
 54	777.440202572	49.92	0.000000000
 55	777.440202938	49.92	0.000000000
 56	777.440203301	49.92	0.000000000
 57	777.440203648	49.92	0.000000000
 58	777.440204031	49.92	0.000000000
 59	777.440204383	49.92	0.000000000
 60	777.440204766	49.92	0.000000000
 61	777.440205122	49.92	0.000000000
 62	777.440205472	49.92	0.000000000
 63	777.440205841	49.92	0.000000000

 Id.	Mess.sent	Bytes sent	Immediate recv	Waiting recv	Bytes recv	Coll.op.	Block time	Comm. time	Wait link time	Wait buses time	I/O time
  0	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  1	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  2	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  3	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  4	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  5	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  6	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  7	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  8	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  9	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 10	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 11	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 12	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 13	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 14	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 15	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 16	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 17	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 18	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 19	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 20	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 21	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 22	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 23	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 24	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 25	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 26	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 27	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 28	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 29	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 30	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 31	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 32	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 33	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 34	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 35	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 36	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 37	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 38	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 39	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 40	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 41	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 42	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 43	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 44	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 45	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 46	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 47	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 48	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 49	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 50	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 51	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 52	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 53	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 54	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 55	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 56	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 57	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 58	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 59	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 60	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 61	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 62	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 63	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	6.300000e+01	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Average	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	9.843750e-01	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Maximum	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Minimum	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Stdev	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	1.240196e-01	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	

Output Paraver trace "output.prv" generated

WARNING: Some task P00 T01 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T02 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T03 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T04 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T05 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T06 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T07 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T08 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T09 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T10 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T11 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T12 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T13 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T14 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T15 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T00 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T48 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T49 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T50 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T51 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T52 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T53 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T54 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T55 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T56 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T57 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T58 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T59 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T60 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T61 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T62 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T63 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T32 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T33 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T34 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T35 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T36 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T37 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T38 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T39 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T40 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T41 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T42 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T43 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T44 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T45 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T46 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T47 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T17 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T18 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T19 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T20 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T21 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T22 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T23 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T24 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T25 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T26 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T27 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T28 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T29 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T30 (t00) waiting for global operation MPI_Barrier
WARNING: Some task P00 T31 (t00) waiting for global operation MPI_Barrier
#+end_example

Nothing has changed.

Our feeling is that the input trace file Lucas has obtained is somehow
corrupted, or at least missing information from task 16. We should
re-run the MPI application again, and do everything (=prv2dim= and
=Dimemas= simulation) once again.
* 2016-05-13 Executing Ondes3D in =draco= cluster with four nodes       :Lucas:

We have observed previously (see entry below) something strange with
traces I have collected in draco machine. I'll try to reproduce the
experiment in a more controlled way (using =:dir= in sh code source
blocks).

First, check the contents of the =Makefile= in =draco1=.

#+begin_src R :results output :session :exports both

#+end_src

#+begin_src sh :results output verbatim :dir /ssh:draco1:~/ondes3d-dir/Ondes3d-1.0/
cat Makefile
#+end_src

#+RESULTS:
#+begin_example
CC		= scorep --pomp --nocompiler --nocuda --noonline-access --nopdt --nouser  --noopencl mpicc
CC = mpicc

#List of Path to search sources files
VPATH		= .:../src
#Ansi conformity
#TESTFLAGS	+= -Xs

#######################################################
## DEBUG PARAMETERS
TESTFLAGS       += -DVERBOSE=0

# lecture
#TESTFLAGS	+= -DDEBUG_READ
# allocation  
#TESTFLAGS	+= -DDEBUG_ALLO
# no velocity computation
#TESTFLAGS      += -DNOVELOCITY
# no stress computation
#TESTFLAGS      += -DNOSTRESS
# no intermediates computation
#TESTFLAGS      += -DNOINTERMEDIATES
# no absorbing condition computation
#TESTFLAGS       += -DNOABS
# no anelasticity computation
#TESTFLAGS      += -DNOANELASTICITY
###################################################

####################################################
#EXECUTION FLAG
TESTFLAGS	+= -DMPI
MPI_FLAGS	=	
#TESTFLAGS	+= -DOMP
#OMP_FLAGS	= -fopenmp
# COMM=1 : persistant / COMM=2 : blocking
TESTFLAGS	+= -DCOMM=1
# with MPI topologie file (topologie.in)
#TESTFLAGS	+= -DTOPO_MPI
#####################################################





####################################################
#PROFILING FLAG

# TIMER=1 : standard / TIMER = 2 : timer vith MPI barriers
TESTFLAGS       += -DTIMER=2
#TAU detailed profiling
#TESTFLAGS	+= -DPROFILE1
#TAU global profiling
#TESTFLAGS       += -DPROFILE2
#FLOPS based on top of PAPI library
#TESTFLAGS       += -DFLOPS
#PAPI counters (cache misses) using PAPI library
#TESTFLAGS       += -DMISS

#######################################################
# OUTPUT
#Write VTK files
#TESTFLAGS	+= -DVTK
#Write geological model
#TESTFLAGS	+= -DOUT_HOGE

########################################################



#########################################################"
## OPTIMISATION PARAMETERS
OPTI 		+=  -O3
###########################################################


#MODEL parameters; default values are in options.h
CFLAGS		=   $(TESTFLAGS)  $(OPTI)  $(MODEL) 
PREFIX =ondes3d$(POST)
OBJS = main.o nrutil.o  computeVeloAndSource.o computeStress.o computeIntermediates.o alloAndInit.o IO.o alloAndInit_LayerModel.o
HEADERS=struct.h inlineFunctions.h options.h


all: $(PREFIX)
$(PREFIX): $(OBJS)
	$(CC)  $(MPI_FLAGS) $(OMP_FLAGS) -o $@ $^ -lm
%.o: %.c
	$(CC) $(MPI_FLAGS) $(OMP_FLAGS) $(CFLAGS) -c $<

clean:
	rm -f *.o  *~
#+end_example

So, MPI is available. Let's compile it again, and copy the compiled
binary to the $HOME directory of all draco machines that will be
involved in the experiment execution.

#+begin_src sh :results output verbatim :dir /ssh:draco1:~/ondes3d-dir/Ondes3d-1.0/
which mpicc
which mpirun
mpicc --version
mpirun --version
make clean
make
for i in 1 2 3 4; do
  echo "Copying ondes3d binary to draco${i}"
  scp -v ondes3d draco${i}:$HOME
  echo "Check if it is there"
  ssh draco${i} ls -lh $HOME/ondes3d
  ssh draco${i} "which mpirun"
  ssh draco${i} "mpirun --version"
done
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr/install/stow/bin/mpicc
/home/schnorr/install/stow/bin/mpirun
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.1) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

mpirun (Open MPI) 1.10.2

Report bugs to http://www.open-mpi.org/community/help/
rm -f *.o  *~
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c main.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c nrutil.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c computeVeloAndSource.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c computeStress.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c computeIntermediates.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c alloAndInit.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c IO.c
mpicc   -DVERBOSE=0 -DMPI -DCOMM=1 -DTIMER=2  -O3    -c alloAndInit_LayerModel.c
mpicc    -o ondes3d main.o nrutil.o computeVeloAndSource.o computeStress.o computeIntermediates.o alloAndInit.o IO.o alloAndInit_LayerModel.o -lm
Copying ondes3d binary to draco1
ondes3d                                         0%    0     0.0KB/s   --:-- ETAondes3d                                       100%  151KB 151.5KB/s   00:00    
Check if it is there
-rwxrwxr-x 1 schnorr schnorr 152K May 13 15:41 /home/schnorr/ondes3d
/home/schnorr/install/stow/bin/mpirun
mpirun (Open MPI) 1.10.2

Report bugs to http://www.open-mpi.org/community/help/
Copying ondes3d binary to draco2
ondes3d                                         0%    0     0.0KB/s   --:-- ETAondes3d                                       100%  151KB 151.5KB/s   00:00    
Check if it is there
-rwxrwxr-x 1 schnorr schnorr 152K May 13 15:41 /home/schnorr/ondes3d
/home/schnorr/install/stow/bin/mpirun
mpirun (Open MPI) 1.10.2

Report bugs to http://www.open-mpi.org/community/help/
Copying ondes3d binary to draco3
ondes3d                                         0%    0     0.0KB/s   --:-- ETAondes3d                                       100%  151KB 151.5KB/s   00:00    
Check if it is there
-rwxrwxr-x 1 schnorr schnorr 152K May 13 15:42 /home/schnorr/ondes3d
/home/schnorr/install/stow/bin/mpirun
mpirun (Open MPI) 1.10.2

Report bugs to http://www.open-mpi.org/community/help/
Copying ondes3d binary to draco4
ondes3d                                         0%    0     0.0KB/s   --:-- ETAondes3d                                       100%  151KB 151.5KB/s   00:00    
Check if it is there
-rwxrwxr-x 1 schnorr schnorr 152K May 13 15:33 /home/schnorr/ondes3d
/home/schnorr/install/stow/bin/mpirun
mpirun (Open MPI) 1.10.2

Report bugs to http://www.open-mpi.org/community/help/
#+end_example

Great, all set. I have previously installed =Open MPI 1.10.2= in all
machines, in my $HOME directory and stowed them in =stow/bin=.

Let's proceed with the execution, tracing it with Extrae.

The contents of the machine file are:

#+begin_src sh :results output verbatim :dir /ssh:draco1:~/ondes3d-dir/Ondes3d-1.0/
cat machinefile
#+end_src

#+RESULTS:
: draco1 slots=16 max_slots=16
: draco2 slots=16 max_slots=16
: draco3 slots=16 max_slots=16
: draco4 slots=16 max_slots=16

Execute without extrae tracing.

_Ondes3D Configuration_

Ondes3D is hard-coded to simulate the =NICE= Earthquake. Here's how:

#+begin_src sh :results output verbatim :dir /ssh:draco1:~/ondes3d-dir/Ondes3d-1.0/
grep PRMFILE *.h
#+end_src

I have two directory in my $HOME with the Ondes3D input:
- NICE-XML
- NICE-OUTPUT

Here's the parameters that drive how long it takes the execution:

#+begin_src sh :results output verbatim :dir /ssh:draco1:~/ondes3d-dir/Ondes3d-1.0/
cat $HOME/NICE-XML/nice.prm
#+end_src

I'm simulating only two timesteps to make it faster.

See the entry below to understand the parameters for the next call to =mpirun=:
- [[*2016-05-06 Compiling and instrumenting Ondes3D for Draco][2016-05-06 Compiling and instrumenting Ondes3D for Draco]]

#+begin_src sh :results output verbatim :dir /ssh:draco1:~/ondes3d-dir/Ondes3d-1.0/
  unset LD_PRELOAD
  unset EXTRAE_CONFIG_FILE
  unset EXTRAE_HOME
  pwd
  mpirun  -mca btl_tcp_if_include em1 --mca btl tcp,self \
          --machinefile machinefile \
          -np 64 \
          /home/schnorr/ondes3d
#+end_src

#+RESULTS:
: /home/schnorr/ondes3d-dir/Ondes3d-1.0

Great, it looks like it worked.

Extrae configuration is also described here:
- [[*2016-05-06 Compiling and instrumenting Ondes3D for Draco][2016-05-06 Compiling and instrumenting Ondes3D for Draco]]

Now let's proceed with the tracing:

#+BEGIN_EXAMPLE
export EXTRAE_CONFIG_FILE=/home/schnorr/extrae.xml
export EXTRAE_HOME=/home/schnorr/install/extrae-3.3.0/
export LD_PRELOAD=${EXTRAE_HOME}/lib/libmpitrace.so
mpirun  -mca btl_tcp_if_include em1 --mca btl tcp,self \
        --machinefile machinefile \
        -np 64 \
        /home/schnorr/ondes3d
#+END_EXAMPLE

Will have to wait =draco[1-4]= be free again, someone has locked the machine.
* 2016-05-14 Heat Equation in MPI in =draco=                            :Lucas:

I was uncapable of running Ondes3D in =draco=.

So, I found out this site:
- http://www.cas.usf.edu/~cconnor/parallel/2dheat/2dheat.html

They propose a code in MPI (blocking operations) that calculate a 2D
heat transfer. The code seems to run with a few modifications. I have
modified the size of the 2D plate so we have an acceptable size for
the =draco= experiment with 128 cores (15k x 15k, 500 timesteps). I'll
show below the steps in order to reproduce the experiment.

#+begin_src sh :results output verbatim :dir /ssh:draco1.inf:~/
DIR=2016-05-14-2D_Heat_Transfer
mkdir -p $DIR; cd $DIR
rm -rf github 2d_heat_equation #clean-up
git clone https://github.com/tcbozzetti/trabalhoconclusao.git github
mpicc ./github/applications/2d_heat_transfer/2d_heat_equation.c -o 2d_heat_equation
#copy the binary to all nodes
for i in `seq 1 8`; do
  rsync 2d_heat_equation draco${i}:./
  ssh draco${i} ls -lh 2d_heat_equation
done
#+end_src

#+RESULTS:
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:26 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:26 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:27 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:18 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:28 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:30 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:22 2d_heat_equation
: -rwxrwxr-x 1 schnorr schnorr 14K May 14 01:18 2d_heat_equation

The machine file with all resources:

#+begin_src sh :results output verbatim :dir /ssh:draco1.inf:~/
cat machinefile
#+end_src

#+RESULTS:
: draco1 slots=16 max_slots=32
: draco2 slots=16 max_slots=32
: draco3 slots=16 max_slots=32
: draco4 slots=16 max_slots=32
: draco5 slots=16 max_slots=32
: draco6 slots=16 max_slots=32
: draco7 slots=16 max_slots=32
: draco8 slots=16 max_slots=32

Great, now let's execute with =--np 128=:

#+begin_src sh :results output verbatim :dir /ssh:draco1.inf:~/
unset LD_PRELOAD
unset EXTRAE_CONFIG_FILE
unset EXTRAE_HOME
pwd
mpirun  -mca btl_tcp_if_include em1 --mca btl tcp,self \
        --machinefile /home/schnorr/machinefile \
        -np 128 \
        /home/schnorr/2d_heat_equation
#+end_src

#+RESULTS:
#+begin_example
/home/schnorr
Grid size: X= 15000 Y= 15000 Time steps= 500
Initializing grid and writing initial.dat file...
Sent to= 1 offset= 0 number_rows= 119 neighbor1= 0 neighbor2=2
worker number = 1 offset= 0 number_rows= 119 start = 1 end =118
Sent to= 2 offset= 119 number_rows= 119 neighbor1= 1 neighbor2=3
worker number = 2 offset= 119 number_rows= 119 start = 119 end =237
Sent to= 3 offset= 238 number_rows= 119 neighbor1= 2 neighbor2=4
worker number = 3 offset= 238 number_rows= 119 start = 238 end =356
Sent to= 4 offset= 357 number_rows= 119 neighbor1= 3 neighbor2=5
worker number = 4 offset= 357 number_rows= 119 start = 357 end =475
Sent to= 5 offset= 476 number_rows= 119 neighbor1= 4 neighbor2=6
worker number = 5 offset= 476 number_rows= 119 start = 476 end =594
Sent to= 6 offset= 595 number_rows= 119 neighbor1= 5 neighbor2=7
worker number = 6 offset= 595 number_rows= 119 start = 595 end =713
Sent to= 7 offset= 714 number_rows= 119 neighbor1= 6 neighbor2=8
worker number = 7 offset= 714 number_rows= 119 start = 714 end =832
Sent to= 8 offset= 833 number_rows= 119 neighbor1= 7 neighbor2=9
worker number = 8 offset= 833 number_rows= 119 start = 833 end =951
Sent to= 9 offset= 952 number_rows= 119 neighbor1= 8 neighbor2=10
worker number = 9 offset= 952 number_rows= 119 start = 952 end =1070
Sent to= 10 offset= 1071 number_rows= 119 neighbor1= 9 neighbor2=11
worker number = 10 offset= 1071 number_rows= 119 start = 1071 end =1189
Sent to= 11 offset= 1190 number_rows= 119 neighbor1= 10 neighbor2=12
worker number = 11 offset= 1190 number_rows= 119 start = 1190 end =1308
Sent to= 12 offset= 1309 number_rows= 119 neighbor1= 11 neighbor2=13
worker number = 12 offset= 1309 number_rows= 119 start = 1309 end =1427
Sent to= 13 offset= 1428 number_rows= 119 neighbor1= 12 neighbor2=14
worker number = 13 offset= 1428 number_rows= 119 start = 1428 end =1546
Sent to= 14 offset= 1547 number_rows= 119 neighbor1= 13 neighbor2=15
worker number = 14 offset= 1547 number_rows= 119 start = 1547 end =1665
Sent to= 15 offset= 1666 number_rows= 118 neighbor1= 14 neighbor2=16
worker number = 15 offset= 1666 number_rows= 118 start = 1666 end =1783
Sent to= 16 offset= 1784 number_rows= 118 neighbor1= 15 neighbor2=17
worker number = 16 offset= 1784 number_rows= 118 start = 1784 end =1901
Sent to= 17 offset= 1902 number_rows= 118 neighbor1= 16 neighbor2=18
worker number = 17 offset= 1902 number_rows= 118 start = 1902 end =2019
Sent to= 18 offset= 2020 number_rows= 118 neighbor1= 17 neighbor2=19
worker number = 18 offset= 2020 number_rows= 118 start = 2020 end =2137
Sent to= 19 offset= 2138 number_rows= 118 neighbor1= 18 neighbor2=20
worker number = 19 offset= 2138 number_rows= 118 start = 2138 end =2255
Sent to= 20 offset= 2256 number_rows= 118 neighbor1= 19 neighbor2=21
worker number = 20 offset= 2256 number_rows= 118 start = 2256 end =2373
Sent to= 21 offset= 2374 number_rows= 118 neighbor1= 20 neighbor2=22
worker number = 21 offset= 2374 number_rows= 118 start = 2374 end =2491
Sent to= 22 offset= 2492 number_rows= 118 neighbor1= 21 neighbor2=23
worker number = 22 offset= 2492 number_rows= 118 start = 2492 end =2609
Sent to= 23 offset= 2610 number_rows= 118 neighbor1= 22 neighbor2=24
worker number = 23 offset= 2610 number_rows= 118 start = 2610 end =2727
Sent to= 24 offset= 2728 number_rows= 118 neighbor1= 23 neighbor2=25
worker number = 24 offset= 2728 number_rows= 118 start = 2728 end =2845
Sent to= 25 offset= 2846 number_rows= 118 neighbor1= 24 neighbor2=26
worker number = 25 offset= 2846 number_rows= 118 start = 2846 end =2963
Sent to= 26 offset= 2964 number_rows= 118 neighbor1= 25 neighbor2=27
worker number = 26 offset= 2964 number_rows= 118 start = 2964 end =3081
Sent to= 27 offset= 3082 number_rows= 118 neighbor1= 26 neighbor2=28
worker number = 27 offset= 3082 number_rows= 118 start = 3082 end =3199
Sent to= 28 offset= 3200 number_rows= 118 neighbor1= 27 neighbor2=29
worker number = 28 offset= 3200 number_rows= 118 start = 3200 end =3317
Sent to= 29 offset= 3318 number_rows= 118 neighbor1= 28 neighbor2=30
worker number = 29 offset= 3318 number_rows= 118 start = 3318 end =3435
Sent to= 30 offset= 3436 number_rows= 118 neighbor1= 29 neighbor2=31
worker number = 30 offset= 3436 number_rows= 118 start = 3436 end =3553
Sent to= 31 offset= 3554 number_rows= 118 neighbor1= 30 neighbor2=32
worker number = 31 offset= 3554 number_rows= 118 start = 3554 end =3671
Sent to= 32 offset= 3672 number_rows= 118 neighbor1= 31 neighbor2=33
worker number = 32 offset= 3672 number_rows= 118 start = 3672 end =3789
Sent to= 33 offset= 3790 number_rows= 118 neighbor1= 32 neighbor2=34
worker number = 33 offset= 3790 number_rows= 118 start = 3790 end =3907
Sent to= 34 offset= 3908 number_rows= 118 neighbor1= 33 neighbor2=35
worker number = 34 offset= 3908 number_rows= 118 start = 3908 end =4025
Sent to= 35 offset= 4026 number_rows= 118 neighbor1= 34 neighbor2=36
worker number = 35 offset= 4026 number_rows= 118 start = 4026 end =4143
Sent to= 36 offset= 4144 number_rows= 118 neighbor1= 35 neighbor2=37
worker number = 36 offset= 4144 number_rows= 118 start = 4144 end =4261
Sent to= 37 offset= 4262 number_rows= 118 neighbor1= 36 neighbor2=38
worker number = 37 offset= 4262 number_rows= 118 start = 4262 end =4379
Sent to= 38 offset= 4380 number_rows= 118 neighbor1= 37 neighbor2=39
worker number = 38 offset= 4380 number_rows= 118 start = 4380 end =4497
Sent to= 39 offset= 4498 number_rows= 118 neighbor1= 38 neighbor2=40
worker number = 39 offset= 4498 number_rows= 118 start = 4498 end =4615
Sent to= 40 offset= 4616 number_rows= 118 neighbor1= 39 neighbor2=41
worker number = 40 offset= 4616 number_rows= 118 start = 4616 end =4733
Sent to= 41 offset= 4734 number_rows= 118 neighbor1= 40 neighbor2=42
worker number = 41 offset= 4734 number_rows= 118 start = 4734 end =4851
Sent to= 42 offset= 4852 number_rows= 118 neighbor1= 41 neighbor2=43
worker number = 42 offset= 4852 number_rows= 118 start = 4852 end =4969
Sent to= 43 offset= 4970 number_rows= 118 neighbor1= 42 neighbor2=44
worker number = 43 offset= 4970 number_rows= 118 start = 4970 end =5087
Sent to= 44 offset= 5088 number_rows= 118 neighbor1= 43 neighbor2=45
worker number = 44 offset= 5088 number_rows= 118 start = 5088 end =5205
Sent to= 45 offset= 5206 number_rows= 118 neighbor1= 44 neighbor2=46
worker number = 45 offset= 5206 number_rows= 118 start = 5206 end =5323
Sent to= 46 offset= 5324 number_rows= 118 neighbor1= 45 neighbor2=47
worker number = 46 offset= 5324 number_rows= 118 start = 5324 end =5441
Sent to= 47 offset= 5442 number_rows= 118 neighbor1= 46 neighbor2=48
worker number = 47 offset= 5442 number_rows= 118 start = 5442 end =5559
Sent to= 48 offset= 5560 number_rows= 118 neighbor1= 47 neighbor2=49
worker number = 48 offset= 5560 number_rows= 118 start = 5560 end =5677
Sent to= 49 offset= 5678 number_rows= 118 neighbor1= 48 neighbor2=50
worker number = 49 offset= 5678 number_rows= 118 start = 5678 end =5795
Sent to= 50 offset= 5796 number_rows= 118 neighbor1= 49 neighbor2=51
worker number = 50 offset= 5796 number_rows= 118 start = 5796 end =5913
Sent to= 51 offset= 5914 number_rows= 118 neighbor1= 50 neighbor2=52
worker number = 51 offset= 5914 number_rows= 118 start = 5914 end =6031
Sent to= 52 offset= 6032 number_rows= 118 neighbor1= 51 neighbor2=53
worker number = 52 offset= 6032 number_rows= 118 start = 6032 end =6149
Sent to= 53 offset= 6150 number_rows= 118 neighbor1= 52 neighbor2=54
worker number = 53 offset= 6150 number_rows= 118 start = 6150 end =6267
Sent to= 54 offset= 6268 number_rows= 118 neighbor1= 53 neighbor2=55
worker number = 54 offset= 6268 number_rows= 118 start = 6268 end =6385
Sent to= 55 offset= 6386 number_rows= 118 neighbor1= 54 neighbor2=56
worker number = 55 offset= 6386 number_rows= 118 start = 6386 end =6503
Sent to= 56 offset= 6504 number_rows= 118 neighbor1= 55 neighbor2=57
worker number = 56 offset= 6504 number_rows= 118 start = 6504 end =6621
Sent to= 57 offset= 6622 number_rows= 118 neighbor1= 56 neighbor2=58
worker number = 57 offset= 6622 number_rows= 118 start = 6622 end =6739
Sent to= 58 offset= 6740 number_rows= 118 neighbor1= 57 neighbor2=59
worker number = 58 offset= 6740 number_rows= 118 start = 6740 end =6857
Sent to= 59 offset= 6858 number_rows= 118 neighbor1= 58 neighbor2=60
worker number = 59 offset= 6858 number_rows= 118 start = 6858 end =6975
Sent to= 60 offset= 6976 number_rows= 118 neighbor1= 59 neighbor2=61
worker number = 60 offset= 6976 number_rows= 118 start = 6976 end =7093
Sent to= 61 offset= 7094 number_rows= 118 neighbor1= 60 neighbor2=62
worker number = 61 offset= 7094 number_rows= 118 start = 7094 end =7211
Sent to= 62 offset= 7212 number_rows= 118 neighbor1= 61 neighbor2=63
worker number = 62 offset= 7212 number_rows= 118 start = 7212 end =7329
Sent to= 63 offset= 7330 number_rows= 118 neighbor1= 62 neighbor2=64
worker number = 63 offset= 7330 number_rows= 118 start = 7330 end =7447
Sent to= 64 offset= 7448 number_rows= 118 neighbor1= 63 neighbor2=65
worker number = 64 offset= 7448 number_rows= 118 start = 7448 end =7565
Sent to= 65 offset= 7566 number_rows= 118 neighbor1= 64 neighbor2=66
worker number = 65 offset= 7566 number_rows= 118 start = 7566 end =7683
Sent to= 66 offset= 7684 number_rows= 118 neighbor1= 65 neighbor2=67
worker number = 66 offset= 7684 number_rows= 118 start = 7684 end =7801
Sent to= 67 offset= 7802 number_rows= 118 neighbor1= 66 neighbor2=68
worker number = 67 offset= 7802 number_rows= 118 start = 7802 end =7919
Sent to= 68 offset= 7920 number_rows= 118 neighbor1= 67 neighbor2=69
worker number = 68 offset= 7920 number_rows= 118 start = 7920 end =8037
Sent to= 69 offset= 8038 number_rows= 118 neighbor1= 68 neighbor2=70
worker number = 69 offset= 8038 number_rows= 118 start = 8038 end =8155
Sent to= 70 offset= 8156 number_rows= 118 neighbor1= 69 neighbor2=71
worker number = 70 offset= 8156 number_rows= 118 start = 8156 end =8273
Sent to= 71 offset= 8274 number_rows= 118 neighbor1= 70 neighbor2=72
worker number = 71 offset= 8274 number_rows= 118 start = 8274 end =8391
Sent to= 72 offset= 8392 number_rows= 118 neighbor1= 71 neighbor2=73
worker number = 72 offset= 8392 number_rows= 118 start = 8392 end =8509
Sent to= 73 offset= 8510 number_rows= 118 neighbor1= 72 neighbor2=74
worker number = 73 offset= 8510 number_rows= 118 start = 8510 end =8627
Sent to= 74 offset= 8628 number_rows= 118 neighbor1= 73 neighbor2=75
worker number = 74 offset= 8628 number_rows= 118 start = 8628 end =8745
Sent to= 75 offset= 8746 number_rows= 118 neighbor1= 74 neighbor2=76
worker number = 75 offset= 8746 number_rows= 118 start = 8746 end =8863
Sent to= 76 offset= 8864 number_rows= 118 neighbor1= 75 neighbor2=77
worker number = 76 offset= 8864 number_rows= 118 start = 8864 end =8981
Sent to= 77 offset= 8982 number_rows= 118 neighbor1= 76 neighbor2=78
worker number = 77 offset= 8982 number_rows= 118 start = 8982 end =9099
Sent to= 78 offset= 9100 number_rows= 118 neighbor1= 77 neighbor2=79
worker number = 78 offset= 9100 number_rows= 118 start = 9100 end =9217
Sent to= 79 offset= 9218 number_rows= 118 neighbor1= 78 neighbor2=80
worker number = 79 offset= 9218 number_rows= 118 start = 9218 end =9335
Sent to= 80 offset= 9336 number_rows= 118 neighbor1= 79 neighbor2=81
worker number = 80 offset= 9336 number_rows= 118 start = 9336 end =9453
Sent to= 81 offset= 9454 number_rows= 118 neighbor1= 80 neighbor2=82
worker number = 81 offset= 9454 number_rows= 118 start = 9454 end =9571
Sent to= 82 offset= 9572 number_rows= 118 neighbor1= 81 neighbor2=83
worker number = 82 offset= 9572 number_rows= 118 start = 9572 end =9689
Sent to= 83 offset= 9690 number_rows= 118 neighbor1= 82 neighbor2=84
worker number = 83 offset= 9690 number_rows= 118 start = 9690 end =9807
Sent to= 84 offset= 9808 number_rows= 118 neighbor1= 83 neighbor2=85
worker number = 84 offset= 9808 number_rows= 118 start = 9808 end =9925
Sent to= 85 offset= 9926 number_rows= 118 neighbor1= 84 neighbor2=86
worker number = 85 offset= 9926 number_rows= 118 start = 9926 end =10043
Sent to= 86 offset= 10044 number_rows= 118 neighbor1= 85 neighbor2=87
worker number = 86 offset= 10044 number_rows= 118 start = 10044 end =10161
Sent to= 87 offset= 10162 number_rows= 118 neighbor1= 86 neighbor2=88
worker number = 87 offset= 10162 number_rows= 118 start = 10162 end =10279
Sent to= 88 offset= 10280 number_rows= 118 neighbor1= 87 neighbor2=89
worker number = 88 offset= 10280 number_rows= 118 start = 10280 end =10397
Sent to= 89 offset= 10398 number_rows= 118 neighbor1= 88 neighbor2=90
worker number = 89 offset= 10398 number_rows= 118 start = 10398 end =10515
Sent to= 90 offset= 10516 number_rows= 118 neighbor1= 89 neighbor2=91
worker number = 90 offset= 10516 number_rows= 118 start = 10516 end =10633
Sent to= 91 offset= 10634 number_rows= 118 neighbor1= 90 neighbor2=92
worker number = 91 offset= 10634 number_rows= 118 start = 10634 end =10751
Sent to= 92 offset= 10752 number_rows= 118 neighbor1= 91 neighbor2=93
worker number = 92 offset= 10752 number_rows= 118 start = 10752 end =10869
Sent to= 93 offset= 10870 number_rows= 118 neighbor1= 92 neighbor2=94
worker number = 93 offset= 10870 number_rows= 118 start = 10870 end =10987
Sent to= 94 offset= 10988 number_rows= 118 neighbor1= 93 neighbor2=95
worker number = 94 offset= 10988 number_rows= 118 start = 10988 end =11105
Sent to= 95 offset= 11106 number_rows= 118 neighbor1= 94 neighbor2=96
worker number = 95 offset= 11106 number_rows= 118 start = 11106 end =11223
Sent to= 96 offset= 11224 number_rows= 118 neighbor1= 95 neighbor2=97
worker number = 96 offset= 11224 number_rows= 118 start = 11224 end =11341
Sent to= 97 offset= 11342 number_rows= 118 neighbor1= 96 neighbor2=98
worker number = 97 offset= 11342 number_rows= 118 start = 11342 end =11459
Sent to= 98 offset= 11460 number_rows= 118 neighbor1= 97 neighbor2=99
worker number = 98 offset= 11460 number_rows= 118 start = 11460 end =11577
Sent to= 99 offset= 11578 number_rows= 118 neighbor1= 98 neighbor2=100
worker number = 99 offset= 11578 number_rows= 118 start = 11578 end =11695
Sent to= 100 offset= 11696 number_rows= 118 neighbor1= 99 neighbor2=101
worker number = 100 offset= 11696 number_rows= 118 start = 11696 end =11813
Sent to= 101 offset= 11814 number_rows= 118 neighbor1= 100 neighbor2=102
worker number = 101 offset= 11814 number_rows= 118 start = 11814 end =11931
Sent to= 102 offset= 11932 number_rows= 118 neighbor1= 101 neighbor2=103
worker number = 102 offset= 11932 number_rows= 118 start = 11932 end =12049
Sent to= 103 offset= 12050 number_rows= 118 neighbor1= 102 neighbor2=104
worker number = 103 offset= 12050 number_rows= 118 start = 12050 end =12167
Sent to= 104 offset= 12168 number_rows= 118 neighbor1= 103 neighbor2=105
worker number = 104 offset= 12168 number_rows= 118 start = 12168 end =12285
Sent to= 105 offset= 12286 number_rows= 118 neighbor1= 104 neighbor2=106
worker number = 105 offset= 12286 number_rows= 118 start = 12286 end =12403
Sent to= 106 offset= 12404 number_rows= 118 neighbor1= 105 neighbor2=107
worker number = 106 offset= 12404 number_rows= 118 start = 12404 end =12521
Sent to= 107 offset= 12522 number_rows= 118 neighbor1= 106 neighbor2=108
worker number = 107 offset= 12522 number_rows= 118 start = 12522 end =12639
Sent to= 108 offset= 12640 number_rows= 118 neighbor1= 107 neighbor2=109
worker number = 108 offset= 12640 number_rows= 118 start = 12640 end =12757
Sent to= 109 offset= 12758 number_rows= 118 neighbor1= 108 neighbor2=110
worker number = 109 offset= 12758 number_rows= 118 start = 12758 end =12875
Sent to= 110 offset= 12876 number_rows= 118 neighbor1= 109 neighbor2=111
worker number = 110 offset= 12876 number_rows= 118 start = 12876 end =12993
Sent to= 111 offset= 12994 number_rows= 118 neighbor1= 110 neighbor2=112
worker number = 111 offset= 12994 number_rows= 118 start = 12994 end =13111
Sent to= 112 offset= 13112 number_rows= 118 neighbor1= 111 neighbor2=113
worker number = 112 offset= 13112 number_rows= 118 start = 13112 end =13229
Sent to= 113 offset= 13230 number_rows= 118 neighbor1= 112 neighbor2=114
worker number = 113 offset= 13230 number_rows= 118 start = 13230 end =13347
Sent to= 114 offset= 13348 number_rows= 118 neighbor1= 113 neighbor2=115
worker number = 114 offset= 13348 number_rows= 118 start = 13348 end =13465
Sent to= 115 offset= 13466 number_rows= 118 neighbor1= 114 neighbor2=116
worker number = 115 offset= 13466 number_rows= 118 start = 13466 end =13583
Sent to= 116 offset= 13584 number_rows= 118 neighbor1= 115 neighbor2=117
worker number = 116 offset= 13584 number_rows= 118 start = 13584 end =13701
Sent to= 117 offset= 13702 number_rows= 118 neighbor1= 116 neighbor2=118
worker number = 117 offset= 13702 number_rows= 118 start = 13702 end =13819
Sent to= 118 offset= 13820 number_rows= 118 neighbor1= 117 neighbor2=119
worker number = 118 offset= 13820 number_rows= 118 start = 13820 end =13937
Sent to= 119 offset= 13938 number_rows= 118 neighbor1= 118 neighbor2=120
worker number = 119 offset= 13938 number_rows= 118 start = 13938 end =14055
Sent to= 120 offset= 14056 number_rows= 118 neighbor1= 119 neighbor2=121
worker number = 120 offset= 14056 number_rows= 118 start = 14056 end =14173
Sent to= 121 offset= 14174 number_rows= 118 neighbor1= 120 neighbor2=122
worker number = 121 offset= 14174 number_rows= 118 start = 14174 end =14291
Sent to= 122 offset= 14292 number_rows= 118 neighbor1= 121 neighbor2=123
worker number = 122 offset= 14292 number_rows= 118 start = 14292 end =14409
Sent to= 123 offset= 14410 number_rows= 118 neighbor1= 122 neighbor2=124
worker number = 123 offset= 14410 number_rows= 118 start = 14410 end =14527
Sent to= 124 offset= 14528 number_rows= 118 neighbor1= 123 neighbor2=125
worker number = 124 offset= 14528 number_rows= 118 start = 14528 end =14645
Sent to= 125 offset= 14646 number_rows= 118 neighbor1= 124 neighbor2=126
worker number = 125 offset= 14646 number_rows= 118 start = 14646 end =14763
Sent to= 126 offset= 14764 number_rows= 118 neighbor1= 125 neighbor2=127
worker number = 126 offset= 14764 number_rows= 118 start = 14764 end =14881
Sent to= 127 offset= 14882 number_rows= 118 neighbor1= 126 neighbor2=0
worker number = 127 offset= 14882 number_rows= 118 start = 14882 end =14998
#+end_example

Now trace it.

#+begin_src sh :results output verbatim :dir /ssh:draco1.inf:~/
DIR=2016-05-14-2D_Heat_Transfer
for i in `seq 1 8`; do
  ssh draco${i} rm -rf $HOME/set-0/
done
mpirun \
        -x EXTRAE_CONFIG_FILE=/home/schnorr/extrae.xml \
        -x EXTRAE_HOME=/home/schnorr/install/extrae-3.3.0/ \
        -x LD_PRELOAD=${EXTRAE_HOME}/lib/libmpitrace.so \
        -mca btl_tcp_if_include em1 --mca btl tcp,self \
        --machinefile /home/schnorr/machinefile \
        -np 128 \
        /home/schnorr/2d_heat_equation
for i in `seq 1 8`; do
  ssh draco${i} "ls -l set-0/; ls -l set-0/ | wc -l"
done
#+end_src

#+RESULTS:
#+begin_example
Welcome to Extrae 3.3.0 (revision 3966 based on extrae/trunk)
Extrae: Parsing the configuration file (/home/schnorr/extrae.xml) begins
Extrae: Tracing package is located on /home/schnorr/install/extrae-3.3.0
Extrae: Generating intermediate files for Paraver traces.
Extrae: <counters> tag at <MPI> level will be ignored. This library does not support CPU HW.
Extrae: Tracing 3 level(s) of MPI callers: [ 1 2 3 ]
Extrae: <dynamic-memory> tag at <Callers> level will be ignored. This library does not support dynamic memory instrumentation.
Extrae: Tracing buffer can hold 500000 events
Extrae: Circular buffer disabled.
Extrae: Sampling buffers will be written at instrumentation points
Extrae: Warning! <dynamic-memory> tag will be ignored. This library does support instrumenting dynamic memory calls.
Extrae: Warning! <input-output> tag will be ignored. This library does support instrumenting I/O calls.
Extrae: Dynamic memory instrumentation is disabled.
Extrae: Basic I/O memory instrumentation is disabled.
Extrae: Parsing the configuration file (/home/schnorr/extrae.xml) has ended
Extrae: Intermediate traces will be stored in /home/schnorr
Extrae: Temporal directory (/home/schnorr) is private among processes.
Extrae: Final directory (/home/schnorr) is private among processes.
Extrae: Tracing mode is set to: Detail.
Extrae: Successfully initiated with 128 tasks and 1 threads

Grid size: X= 5000 Y= 5000 Time steps= 500
Initializing grid and writing initial.dat file...
Sent to= 1 offset= 0 number_rows= 40 neighbor1= 0 neighbor2=2
worker number = 1 offset= 0 number_rows= 40 start = 1 end =39
Sent to= 2 offset= 40 number_rows= 40 neighbor1= 1 neighbor2=3
worker number = 2 offset= 40 number_rows= 40 start = 40 end =79
Sent to= 3 offset= 80 number_rows= 40 neighbor1= 2 neighbor2=4
worker number = 3 offset= 80 number_rows= 40 start = 80 end =119
Sent to= 4 offset= 120 number_rows= 40 neighbor1= 3 neighbor2=5
worker number = 4 offset= 120 number_rows= 40 start = 120 end =159
Sent to= 5 offset= 160 number_rows= 40 neighbor1= 4 neighbor2=6
worker number = 5 offset= 160 number_rows= 40 start = 160 end =199
Sent to= 6 offset= 200 number_rows= 40 neighbor1= 5 neighbor2=7
worker number = 6 offset= 200 number_rows= 40 start = 200 end =239
Sent to= 7 offset= 240 number_rows= 40 neighbor1= 6 neighbor2=8
worker number = 7 offset= 240 number_rows= 40 start = 240 end =279
Sent to= 8 offset= 280 number_rows= 40 neighbor1= 7 neighbor2=9
worker number = 8 offset= 280 number_rows= 40 start = 280 end =319
Sent to= 9 offset= 320 number_rows= 40 neighbor1= 8 neighbor2=10
worker number = 9 offset= 320 number_rows= 40 start = 320 end =359
Sent to= 10 offset= 360 number_rows= 40 neighbor1= 9 neighbor2=11
worker number = 10 offset= 360 number_rows= 40 start = 360 end =399
Sent to= 11 offset= 400 number_rows= 40 neighbor1= 10 neighbor2=12
worker number = 11 offset= 400 number_rows= 40 start = 400 end =439
Sent to= 12 offset= 440 number_rows= 40 neighbor1= 11 neighbor2=13
worker number = 12 offset= 440 number_rows= 40 start = 440 end =479
Sent to= 13 offset= 480 number_rows= 40 neighbor1= 12 neighbor2=14
worker number = 13 offset= 480 number_rows= 40 start = 480 end =519
Sent to= 14 offset= 520 number_rows= 40 neighbor1= 13 neighbor2=15
worker number = 14 offset= 520 number_rows= 40 start = 520 end =559
Sent to= 15 offset= 560 number_rows= 40 neighbor1= 14 neighbor2=16
worker number = 15 offset= 560 number_rows= 40 start = 560 end =599
Sent to= 16 offset= 600 number_rows= 40 neighbor1= 15 neighbor2=17
worker number = 16 offset= 600 number_rows= 40 start = 600 end =639
Sent to= 17 offset= 640 number_rows= 40 neighbor1= 16 neighbor2=18
worker number = 17 offset= 640 number_rows= 40 start = 640 end =679
Sent to= 18 offset= 680 number_rows= 40 neighbor1= 17 neighbor2=19
worker number = 18 offset= 680 number_rows= 40 start = 680 end =719
Sent to= 19 offset= 720 number_rows= 40 neighbor1= 18 neighbor2=20
worker number = 19 offset= 720 number_rows= 40 start = 720 end =759
Sent to= 20 offset= 760 number_rows= 40 neighbor1= 19 neighbor2=21
worker number = 20 offset= 760 number_rows= 40 start = 760 end =799
Sent to= 21 offset= 800 number_rows= 40 neighbor1= 20 neighbor2=22
worker number = 21 offset= 800 number_rows= 40 start = 800 end =839
Sent to= 22 offset= 840 number_rows= 40 neighbor1= 21 neighbor2=23
worker number = 22 offset= 840 number_rows= 40 start = 840 end =879
Sent to= 23 offset= 880 number_rows= 40 neighbor1= 22 neighbor2=24
worker number = 23 offset= 880 number_rows= 40 start = 880 end =919
Sent to= 24 offset= 920 number_rows= 40 neighbor1= 23 neighbor2=25
worker number = 24 offset= 920 number_rows= 40 start = 920 end =959
Sent to= 25 offset= 960 number_rows= 40 neighbor1= 24 neighbor2=26
worker number = 25 offset= 960 number_rows= 40 start = 960 end =999
Sent to= 26 offset= 1000 number_rows= 40 neighbor1= 25 neighbor2=27
worker number = 26 offset= 1000 number_rows= 40 start = 1000 end =1039
Sent to= 27 offset= 1040 number_rows= 40 neighbor1= 26 neighbor2=28
worker number = 27 offset= 1040 number_rows= 40 start = 1040 end =1079
Sent to= 28 offset= 1080 number_rows= 40 neighbor1= 27 neighbor2=29
worker number = 28 offset= 1080 number_rows= 40 start = 1080 end =1119
Sent to= 29 offset= 1120 number_rows= 40 neighbor1= 28 neighbor2=30
worker number = 29 offset= 1120 number_rows= 40 start = 1120 end =1159
Sent to= 30 offset= 1160 number_rows= 40 neighbor1= 29 neighbor2=31
worker number = 30 offset= 1160 number_rows= 40 start = 1160 end =1199
Sent to= 31 offset= 1200 number_rows= 40 neighbor1= 30 neighbor2=32
worker number = 31 offset= 1200 number_rows= 40 start = 1200 end =1239
Sent to= 32 offset= 1240 number_rows= 40 neighbor1= 31 neighbor2=33
worker number = 32 offset= 1240 number_rows= 40 start = 1240 end =1279
Sent to= 33 offset= 1280 number_rows= 40 neighbor1= 32 neighbor2=34
worker number = 33 offset= 1280 number_rows= 40 start = 1280 end =1319
Sent to= 34 offset= 1320 number_rows= 40 neighbor1= 33 neighbor2=35
worker number = 34 offset= 1320 number_rows= 40 start = 1320 end =1359
Sent to= 35 offset= 1360 number_rows= 40 neighbor1= 34 neighbor2=36
worker number = 35 offset= 1360 number_rows= 40 start = 1360 end =1399
Sent to= 36 offset= 1400 number_rows= 40 neighbor1= 35 neighbor2=37
worker number = 36 offset= 1400 number_rows= 40 start = 1400 end =1439
Sent to= 37 offset= 1440 number_rows= 40 neighbor1= 36 neighbor2=38
worker number = 37 offset= 1440 number_rows= 40 start = 1440 end =1479
Sent to= 38 offset= 1480 number_rows= 40 neighbor1= 37 neighbor2=39
worker number = 38 offset= 1480 number_rows= 40 start = 1480 end =1519
Sent to= 39 offset= 1520 number_rows= 40 neighbor1= 38 neighbor2=40
worker number = 39 offset= 1520 number_rows= 40 start = 1520 end =1559
Sent to= 40 offset= 1560 number_rows= 40 neighbor1= 39 neighbor2=41
worker number = 40 offset= 1560 number_rows= 40 start = 1560 end =1599
Sent to= 41 offset= 1600 number_rows= 40 neighbor1= 40 neighbor2=42
worker number = 41 offset= 1600 number_rows= 40 start = 1600 end =1639
Sent to= 42 offset= 1640 number_rows= 40 neighbor1= 41 neighbor2=43
worker number = 42 offset= 1640 number_rows= 40 start = 1640 end =1679
Sent to= 43 offset= 1680 number_rows= 40 neighbor1= 42 neighbor2=44
worker number = 43 offset= 1680 number_rows= 40 start = 1680 end =1719
Sent to= 44 offset= 1720 number_rows= 40 neighbor1= 43 neighbor2=45
worker number = 44 offset= 1720 number_rows= 40 start = 1720 end =1759
Sent to= 45 offset= 1760 number_rows= 40 neighbor1= 44 neighbor2=46
worker number = 45 offset= 1760 number_rows= 40 start = 1760 end =1799
Sent to= 46 offset= 1800 number_rows= 40 neighbor1= 45 neighbor2=47
worker number = 46 offset= 1800 number_rows= 40 start = 1800 end =1839
Sent to= 47 offset= 1840 number_rows= 40 neighbor1= 46 neighbor2=48
worker number = 47 offset= 1840 number_rows= 40 start = 1840 end =1879
Sent to= 48 offset= 1880 number_rows= 39 neighbor1= 47 neighbor2=49
worker number = 48 offset= 1880 number_rows= 39 start = 1880 end =1918
Sent to= 49 offset= 1919 number_rows= 39 neighbor1= 48 neighbor2=50
worker number = 49 offset= 1919 number_rows= 39 start = 1919 end =1957
Sent to= 50 offset= 1958 number_rows= 39 neighbor1= 49 neighbor2=51
worker number = 50 offset= 1958 number_rows= 39 start = 1958 end =1996
Sent to= 51 offset= 1997 number_rows= 39 neighbor1= 50 neighbor2=52
worker number = 51 offset= 1997 number_rows= 39 start = 1997 end =2035
Sent to= 52 offset= 2036 number_rows= 39 neighbor1= 51 neighbor2=53
worker number = 52 offset= 2036 number_rows= 39 start = 2036 end =2074
Sent to= 53 offset= 2075 number_rows= 39 neighbor1= 52 neighbor2=54
worker number = 53 offset= 2075 number_rows= 39 start = 2075 end =2113
Sent to= 54 offset= 2114 number_rows= 39 neighbor1= 53 neighbor2=55
worker number = 54 offset= 2114 number_rows= 39 start = 2114 end =2152
Sent to= 55 offset= 2153 number_rows= 39 neighbor1= 54 neighbor2=56
worker number = 55 offset= 2153 number_rows= 39 start = 2153 end =2191
Sent to= 56 offset= 2192 number_rows= 39 neighbor1= 55 neighbor2=57
worker number = 56 offset= 2192 number_rows= 39 start = 2192 end =2230
Sent to= 57 offset= 2231 number_rows= 39 neighbor1= 56 neighbor2=58
worker number = 57 offset= 2231 number_rows= 39 start = 2231 end =2269
Sent to= 58 offset= 2270 number_rows= 39 neighbor1= 57 neighbor2=59
worker number = 58 offset= 2270 number_rows= 39 start = 2270 end =2308
Sent to= 59 offset= 2309 number_rows= 39 neighbor1= 58 neighbor2=60
worker number = 59 offset= 2309 number_rows= 39 start = 2309 end =2347
Sent to= 60 offset= 2348 number_rows= 39 neighbor1= 59 neighbor2=61
worker number = 60 offset= 2348 number_rows= 39 start = 2348 end =2386
Sent to= 61 offset= 2387 number_rows= 39 neighbor1= 60 neighbor2=62
worker number = 61 offset= 2387 number_rows= 39 start = 2387 end =2425
Sent to= 62 offset= 2426 number_rows= 39 neighbor1= 61 neighbor2=63
worker number = 62 offset= 2426 number_rows= 39 start = 2426 end =2464
Sent to= 63 offset= 2465 number_rows= 39 neighbor1= 62 neighbor2=64
worker number = 63 offset= 2465 number_rows= 39 start = 2465 end =2503
Sent to= 64 offset= 2504 number_rows= 39 neighbor1= 63 neighbor2=65
worker number = 64 offset= 2504 number_rows= 39 start = 2504 end =2542
Sent to= 65 offset= 2543 number_rows= 39 neighbor1= 64 neighbor2=66
worker number = 65 offset= 2543 number_rows= 39 start = 2543 end =2581
Sent to= 66 offset= 2582 number_rows= 39 neighbor1= 65 neighbor2=67
worker number = 66 offset= 2582 number_rows= 39 start = 2582 end =2620
Sent to= 67 offset= 2621 number_rows= 39 neighbor1= 66 neighbor2=68
worker number = 67 offset= 2621 number_rows= 39 start = 2621 end =2659
Sent to= 68 offset= 2660 number_rows= 39 neighbor1= 67 neighbor2=69
worker number = 68 offset= 2660 number_rows= 39 start = 2660 end =2698
Sent to= 69 offset= 2699 number_rows= 39 neighbor1= 68 neighbor2=70
worker number = 69 offset= 2699 number_rows= 39 start = 2699 end =2737
Sent to= 70 offset= 2738 number_rows= 39 neighbor1= 69 neighbor2=71
worker number = 70 offset= 2738 number_rows= 39 start = 2738 end =2776
Sent to= 71 offset= 2777 number_rows= 39 neighbor1= 70 neighbor2=72
worker number = 71 offset= 2777 number_rows= 39 start = 2777 end =2815
Sent to= 72 offset= 2816 number_rows= 39 neighbor1= 71 neighbor2=73
worker number = 72 offset= 2816 number_rows= 39 start = 2816 end =2854
Sent to= 73 offset= 2855 number_rows= 39 neighbor1= 72 neighbor2=74
worker number = 73 offset= 2855 number_rows= 39 start = 2855 end =2893
Sent to= 74 offset= 2894 number_rows= 39 neighbor1= 73 neighbor2=75
worker number = 74 offset= 2894 number_rows= 39 start = 2894 end =2932
Sent to= 75 offset= 2933 number_rows= 39 neighbor1= 74 neighbor2=76
worker number = 75 offset= 2933 number_rows= 39 start = 2933 end =2971
Sent to= 76 offset= 2972 number_rows= 39 neighbor1= 75 neighbor2=77
worker number = 76 offset= 2972 number_rows= 39 start = 2972 end =3010
Sent to= 77 offset= 3011 number_rows= 39 neighbor1= 76 neighbor2=78
worker number = 77 offset= 3011 number_rows= 39 start = 3011 end =3049
Sent to= 78 offset= 3050 number_rows= 39 neighbor1= 77 neighbor2=79
worker number = 78 offset= 3050 number_rows= 39 start = 3050 end =3088
Sent to= 79 offset= 3089 number_rows= 39 neighbor1= 78 neighbor2=80
worker number = 79 offset= 3089 number_rows= 39 start = 3089 end =3127
Sent to= 80 offset= 3128 number_rows= 39 neighbor1= 79 neighbor2=81
worker number = 80 offset= 3128 number_rows= 39 start = 3128 end =3166
Sent to= 81 offset= 3167 number_rows= 39 neighbor1= 80 neighbor2=82
worker number = 81 offset= 3167 number_rows= 39 start = 3167 end =3205
Sent to= 82 offset= 3206 number_rows= 39 neighbor1= 81 neighbor2=83
worker number = 82 offset= 3206 number_rows= 39 start = 3206 end =3244
Sent to= 83 offset= 3245 number_rows= 39 neighbor1= 82 neighbor2=84
worker number = 83 offset= 3245 number_rows= 39 start = 3245 end =3283
Sent to= 84 offset= 3284 number_rows= 39 neighbor1= 83 neighbor2=85
worker number = 84 offset= 3284 number_rows= 39 start = 3284 end =3322
Sent to= 85 offset= 3323 number_rows= 39 neighbor1= 84 neighbor2=86
worker number = 85 offset= 3323 number_rows= 39 start = 3323 end =3361
Sent to= 86 offset= 3362 number_rows= 39 neighbor1= 85 neighbor2=87
worker number = 86 offset= 3362 number_rows= 39 start = 3362 end =3400
Sent to= 87 offset= 3401 number_rows= 39 neighbor1= 86 neighbor2=88
worker number = 87 offset= 3401 number_rows= 39 start = 3401 end =3439
Sent to= 88 offset= 3440 number_rows= 39 neighbor1= 87 neighbor2=89
worker number = 88 offset= 3440 number_rows= 39 start = 3440 end =3478
Sent to= 89 offset= 3479 number_rows= 39 neighbor1= 88 neighbor2=90
worker number = 89 offset= 3479 number_rows= 39 start = 3479 end =3517
Sent to= 90 offset= 3518 number_rows= 39 neighbor1= 89 neighbor2=91
worker number = 90 offset= 3518 number_rows= 39 start = 3518 end =3556
Sent to= 91 offset= 3557 number_rows= 39 neighbor1= 90 neighbor2=92
worker number = 91 offset= 3557 number_rows= 39 start = 3557 end =3595
Sent to= 92 offset= 3596 number_rows= 39 neighbor1= 91 neighbor2=93
worker number = 92 offset= 3596 number_rows= 39 start = 3596 end =3634
Sent to= 93 offset= 3635 number_rows= 39 neighbor1= 92 neighbor2=94
worker number = 93 offset= 3635 number_rows= 39 start = 3635 end =3673
Sent to= 94 offset= 3674 number_rows= 39 neighbor1= 93 neighbor2=95
worker number = 94 offset= 3674 number_rows= 39 start = 3674 end =3712
Sent to= 95 offset= 3713 number_rows= 39 neighbor1= 94 neighbor2=96
worker number = 95 offset= 3713 number_rows= 39 start = 3713 end =3751
Sent to= 96 offset= 3752 number_rows= 39 neighbor1= 95 neighbor2=97
worker number = 96 offset= 3752 number_rows= 39 start = 3752 end =3790
Sent to= 97 offset= 3791 number_rows= 39 neighbor1= 96 neighbor2=98
worker number = 97 offset= 3791 number_rows= 39 start = 3791 end =3829
Sent to= 98 offset= 3830 number_rows= 39 neighbor1= 97 neighbor2=99
worker number = 98 offset= 3830 number_rows= 39 start = 3830 end =3868
Sent to= 99 offset= 3869 number_rows= 39 neighbor1= 98 neighbor2=100
worker number = 99 offset= 3869 number_rows= 39 start = 3869 end =3907
Sent to= 100 offset= 3908 number_rows= 39 neighbor1= 99 neighbor2=101
worker number = 100 offset= 3908 number_rows= 39 start = 3908 end =3946
Sent to= 101 offset= 3947 number_rows= 39 neighbor1= 100 neighbor2=102
worker number = 101 offset= 3947 number_rows= 39 start = 3947 end =3985
Sent to= 102 offset= 3986 number_rows= 39 neighbor1= 101 neighbor2=103
worker number = 102 offset= 3986 number_rows= 39 start = 3986 end =4024
Sent to= 103 offset= 4025 number_rows= 39 neighbor1= 102 neighbor2=104
worker number = 103 offset= 4025 number_rows= 39 start = 4025 end =4063
Sent to= 104 offset= 4064 number_rows= 39 neighbor1= 103 neighbor2=105
worker number = 104 offset= 4064 number_rows= 39 start = 4064 end =4102
Sent to= 105 offset= 4103 number_rows= 39 neighbor1= 104 neighbor2=106
worker number = 105 offset= 4103 number_rows= 39 start = 4103 end =4141
Sent to= 106 offset= 4142 number_rows= 39 neighbor1= 105 neighbor2=107
worker number = 106 offset= 4142 number_rows= 39 start = 4142 end =4180
Sent to= 107 offset= 4181 number_rows= 39 neighbor1= 106 neighbor2=108
worker number = 107 offset= 4181 number_rows= 39 start = 4181 end =4219
Sent to= 108 offset= 4220 number_rows= 39 neighbor1= 107 neighbor2=109
worker number = 108 offset= 4220 number_rows= 39 start = 4220 end =4258
Sent to= 109 offset= 4259 number_rows= 39 neighbor1= 108 neighbor2=110
worker number = 109 offset= 4259 number_rows= 39 start = 4259 end =4297
Sent to= 110 offset= 4298 number_rows= 39 neighbor1= 109 neighbor2=111
worker number = 110 offset= 4298 number_rows= 39 start = 4298 end =4336
Sent to= 111 offset= 4337 number_rows= 39 neighbor1= 110 neighbor2=112
worker number = 111 offset= 4337 number_rows= 39 start = 4337 end =4375
Sent to= 112 offset= 4376 number_rows= 39 neighbor1= 111 neighbor2=113
worker number = 112 offset= 4376 number_rows= 39 start = 4376 end =4414
Sent to= 113 offset= 4415 number_rows= 39 neighbor1= 112 neighbor2=114
worker number = 113 offset= 4415 number_rows= 39 start = 4415 end =4453
Sent to= 114 offset= 4454 number_rows= 39 neighbor1= 113 neighbor2=115
worker number = 114 offset= 4454 number_rows= 39 start = 4454 end =4492
Sent to= 115 offset= 4493 number_rows= 39 neighbor1= 114 neighbor2=116
worker number = 115 offset= 4493 number_rows= 39 start = 4493 end =4531
Sent to= 116 offset= 4532 number_rows= 39 neighbor1= 115 neighbor2=117
worker number = 116 offset= 4532 number_rows= 39 start = 4532 end =4570
Sent to= 117 offset= 4571 number_rows= 39 neighbor1= 116 neighbor2=118
worker number = 117 offset= 4571 number_rows= 39 start = 4571 end =4609
Sent to= 118 offset= 4610 number_rows= 39 neighbor1= 117 neighbor2=119
worker number = 118 offset= 4610 number_rows= 39 start = 4610 end =4648
Sent to= 119 offset= 4649 number_rows= 39 neighbor1= 118 neighbor2=120
worker number = 119 offset= 4649 number_rows= 39 start = 4649 end =4687
Sent to= 120 offset= 4688 number_rows= 39 neighbor1= 119 neighbor2=121
worker number = 120 offset= 4688 number_rows= 39 start = 4688 end =4726
Sent to= 121 offset= 4727 number_rows= 39 neighbor1= 120 neighbor2=122
worker number = 121 offset= 4727 number_rows= 39 start = 4727 end =4765
Sent to= 122 offset= 4766 number_rows= 39 neighbor1= 121 neighbor2=123
worker number = 122 offset= 4766 number_rows= 39 start = 4766 end =4804
Sent to= 123 offset= 4805 number_rows= 39 neighbor1= 122 neighbor2=124
worker number = 123 offset= 4805 number_rows= 39 start = 4805 end =4843
Sent to= 124 offset= 4844 number_rows= 39 neighbor1= 123 neighbor2=125
worker number = 124 offset= 4844 number_rows= 39 start = 4844 end =4882
Sent to= 125 offset= 4883 number_rows= 39 neighbor1= 124 neighbor2=126
worker number = 125 offset= 4883 number_rows= 39 start = 4883 end =4921
Sent to= 126 offset= 4922 number_rows= 39 neighbor1= 125 neighbor2=127
worker number = 126 offset= 4922 number_rows= 39 start = 4922 end =4960
Sent to= 127 offset= 4961 number_rows= 39 neighbor1= 126 neighbor2=0
worker number = 127 offset= 4961 number_rows= 39 start = 4961 end =4998
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035738000009000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035739000010000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035739000010000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035738000009000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010171000017000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010171000017000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010173000019000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010173000019000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010175000021000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010175000021000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010177000023000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010177000023000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010170000016000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010170000016000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035730000001000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035730000001000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003518000045000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003518000045000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035732000003000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035732000003000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040922000048000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040922000048000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040923000049000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040923000049000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035731000002000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035731000002000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035734000005000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035734000005000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035733000004000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035733000004000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003505000032000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003505000032000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035737000008000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035737000008000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035742000013000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035742000013000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035736000007000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035736000007000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040925000051000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035735000006000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035735000006000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040924000050000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035744000014000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035744000014000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035740000011000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035740000011000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035741000012000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035741000012000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035746000015000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035746000015000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040928000054000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040924000050000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040925000051000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040928000054000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040926000052000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040931000057000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040931000057000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004560000071000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040926000052000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004560000071000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040934000060000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040934000060000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040937000062000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040927000053000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040927000053000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040937000062000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004564000075000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004564000075000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004566000077000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004566000077000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040930000056000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040932000058000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040930000056000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040932000058000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040935000061000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040935000061000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040940000063000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040940000063000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040929000055000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040929000055000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco4.0000040933000059000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco4.0000040933000059000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010174000020000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010179000025000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013893000081000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013893000081000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010187000031000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010172000018000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010176000022000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010178000024000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003507000034000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003507000034000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010183000029000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010182000028000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003506000033000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010185000030000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010185000030000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010183000029000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003506000033000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010179000025000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003509000036000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010176000022000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003509000036000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010174000020000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010180000026000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010180000026000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco2.0000010181000027000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010181000027000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010187000031000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010172000018000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010178000024000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco2.0000010182000028000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013899000087000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013899000087000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004553000064000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004553000064000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013905000093000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013905000093000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004554000065000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004554000065000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003512000039000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003512000039000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003514000041000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003514000041000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003508000035000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003508000035000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003511000038000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013892000080000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003511000038000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013892000080000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004555000066000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004555000066000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003510000037000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003519000046000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003510000037000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015320000101000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003517000044000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003517000044000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003519000046000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003513000040000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015320000101000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003513000040000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003515000042000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003516000043000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco3.0000003521000047000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003521000047000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003515000042000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco3.0000003516000043000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015326000107000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015326000107000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004558000069000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004558000069000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015328000109000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015328000109000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004556000067000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004556000067000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015315000096000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015315000096000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013894000082000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013894000082000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016380000114000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016380000114000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004557000068000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004561000072000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004561000072000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004557000068000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004562000073000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004562000073000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004570000079000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004570000079000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004565000076000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004565000076000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004559000070000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004559000070000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004563000074000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004563000074000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco1.0000035729000000000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco1.0000035729000000000000.sym
Extrae: Deallocating memory.
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco5.0000004567000078000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco5.0000004567000078000000.sym
Extrae: Application has ended. Tracing has been terminated.
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013895000083000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013895000083000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016392000126000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013897000085000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013897000085000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016392000126000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016390000124000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016390000124000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013896000084000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013896000084000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013903000091000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013903000091000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015317000098000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015317000098000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013900000088000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016378000112000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013898000086000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016378000112000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013898000086000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013900000088000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013902000090000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013902000090000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015316000097000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013904000092000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013904000092000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015316000097000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013901000089000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013906000094000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013901000089000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013906000094000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco6.0000013907000095000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco6.0000013907000095000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016379000113000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016379000113000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015319000100000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015319000100000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015318000099000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015318000099000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015321000102000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015321000102000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015324000105000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016393000127000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015324000105000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016393000127000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015322000103000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015322000103000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016381000115000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016381000115000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015329000110000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015329000110000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015325000106000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015325000106000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015330000111000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015330000111000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015323000104000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015323000104000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco7.0000015327000108000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco7.0000015327000108000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016382000116000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016382000116000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016384000118000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016384000118000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016386000120000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016386000120000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016388000122000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016383000117000000.mpit
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016387000121000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016387000121000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016388000122000000.sym
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016383000117000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016391000125000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016391000125000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016389000123000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016389000123000000.sym
Extrae: Intermediate raw trace file created : /home/schnorr/set-0/TRACE@draco8.0000016385000119000000.mpit
Extrae: Intermediate raw sym file created : /home/schnorr/set-0/TRACE@draco8.0000016385000119000000.sym
total 16708
-rw-r--r-- 1 schnorr schnorr  643664 May 14 02:00 TRACE@draco1.0000035729000000000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035729000000000000.sym
-rw-r--r-- 1 schnorr schnorr  568624 May 14 02:00 TRACE@draco1.0000035730000001000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035730000001000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035731000002000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035731000002000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035732000003000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035732000003000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035733000004000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035733000004000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035734000005000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035734000005000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035735000006000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035735000006000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035736000007000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035736000007000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035737000008000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035737000008000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035738000009000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035738000009000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035739000010000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035739000010000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035740000011000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035740000011000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035741000012000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035741000012000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035742000013000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035742000013000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035744000014000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035744000014000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:00 TRACE@draco1.0000035746000015000000.mpit
-rw-r--r-- 1 schnorr schnorr    1970 May 14 02:00 TRACE@draco1.0000035746000015000000.sym
33
total 17728
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010170000016000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010170000016000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010171000017000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010171000017000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010172000018000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010172000018000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010173000019000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010173000019000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010174000020000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010174000020000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010175000021000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010175000021000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010176000022000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010176000022000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010177000023000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010177000023000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010178000024000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010178000024000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010179000025000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010179000025000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010180000026000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010180000026000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010181000027000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010181000027000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010182000028000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010182000028000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010183000029000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010183000029000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010185000030000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010185000030000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:59 TRACE@draco2.0000010187000031000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 01:59 TRACE@draco2.0000010187000031000000.sym
33
total 17728
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003505000032000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003505000032000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003506000033000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003506000033000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003507000034000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003507000034000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003508000035000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003508000035000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003509000036000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003509000036000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003510000037000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003510000037000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003511000038000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003511000038000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003512000039000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003512000039000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003513000040000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003513000040000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003514000041000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003514000041000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003515000042000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003515000042000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003516000043000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003516000043000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003517000044000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003517000044000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003518000045000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003518000045000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003519000046000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003519000046000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco3.0000003521000047000000.mpit
-rw-r--r-- 1 schnorr schnorr    1574 May 14 02:00 TRACE@draco3.0000003521000047000000.sym
33
total 17728
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040922000048000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040922000048000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040923000049000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040923000049000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040924000050000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040924000050000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040925000051000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040925000051000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040926000052000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040926000052000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040927000053000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040927000053000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040928000054000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040928000054000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040929000055000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040929000055000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040930000056000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040930000056000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040931000057000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040931000057000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040932000058000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040932000058000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040933000059000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040933000059000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040934000060000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040934000060000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040935000061000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040935000061000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040937000062000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040937000062000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:52 TRACE@draco4.0000040940000063000000.mpit
-rw-r--r-- 1 schnorr schnorr    1960 May 14 01:51 TRACE@draco4.0000040940000063000000.sym
33
total 17728
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004553000064000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004553000064000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004554000065000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004554000065000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004555000066000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004555000066000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004556000067000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004556000067000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004557000068000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004557000068000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004558000069000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004558000069000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004559000070000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004559000070000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004560000071000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004560000071000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004561000072000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004561000072000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004562000073000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004562000073000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004563000074000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004563000074000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004564000075000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004564000075000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004565000076000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004565000076000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004566000077000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004566000077000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004567000078000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004567000078000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:01 TRACE@draco5.0000004570000079000000.mpit
-rw-r--r-- 1 schnorr schnorr    1883 May 14 02:01 TRACE@draco5.0000004570000079000000.sym
33
total 17728
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013892000080000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013892000080000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013893000081000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013893000081000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013894000082000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013894000082000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013895000083000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013895000083000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013896000084000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013896000084000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013897000085000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013897000085000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013898000086000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013898000086000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013899000087000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013899000087000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013900000088000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013900000088000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013901000089000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013901000089000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013902000090000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013902000090000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013903000091000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013903000091000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013904000092000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013904000092000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013905000093000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013905000093000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013906000094000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013906000094000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 02:04 TRACE@draco6.0000013907000095000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 02:04 TRACE@draco6.0000013907000095000000.sym
33
total 17728
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015315000096000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015315000096000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015316000097000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015316000097000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015317000098000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015317000098000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015318000099000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015318000099000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015319000100000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015319000100000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015320000101000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015320000101000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015321000102000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015321000102000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015322000103000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015322000103000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015323000104000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015323000104000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015324000105000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015324000105000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015325000106000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015325000106000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015326000107000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015326000107000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015327000108000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015327000108000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015328000109000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015328000109000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015329000110000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015329000110000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:55 TRACE@draco7.0000015330000111000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:55 TRACE@draco7.0000015330000111000000.sym
33
total 17180
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016378000112000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016378000112000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016379000113000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016379000113000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016380000114000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016380000114000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016381000115000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016381000115000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016382000116000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016382000116000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016383000117000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016383000117000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016384000118000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016384000118000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016385000119000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016385000119000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016386000120000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016386000120000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016387000121000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016387000121000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016388000122000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016388000122000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016389000123000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016389000123000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016390000124000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016390000124000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016391000125000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016391000125000000.sym
-rw-r--r-- 1 schnorr schnorr 1128624 May 14 01:51 TRACE@draco8.0000016392000126000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016392000126000000.sym
-rw-r--r-- 1 schnorr schnorr  568624 May 14 01:51 TRACE@draco8.0000016393000127000000.mpit
-rw-r--r-- 1 schnorr schnorr    2048 May 14 01:51 TRACE@draco8.0000016393000127000000.sym
33
#+end_example

#+begin_src sh :results output verbatim :dir /ssh:draco1.inf:~/
DIR=2016-05-14-2D_Heat_Transfer
rm -rf $DIR/draco*
mkdir -p $DIR/draco1-set-0/
cp -prf $HOME/set-0/* $DIR/draco1-set-0/
for i in `seq 2 8`; do
  T=$DIR/draco${i}-set-0/
  mkdir -p $T
  rsync -r draco${i}:./set-0/* $T
done
cd $DIR
find draco* | grep mpit$ | wc -l
find draco* | grep sym$ | wc -l
${EXTRAE_HOME}/bin/mpi2prv -f `find draco* | grep mpit$` -o 2016-05-14-2D_Heat_Transfer_5k_5k_500.prv
cd ..
tar cfz 2016-05-14-2D_Heat_Transfer.tar.gz 2016-05-14-2D_Heat_Transfer
#+end_src

#+RESULTS:
#+begin_example
128
128
merger: Output trace format is: Paraver
merger: Extrae 3.3.0 (revision 3966 based on extrae/trunk)
mpi2prv: Assigned nodes < draco1 >
mpi2prv: Assigned size per processor < 135 Mbytes >
mpi2prv: File draco1-set-0/TRACE@draco1.0000035744000014000000.mpit is object 1.15.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035737000008000000.mpit is object 1.9.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035735000006000000.mpit is object 1.7.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035742000013000000.mpit is object 1.14.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035732000003000000.mpit is object 1.4.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035733000004000000.mpit is object 1.5.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035739000010000000.mpit is object 1.11.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035730000001000000.mpit is object 1.2.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035746000015000000.mpit is object 1.16.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035738000009000000.mpit is object 1.10.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035734000005000000.mpit is object 1.6.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035740000011000000.mpit is object 1.12.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035729000000000000.mpit is object 1.1.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035731000002000000.mpit is object 1.3.1 on node draco1 assigned to processor 0
mpi2prv: File draco1-set-0/TRACE@draco1.0000035736000007000000.mpit is object 1.8.1 on node draco1 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010180000026000000.mpit is object 1.27.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010181000027000000.mpit is object 1.28.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010179000025000000.mpit is object 1.26.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010174000020000000.mpit is object 1.21.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010183000029000000.mpit is object 1.30.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010187000031000000.mpit is object 1.32.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010185000030000000.mpit is object 1.31.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010171000017000000.mpit is object 1.18.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010176000022000000.mpit is object 1.23.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010182000028000000.mpit is object 1.29.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010173000019000000.mpit is object 1.20.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010172000018000000.mpit is object 1.19.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010175000021000000.mpit is object 1.22.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010177000023000000.mpit is object 1.24.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010178000024000000.mpit is object 1.25.1 on node draco2 assigned to processor 0
mpi2prv: File draco2-set-0/TRACE@draco2.0000010170000016000000.mpit is object 1.17.1 on node draco2 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003517000044000000.mpit is object 1.45.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003513000040000000.mpit is object 1.41.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003507000034000000.mpit is object 1.35.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003506000033000000.mpit is object 1.34.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003510000037000000.mpit is object 1.38.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003514000041000000.mpit is object 1.42.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003505000032000000.mpit is object 1.33.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003518000045000000.mpit is object 1.46.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003508000035000000.mpit is object 1.36.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003512000039000000.mpit is object 1.40.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003519000046000000.mpit is object 1.47.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003511000038000000.mpit is object 1.39.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003515000042000000.mpit is object 1.43.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003509000036000000.mpit is object 1.37.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003521000047000000.mpit is object 1.48.1 on node draco3 assigned to processor 0
mpi2prv: File draco3-set-0/TRACE@draco3.0000003516000043000000.mpit is object 1.44.1 on node draco3 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040925000051000000.mpit is object 1.52.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040926000052000000.mpit is object 1.53.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040928000054000000.mpit is object 1.55.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040934000060000000.mpit is object 1.61.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040924000050000000.mpit is object 1.51.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040935000061000000.mpit is object 1.62.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040933000059000000.mpit is object 1.60.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040937000062000000.mpit is object 1.63.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040927000053000000.mpit is object 1.54.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040931000057000000.mpit is object 1.58.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040930000056000000.mpit is object 1.57.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040923000049000000.mpit is object 1.50.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040940000063000000.mpit is object 1.64.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040932000058000000.mpit is object 1.59.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040922000048000000.mpit is object 1.49.1 on node draco4 assigned to processor 0
mpi2prv: File draco4-set-0/TRACE@draco4.0000040929000055000000.mpit is object 1.56.1 on node draco4 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004570000079000000.mpit is object 1.80.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004564000075000000.mpit is object 1.76.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004563000074000000.mpit is object 1.75.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004565000076000000.mpit is object 1.77.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004566000077000000.mpit is object 1.78.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004553000064000000.mpit is object 1.65.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004561000072000000.mpit is object 1.73.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004562000073000000.mpit is object 1.74.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004555000066000000.mpit is object 1.67.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004559000070000000.mpit is object 1.71.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004557000068000000.mpit is object 1.69.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004558000069000000.mpit is object 1.70.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004567000078000000.mpit is object 1.79.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004554000065000000.mpit is object 1.66.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004560000071000000.mpit is object 1.72.1 on node draco5 assigned to processor 0
mpi2prv: File draco5-set-0/TRACE@draco5.0000004556000067000000.mpit is object 1.68.1 on node draco5 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013895000083000000.mpit is object 1.84.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013899000087000000.mpit is object 1.88.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013897000085000000.mpit is object 1.86.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013898000086000000.mpit is object 1.87.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013907000095000000.mpit is object 1.96.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013904000092000000.mpit is object 1.93.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013894000082000000.mpit is object 1.83.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013900000088000000.mpit is object 1.89.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013906000094000000.mpit is object 1.95.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013902000090000000.mpit is object 1.91.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013901000089000000.mpit is object 1.90.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013893000081000000.mpit is object 1.82.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013892000080000000.mpit is object 1.81.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013905000093000000.mpit is object 1.94.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013903000091000000.mpit is object 1.92.1 on node draco6 assigned to processor 0
mpi2prv: File draco6-set-0/TRACE@draco6.0000013896000084000000.mpit is object 1.85.1 on node draco6 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015330000111000000.mpit is object 1.112.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015320000101000000.mpit is object 1.102.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015317000098000000.mpit is object 1.99.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015323000104000000.mpit is object 1.105.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015326000107000000.mpit is object 1.108.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015319000100000000.mpit is object 1.101.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015324000105000000.mpit is object 1.106.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015318000099000000.mpit is object 1.100.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015322000103000000.mpit is object 1.104.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015321000102000000.mpit is object 1.103.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015315000096000000.mpit is object 1.97.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015328000109000000.mpit is object 1.110.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015325000106000000.mpit is object 1.107.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015329000110000000.mpit is object 1.111.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015316000097000000.mpit is object 1.98.1 on node draco7 assigned to processor 0
mpi2prv: File draco7-set-0/TRACE@draco7.0000015327000108000000.mpit is object 1.109.1 on node draco7 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016393000127000000.mpit is object 1.128.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016390000124000000.mpit is object 1.125.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016387000121000000.mpit is object 1.122.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016391000125000000.mpit is object 1.126.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016389000123000000.mpit is object 1.124.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016386000120000000.mpit is object 1.121.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016378000112000000.mpit is object 1.113.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016384000118000000.mpit is object 1.119.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016388000122000000.mpit is object 1.123.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016381000115000000.mpit is object 1.116.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016385000119000000.mpit is object 1.120.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016392000126000000.mpit is object 1.127.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016383000117000000.mpit is object 1.118.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016380000114000000.mpit is object 1.115.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016379000113000000.mpit is object 1.114.1 on node draco8 assigned to processor 0
mpi2prv: File draco8-set-0/TRACE@draco8.0000016382000116000000.mpit is object 1.117.1 on node draco8 assigned to processor 0
mpi2prv: Time synchronization has been turned on
mpi2prv: Checking for target directory existance... exists, ok!
mpi2prv: Selected output trace format is Paraver
mpi2prv: Stored trace format is Paraver
mpi2prv: Searching synchronization points... done
mpi2prv: Enabling Time Synchronization (Task).
mpi2prv: Circular buffer enabled at tracing time? NO
mpi2prv: Parsing intermediate files
mpi2prv: Progress 1 of 2 ... 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% done
mpi2prv: Processor 0 succeeded to translate its assigned files
mpi2prv: Elapsed time translating files: 0 hours 0 minutes 2 seconds
mpi2prv: Elapsed time sorting addresses: 0 hours 0 minutes 0 seconds
mpi2prv: Generating tracefile (intermediate buffers of 52840 events)
         This process can take a while. Please, be patient.
mpi2prv: Progress 2 of 2 ... 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% done
mpi2prv: Elapsed time merge step: 0 hours 0 minutes 3 seconds
mpi2prv: Resulting tracefile occupies 73502862 bytes
mpi2prv: Removing temporal files... done
mpi2prv: Elapsed time removing temporal files: 0 hours 0 minutes 0 seconds
mpi2prv: Congratulations! 2016-05-14-2D_Heat_Transfer_5k_5k_500.prv has been generated.
#+end_example

Make it available.

#+begin_src sh :results output verbatim :dir /ssh:guarani.inf:~/
cd Dropbox
rsync -v draco1:./2016-05-14-2D_Heat_Transfer.tar.gz .
dropbox sharelink 2016-05-14-2D_Heat_Transfer.tar.gz
#+end_src

#+RESULTS:
: 2016-05-14-2D_Heat_Transfer.tar.gz
: 
: sent 43 bytes  received 24,882,326 bytes  7,109,248.29 bytes/sec
: total size is 24,876,145  speedup is 1.00
: https://www.dropbox.com/s/pr17yli94u9v3v0/2016-05-14-2D_Heat_Transfer.tar.gz?dl=0

You have the link to download.

Let's try to convert to tit on =guarani=.

#+begin_src sh :results output verbatim :dir /ssh:guarani.inf:~/svn/bozzetti/
tar xfz ~/Dropbox/2016-05-14-2D_Heat_Transfer.tar.gz
cd 2016-05-14-2D_Heat_Transfer
perl ../getpjdump/prv2tit.pl -i 2016-05-14-2D_Heat_Transfer_5k_5k_500 > 2016-05-14-2D_Heat_Transfer_5k_5k_500.tit
#+end_src

#+RESULTS:

#+begin_src sh :results output verbatim :dir /ssh:guarani.inf:~/svn/bozzetti/2016-05-14-2D_Heat_Transfer/
~/install/dimemas-5.2.12/bin/prv2dim -s 2016-05-14-2D_Heat_Transfer_5k_5k_500.prv 2016-05-14-2D_Heat_Transfer_5k_5k_500.dim
ls -l 2016-05-14-2D_Heat_Transfer_5k_5k_500.*
#+end_src

#+RESULTS:
#+begin_example
INITIALIZING PARSER... OK!
SPLITTING COMMUNICATIONS 000 %SPLITTING COMMUNICATIONS 001 %SPLITTING COMMUNICATIONS 002 %SPLITTING COMMUNICATIONS 003 %SPLITTING COMMUNICATIONS 004 %SPLITTING COMMUNICATIONS 005 %SPLITTING COMMUNICATIONS 006 %SPLITTING COMMUNICATIONS 007 %SPLITTING COMMUNICATIONS 008 %SPLITTING COMMUNICATIONS 009 %SPLITTING COMMUNICATIONS 010 %SPLITTING COMMUNICATIONS 011 %SPLITTING COMMUNICATIONS 012 %SPLITTING COMMUNICATIONS 013 %SPLITTING COMMUNICATIONS 014 %SPLITTING COMMUNICATIONS 015 %SPLITTING COMMUNICATIONS 016 %SPLITTING COMMUNICATIONS 017 %SPLITTING COMMUNICATIONS 018 %SPLITTING COMMUNICATIONS 019 %SPLITTING COMMUNICATIONS 020 %SPLITTING COMMUNICATIONS 021 %SPLITTING COMMUNICATIONS 022 %SPLITTING COMMUNICATIONS 023 %SPLITTING COMMUNICATIONS 024 %SPLITTING COMMUNICATIONS 025 %SPLITTING COMMUNICATIONS 026 %SPLITTING COMMUNICATIONS 027 %SPLITTING COMMUNICATIONS 028 %SPLITTING COMMUNICATIONS 029 %SPLITTING COMMUNICATIONS 030 %SPLITTING COMMUNICATIONS 031 %SPLITTING COMMUNICATIONS 032 %SPLITTING COMMUNICATIONS 033 %SPLITTING COMMUNICATIONS 034 %SPLITTING COMMUNICATIONS 035 %SPLITTING COMMUNICATIONS 036 %SPLITTING COMMUNICATIONS 037 %SPLITTING COMMUNICATIONS 038 %SPLITTING COMMUNICATIONS 039 %SPLITTING COMMUNICATIONS 040 %SPLITTING COMMUNICATIONS 041 %SPLITTING COMMUNICATIONS 042 %SPLITTING COMMUNICATIONS 043 %SPLITTING COMMUNICATIONS 044 %SPLITTING COMMUNICATIONS 045 %SPLITTING COMMUNICATIONS 046 %SPLITTING COMMUNICATIONS 047 %SPLITTING COMMUNICATIONS 048 %SPLITTING COMMUNICATIONS 049 %SPLITTING COMMUNICATIONS 050 %SPLITTING COMMUNICATIONS 051 %SPLITTING COMMUNICATIONS 052 %SPLITTING COMMUNICATIONS 053 %SPLITTING COMMUNICATIONS 054 %SPLITTING COMMUNICATIONS 055 %SPLITTING COMMUNICATIONS 056 %SPLITTING COMMUNICATIONS 057 %SPLITTING COMMUNICATIONS 058 %SPLITTING COMMUNICATIONS 059 %SPLITTING COMMUNICATIONS 060 %SPLITTING COMMUNICATIONS 061 %SPLITTING COMMUNICATIONS 062 %SPLITTING COMMUNICATIONS 063 %SPLITTING COMMUNICATIONS 064 %SPLITTING COMMUNICATIONS 065 %SPLITTING COMMUNICATIONS 066 %SPLITTING COMMUNICATIONS 067 %SPLITTING COMMUNICATIONS 068 %SPLITTING COMMUNICATIONS 069 %SPLITTING COMMUNICATIONS 070 %SPLITTING COMMUNICATIONS 071 %SPLITTING COMMUNICATIONS 072 %SPLITTING COMMUNICATIONS 073 %SPLITTING COMMUNICATIONS 074 %SPLITTING COMMUNICATIONS 075 %SPLITTING COMMUNICATIONS 076 %SPLITTING COMMUNICATIONS 077 %SPLITTING COMMUNICATIONS 078 %SPLITTING COMMUNICATIONS 079 %SPLITTING COMMUNICATIONS 080 %SPLITTING COMMUNICATIONS 081 %SPLITTING COMMUNICATIONS 082 %SPLITTING COMMUNICATIONS 083 %SPLITTING COMMUNICATIONS 084 %SPLITTING COMMUNICATIONS 085 %SPLITTING COMMUNICATIONS 086 %SPLITTING COMMUNICATIONS 087 %SPLITTING COMMUNICATIONS 088 %SPLITTING COMMUNICATIONS 089 %SPLITTING COMMUNICATIONS 090 %SPLITTING COMMUNICATIONS 091 %SPLITTING COMMUNICATIONS 092 %SPLITTING COMMUNICATIONS 093 %SPLITTING COMMUNICATIONS 094 %SPLITTING COMMUNICATIONS 095 %SPLITTING COMMUNICATIONS 096 %SPLITTING COMMUNICATIONS 097 %SPLITTING COMMUNICATIONS 098 %SPLITTING COMMUNICATIONS 099 %SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          629411
   * Splitted communications 375333
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK
CREATING TRANSLATION STRUCTURES  001/128CREATING TRANSLATION STRUCTURES  002/128CREATING TRANSLATION STRUCTURES  003/128CREATING TRANSLATION STRUCTURES  004/128CREATING TRANSLATION STRUCTURES  005/128CREATING TRANSLATION STRUCTURES  006/128CREATING TRANSLATION STRUCTURES  007/128CREATING TRANSLATION STRUCTURES  008/128CREATING TRANSLATION STRUCTURES  009/128CREATING TRANSLATION STRUCTURES  010/128CREATING TRANSLATION STRUCTURES  011/128CREATING TRANSLATION STRUCTURES  012/128CREATING TRANSLATION STRUCTURES  013/128CREATING TRANSLATION STRUCTURES  014/128CREATING TRANSLATION STRUCTURES  015/128CREATING TRANSLATION STRUCTURES  016/128CREATING TRANSLATION STRUCTURES  017/128CREATING TRANSLATION STRUCTURES  018/128CREATING TRANSLATION STRUCTURES  019/128CREATING TRANSLATION STRUCTURES  020/128CREATING TRANSLATION STRUCTURES  021/128CREATING TRANSLATION STRUCTURES  022/128CREATING TRANSLATION STRUCTURES  023/128CREATING TRANSLATION STRUCTURES  024/128CREATING TRANSLATION STRUCTURES  025/128CREATING TRANSLATION STRUCTURES  026/128CREATING TRANSLATION STRUCTURES  027/128CREATING TRANSLATION STRUCTURES  028/128CREATING TRANSLATION STRUCTURES  029/128CREATING TRANSLATION STRUCTURES  030/128CREATING TRANSLATION STRUCTURES  031/128CREATING TRANSLATION STRUCTURES  032/128CREATING TRANSLATION STRUCTURES  033/128CREATING TRANSLATION STRUCTURES  034/128CREATING TRANSLATION STRUCTURES  035/128CREATING TRANSLATION STRUCTURES  036/128CREATING TRANSLATION STRUCTURES  037/128CREATING TRANSLATION STRUCTURES  038/128CREATING TRANSLATION STRUCTURES  039/128CREATING TRANSLATION STRUCTURES  040/128CREATING TRANSLATION STRUCTURES  041/128CREATING TRANSLATION STRUCTURES  042/128CREATING TRANSLATION STRUCTURES  043/128CREATING TRANSLATION STRUCTURES  044/128CREATING TRANSLATION STRUCTURES  045/128CREATING TRANSLATION STRUCTURES  046/128CREATING TRANSLATION STRUCTURES  047/128CREATING TRANSLATION STRUCTURES  048/128CREATING TRANSLATION STRUCTURES  049/128CREATING TRANSLATION STRUCTURES  050/128CREATING TRANSLATION STRUCTURES  051/128CREATING TRANSLATION STRUCTURES  052/128CREATING TRANSLATION STRUCTURES  053/128CREATING TRANSLATION STRUCTURES  054/128CREATING TRANSLATION STRUCTURES  055/128CREATING TRANSLATION STRUCTURES  056/128CREATING TRANSLATION STRUCTURES  057/128CREATING TRANSLATION STRUCTURES  058/128CREATING TRANSLATION STRUCTURES  059/128CREATING TRANSLATION STRUCTURES  060/128CREATING TRANSLATION STRUCTURES  061/128CREATING TRANSLATION STRUCTURES  062/128CREATING TRANSLATION STRUCTURES  063/128CREATING TRANSLATION STRUCTURES  064/128CREATING TRANSLATION STRUCTURES  065/128CREATING TRANSLATION STRUCTURES  066/128CREATING TRANSLATION STRUCTURES  067/128CREATING TRANSLATION STRUCTURES  068/128CREATING TRANSLATION STRUCTURES  069/128CREATING TRANSLATION STRUCTURES  070/128CREATING TRANSLATION STRUCTURES  071/128CREATING TRANSLATION STRUCTURES  072/128CREATING TRANSLATION STRUCTURES  073/128CREATING TRANSLATION STRUCTURES  074/128CREATING TRANSLATION STRUCTURES  075/128CREATING TRANSLATION STRUCTURES  076/128CREATING TRANSLATION STRUCTURES  077/128CREATING TRANSLATION STRUCTURES  078/128CREATING TRANSLATION STRUCTURES  079/128CREATING TRANSLATION STRUCTURES  080/128CREATING TRANSLATION STRUCTURES  081/128CREATING TRANSLATION STRUCTURES  082/128CREATING TRANSLATION STRUCTURES  083/128CREATING TRANSLATION STRUCTURES  084/128CREATING TRANSLATION STRUCTURES  085/128CREATING TRANSLATION STRUCTURES  086/128CREATING TRANSLATION STRUCTURES  087/128CREATING TRANSLATION STRUCTURES  088/128CREATING TRANSLATION STRUCTURES  089/128CREATING TRANSLATION STRUCTURES  090/128CREATING TRANSLATION STRUCTURES  091/128CREATING TRANSLATION STRUCTURES  092/128CREATING TRANSLATION STRUCTURES  093/128CREATING TRANSLATION STRUCTURES  094/128CREATING TRANSLATION STRUCTURES  095/128CREATING TRANSLATION STRUCTURES  096/128CREATING TRANSLATION STRUCTURES  097/128CREATING TRANSLATION STRUCTURES  098/128CREATING TRANSLATION STRUCTURES  099/128CREATING TRANSLATION STRUCTURES  100/128CREATING TRANSLATION STRUCTURES  101/128CREATING TRANSLATION STRUCTURES  102/128CREATING TRANSLATION STRUCTURES  103/128CREATING TRANSLATION STRUCTURES  104/128CREATING TRANSLATION STRUCTURES  105/128CREATING TRANSLATION STRUCTURES  106/128CREATING TRANSLATION STRUCTURES  107/128CREATING TRANSLATION STRUCTURES  108/128CREATING TRANSLATION STRUCTURES  109/128CREATING TRANSLATION STRUCTURES  110/128CREATING TRANSLATION STRUCTURES  111/128CREATING TRANSLATION STRUCTURES  112/128CREATING TRANSLATION STRUCTURES  113/128CREATING TRANSLATION STRUCTURES  114/128CREATING TRANSLATION STRUCTURES  115/128CREATING TRANSLATION STRUCTURES  116/128CREATING TRANSLATION STRUCTURES  117/128CREATING TRANSLATION STRUCTURES  118/128CREATING TRANSLATION STRUCTURES  119/128CREATING TRANSLATION STRUCTURES  120/128CREATING TRANSLATION STRUCTURES  121/128CREATING TRANSLATION STRUCTURES  122/128CREATING TRANSLATION STRUCTURES  123/128CREATING TRANSLATION STRUCTURES  124/128CREATING TRANSLATION STRUCTURES  125/128CREATING TRANSLATION STRUCTURES  126/128CREATING TRANSLATION STRUCTURES  127/128CREATING TRANSLATION STRUCTURES  128/128CREATING TRANSLATION STRUCTURES  128/128
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION
TRANSLATING RECORDS 000 %TRANSLATING RECORDS 001 %TRANSLATING RECORDS 002 %TRANSLATING RECORDS 003 %TRANSLATING RECORDS 004 %TRANSLATING RECORDS 005 %TRANSLATING RECORDS 006 %TRANSLATING RECORDS 007 %TRANSLATING RECORDS 008 %TRANSLATING RECORDS 009 %TRANSLATING RECORDS 010 %TRANSLATING RECORDS 011 %TRANSLATING RECORDS 012 %TRANSLATING RECORDS 013 %TRANSLATING RECORDS 014 %TRANSLATING RECORDS 015 %TRANSLATING RECORDS 016 %TRANSLATING RECORDS 017 %TRANSLATING RECORDS 018 %TRANSLATING RECORDS 019 %TRANSLATING RECORDS 020 %TRANSLATING RECORDS 021 %TRANSLATING RECORDS 022 %TRANSLATING RECORDS 023 %TRANSLATING RECORDS 024 %TRANSLATING RECORDS 025 %TRANSLATING RECORDS 026 %TRANSLATING RECORDS 027 %TRANSLATING RECORDS 028 %TRANSLATING RECORDS 029 %TRANSLATING RECORDS 030 %TRANSLATING RECORDS 031 %TRANSLATING RECORDS 032 %TRANSLATING RECORDS 033 %TRANSLATING RECORDS 034 %TRANSLATING RECORDS 035 %TRANSLATING RECORDS 036 %TRANSLATING RECORDS 037 %TRANSLATING RECORDS 038 %TRANSLATING RECORDS 039 %TRANSLATING RECORDS 040 %TRANSLATING RECORDS 041 %TRANSLATING RECORDS 042 %TRANSLATING RECORDS 043 %TRANSLATING RECORDS 044 %TRANSLATING RECORDS 045 %TRANSLATING RECORDS 046 %TRANSLATING RECORDS 047 %TRANSLATING RECORDS 048 %TRANSLATING RECORDS 049 %TRANSLATING RECORDS 050 %TRANSLATING RECORDS 051 %TRANSLATING RECORDS 052 %TRANSLATING RECORDS 053 %TRANSLATING RECORDS 054 %TRANSLATING RECORDS 055 %TRANSLATING RECORDS 056 %TRANSLATING RECORDS 057 %TRANSLATING RECORDS 058 %TRANSLATING RECORDS 059 %TRANSLATING RECORDS 060 %TRANSLATING RECORDS 061 %TRANSLATING RECORDS 062 %TRANSLATING RECORDS 063 %TRANSLATING RECORDS 064 %TRANSLATING RECORDS 065 %TRANSLATING RECORDS 066 %TRANSLATING RECORDS 067 %TRANSLATING RECORDS 068 %TRANSLATING RECORDS 069 %TRANSLATING RECORDS 070 %TRANSLATING RECORDS 071 %TRANSLATING RECORDS 072 %TRANSLATING RECORDS 073 %TRANSLATING RECORDS 074 %TRANSLATING RECORDS 075 %TRANSLATING RECORDS 076 %TRANSLATING RECORDS 077 %TRANSLATING RECORDS 078 %TRANSLATING RECORDS 079 %TRANSLATING RECORDS 080 %TRANSLATING RECORDS 081 %TRANSLATING RECORDS 082 %TRANSLATING RECORDS 083 %TRANSLATING RECORDS 084 %TRANSLATING RECORDS 085 %TRANSLATING RECORDS 086 %TRANSLATING RECORDS 087 %TRANSLATING RECORDS 088 %TRANSLATING RECORDS 089 %TRANSLATING RECORDS 090 %TRANSLATING RECORDS 091 %TRANSLATING RECORDS 092 %TRANSLATING RECORDS 093 %TRANSLATING RECORDS 094 %TRANSLATING RECORDS 095 %TRANSLATING RECORDS 096 %TRANSLATING RECORDS 097 %TRANSLATING RECORDS 098 %TRANSLATING RECORDS 099 %TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES
   * Merging task    1 100 %   * Merging task    2 100 %   * Merging task    3 100 %   * Merging task    4 100 %   * Merging task    5 100 %   * Merging task    6 100 %   * Merging task    7 100 %   * Merging task    8 100 %   * Merging task    9 100 %   * Merging task   10 100 %   * Merging task   11 100 %   * Merging task   12 100 %   * Merging task   13 100 %   * Merging task   14 100 %   * Merging task   15 100 %   * Merging task   16 100 %   * Merging task   17 100 %   * Merging task   18 100 %   * Merging task   19 100 %   * Merging task   20 100 %   * Merging task   21 100 %   * Merging task   22 100 %   * Merging task   23 100 %   * Merging task   24 100 %   * Merging task   25 100 %   * Merging task   26 100 %   * Merging task   27 100 %   * Merging task   28 100 %   * Merging task   29 100 %   * Merging task   30 100 %   * Merging task   31 100 %   * Merging task   32 100 %   * Merging task   33 100 %   * Merging task   34 100 %   * Merging task   35 100 %   * Merging task   36 100 %   * Merging task   37 100 %   * Merging task   38 100 %   * Merging task   39 100 %   * Merging task   40 100 %   * Merging task   41 100 %   * Merging task   42 100 %   * Merging task   43 100 %   * Merging task   44 100 %   * Merging task   45 100 %   * Merging task   46 100 %   * Merging task   47 100 %   * Merging task   48 100 %   * Merging task   49 100 %   * Merging task   50 100 %   * Merging task   51 100 %   * Merging task   52 100 %   * Merging task   53 100 %   * Merging task   54 100 %   * Merging task   55 100 %   * Merging task   56 100 %   * Merging task   57 100 %   * Merging task   58 100 %   * Merging task   59 100 %   * Merging task   60 100 %   * Merging task   61 100 %   * Merging task   62 100 %   * Merging task   63 100 %   * Merging task   64 100 %   * Merging task   65 100 %   * Merging task   66 100 %   * Merging task   67 100 %   * Merging task   68 100 %   * Merging task   69 100 %   * Merging task   70 100 %   * Merging task   71 100 %   * Merging task   72 100 %   * Merging task   73 100 %   * Merging task   74 100 %   * Merging task   75 100 %   * Merging task   76 100 %   * Merging task   77 100 %   * Merging task   78 100 %   * Merging task   79 100 %   * Merging task   80 100 %   * Merging task   81 100 %   * Merging task   82 100 %   * Merging task   83 100 %   * Merging task   84 100 %   * Merging task   85 100 %   * Merging task   86 100 %   * Merging task   87 100 %   * Merging task   88 100 %   * Merging task   89 100 %   * Merging task   90 100 %   * Merging task   91 100 %   * Merging task   92 100 %   * Merging task   93 100 %   * Merging task   94 100 %   * Merging task   95 100 %   * Merging task   96 100 %   * Merging task   97 100 %   * Merging task   98 100 %   * Merging task   99 100 %   * Merging task  100 100 %   * Merging task  101 100 %   * Merging task  102 100 %   * Merging task  103 100 %   * Merging task  104 100 %   * Merging task  105 100 %   * Merging task  106 100 %   * Merging task  107 100 %   * Merging task  108 100 %   * Merging task  109 100 %   * Merging task  110 100 %   * Merging task  111 100 %   * Merging task  112 100 %   * Merging task  113 100 %   * Merging task  114 100 %   * Merging task  115 100 %   * Merging task  116 100 %   * Merging task  117 100 %   * Merging task  118 100 %   * Merging task  119 100 %   * Merging task  120 100 %   * Merging task  121 100 %   * Merging task  122 100 %   * Merging task  123 100 %   * Merging task  124 100 %   * Merging task  125 100 %   * Merging task  126 100 %   * Merging task  127 100 %   * Merging task  128 100 %   * All task merged!         

********************************************************************************
 *                               WARNING                                        *
********************************************************************************
1 tasks have communications records outside a communication block
WARNING: The simulation of this trace could be inconsistent
NOTE: If the Paraver trace comes from a trace cut, check the cut limtis
********************************************************************************

TRANSLATION FINISHED
GENERATING PCF
-> Input PCF and ouput PCF have the same name. Please use it in your simulations
COPYING ROW FILE
-> Input and ouput ROW files have the same name. Please use it in your simulations
-rw-r--r-- 1 schnorr schnorr 69855450 May 14 02:58 2016-05-14-2D_Heat_Transfer_5k_5k_500.dim
-rw-r--r-- 1 schnorr schnorr     7422 May 14 02:13 2016-05-14-2D_Heat_Transfer_5k_5k_500.pcf
-rw-r--r-- 1 schnorr schnorr 73502862 May 14 02:13 2016-05-14-2D_Heat_Transfer_5k_5k_500.prv
-rw-r--r-- 1 schnorr schnorr     3312 May 14 02:13 2016-05-14-2D_Heat_Transfer_5k_5k_500.row
-rw-r--r-- 1 schnorr schnorr  8837915 May 14 02:22 2016-05-14-2D_Heat_Transfer_5k_5k_500.tit
#+end_example

Looks nice, let's simulate with Dimemas.

The configuration file.

#+begin_src text :results output :session :exports both  :tangle "./draco-8.cfg"
SDDFA
/*
 * "Dimemas Configuration Format:" "Version 2.5"
 * "Last update"  "09/18/02 at 12:51"
 */ ;;


#0:
"wide area network information" {
   // "wan_name" "name of the wide area network simulated"
   char "wan_name"[];
   // "number_of_machines" "number of machines in wan"
   int "number_of_machines";
   // "number_dedicated_connections" "number of dedicated connections between machines in the simulated system"
   int "number_dedicated_connections";
   //"function_of_traffic" "function that models influence of traffic in the non dedicated network"
        // "options: 1 EXP, 2 LOG, 3 LIN, 4 CT"
   int "function_of_traffic";
   // Maximal value of traffic in the network
   double "max_traffic_value";
   // "external_net_bandwidth" "external net bandwidth in MB/s"
   double "external_net_bandwidth";
   // "1 Constant, 2 Lineal, 3 Logarithmic"
   int "communication_group_model";
};;



#1:
"environment information" {
   char "machine_name"[];
   int "machine_id";
   // "instrumented_architecture" "Architecture used to instrument"
   char    "instrumented_architecture"[];
   // "number_of_nodes" "Number of nodes on virtual machine"
   int     "number_of_nodes";
   // "network_bandwidth" "Data tranfer rate between nodes in Mbytes/s"
   // "0 means instantaneous communication"
   double  "network_bandwidth";
   // "number_of_buses_on_network" "Maximun number of messages on network"
   // "0 means no limit"
   // "1 means bus contention"
   int   "number_of_buses_on_network";
   // "1 Constant, 2 Lineal, 3 Logarithmic"
   int   "communication_group_model";
};;


#2:
"node information" {
   int "machine_id";
   // "node_id" "Node number"
   int     "node_id";
   // "simulated_architecture" "Architecture node name"
   char    "simulated_architecture"[];
   // "number_of_processors" "Number of processors within node"
   int     "number_of_processors";
	// "speed_ratio_instrumented_vs_simulated" "Relative processor speed"
	double  "speed_ratio_instrumented_vs_simulated";
	// "intra_node_startup" "Startup time (s) of intra-node communications model"
	double  "intra_node_startup";
	// "intra_node_bandwidth" "Bandwidth (MB/s) of intra-node communications model"
	// "0 means instantaneous communication"
	double  "intra_node_bandwidth";
	// "intra_node_buses" "Number of buses of intra-node communications model"
	// "0 means infinite buses"
	int     "intra_node_buses";
	// "intra_node_input_links" "Input links of intra-node communications model"
	int     "intra_node_input_links";
	// "intra_node_input_links" "Output links of intra-node communications model"
	int     "intra_node_output_links";
	// "intra_node_startup" "Startup time (s) of inter-node communications model"
	double  "inter_node_startup";
	// "inter_node_input_links" "Input links of inter-node communications model"
	int     "inter_node_input_links";
	// "inter_node_output_links" "Input links of intra-node communications model"
	int     "inter_node_output_links";
	// "wan_startup" "Startup time (s) of inter-machines (WAN) communications model"
	double  "wan_startup";
};;


#3:
"mapping information" {
   // "tracefile" "Tracefile name of application"
   char    "tracefile"[];
   // "number_of_tasks" "Number of tasks in application"
   int     "number_of_tasks";
   // "mapping_tasks_to_nodes" "List of nodes in application"
   int     "mapping_tasks_to_nodes"[];
};;


#4:
"configuration files" {
   char       "scheduler"[];
   char       "file_system"[];
   char       "communication"[];
   char       "sensitivity"[];
};;


#5:
"modules information" {
// Module type
        int     "type";
        // Module value
        int     "value";
        // Speed ratio for this module, 0 means instantaneous execution
        double  "execution_ratio";
};;


#6:
"file system parameters" {
   double     "disk latency";
   double     "disk bandwidth";
   double     "block size";
   int        "concurrent requests";
   double     "hit ratio";
};;


#7:
"dedicated connection information" {
   // "connection_id" "connection number"
   int "connection_id";
   // "source_machine" "source machine number"
   int "source_machine";
   // "destination_machine" "destination machine number"
   int "destination_machine";
   // "connection_bandwidth" "bandwidth of the connection in Mbytes/s"
   double "connection_bandwidth";
   // "tags_list" "list of tags that will use the connection"
   int "tags_list"[];
   // "first_message_size" "size of messages in bytes" int "first_message_size";
   int "first_message_size";
   // "first_size_condition" "size condition that should meet messages to use the connection"
        // "it can be <, =, > and its is referent to message_size
   char "first_size_condition"[];
   // "operation" "& AND, | OR"
   char "operation"[];
    // "second_message_size" "size of messages in bytes"
   int "second_message_size";
   // "second_size_condition" "size condition that should meet messages to use the connection"
        // "it can be <, =, > and its is referent to message_size"
   char "second_size_condition"[];
   // "list_communicators" "list of communicators of coll. Operations that can use the connection"
   int "list_communicators"[];
   // Latency of dedicated connection in seconds
   double "connection_startup";
   //Latency due to distance in seconds
        double "flight_time";
};;


"wide area network information" {"", 1, 0, 4, 0.0, 0.0, 1};;

"environment information" {"draco", 0, "", 8, 1000.0, 0, 1};;

"node information" {0, 0, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 1, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 2, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 3, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 4, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 5, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 6, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;
"node information" {0, 7, "", 16, 1000.0, 0.0, 0.0, 0, 9999, 9999, 0.000128, 9999, 9999, 0.0 };;

"mapping information" {"2016-05-14-2D_Heat_Transfer_5k_5k_500.dim", 128, [128] 
  {
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7,
    0,1,2,3,4,5,6,7
  }};;

"configuration files" {"", "", "", ""};;

"file system parameters" {0.0, 0.0, 8.0, 0, 1.0};;
#+end_src

I need to tangle the code above with C-c C-v t

Then copy to guarani.inf

#+begin_src sh :results output :session :exports both
scp draco-8.cfg guarani.inf:~/svn/bozzetti/2016-05-14-2D_Heat_Transfer/
#+end_src

#+RESULTS:

Now let's replay with Dimemas.

We are using =-S 32K= according to Judit's suggestion.

#+begin_src sh :results output :session :exports both  :dir /ssh:guarani.inf:~/svn/bozzetti/2016-05-14-2D_Heat_Transfer/
~/install/dimemas-5.2.12/bin/Dimemas -S 32K -p output.prv draco-8.cfg > output
cat output
#+end_src

#+RESULTS:
#+begin_example
-> Simulator configuration to be read draco-8.cfg
-> Loading default random values
-> Loading default scheduler configuration
   * Machine 0. Policy: FIFO
   * Loading default communications configuration
-> Loding initial memory status
-> Loding initial semaphores status
-> Loading default file sytem configuration

1610.000000000: END SIMULATION

WARNING: : COMMUNIC_end: Task 00 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T127 Tag: 01 CommId: 253651 Size: 780000
WARNING: : COMMUNIC_end: Task 00 ends with 22 message(s) in reception queue:
          -> Sender: T01 t00  destT00  destt-1  Tag: 04 CommId: 310385 Size: 4
          -> Sender: T01 t00  destT00  destt-1  Tag: 04 CommId: 310388 Size: 4
          -> Sender: T02 t00  destT00  destt-1  Tag: 04 CommId: 310396 Size: 4
          -> Sender: T02 t00  destT00  destt-1  Tag: 04 CommId: 310399 Size: 4
          -> Sender: T03 t00  destT00  destt-1  Tag: 04 CommId: 310468 Size: 4
          -> Sender: T03 t00  destT00  destt-1  Tag: 04 CommId: 310471 Size: 4
          -> Sender: T04 t00  destT00  destt-1  Tag: 04 CommId: 310542 Size: 4
          -> Sender: T04 t00  destT00  destt-1  Tag: 04 CommId: 310545 Size: 4
          -> Sender: T05 t00  destT00  destt-1  Tag: 04 CommId: 310599 Size: 4
          -> Sender: T05 t00  destT00  destt-1  Tag: 04 CommId: 310602 Size: 4
          -> Sender: T06 t00  destT00  destt-1  Tag: 04 CommId: 310663 Size: 4
          -> Sender: T06 t00  destT00  destt-1  Tag: 04 CommId: 310666 Size: 4
          -> Sender: T07 t00  destT00  destt-1  Tag: 04 CommId: 310725 Size: 4
          -> Sender: T07 t00  destT00  destt-1  Tag: 04 CommId: 310731 Size: 4
          -> Sender: T08 t00  destT00  destt-1  Tag: 04 CommId: 310785 Size: 4
          -> Sender: T08 t00  destT00  destt-1  Tag: 04 CommId: 310788 Size: 4
          -> Sender: T09 t00  destT00  destt-1  Tag: 04 CommId: 310820 Size: 4
          -> Sender: T09 t00  destT00  destt-1  Tag: 04 CommId: 310827 Size: 4
          -> Sender: T11 t00  destT00  destt-1  Tag: 04 CommId: 310890 Size: 4
          -> Sender: T10 t00  destT00  destt-1  Tag: 04 CommId: 310856 Size: 4
          -> Sender: T11 t00  destT00  destt-1  Tag: 04 CommId: 310893 Size: 4
          -> Sender: T10 t00  destT00  destt-1  Tag: 04 CommId: 310859 Size: 4
WARNING: : COMMUNIC_end: Task 01 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310409 Size: 800000
WARNING: : COMMUNIC_end: Task 02 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310451 Size: 800000
WARNING: : COMMUNIC_end: Task 03 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310509 Size: 800000
WARNING: : COMMUNIC_end: Task 04 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310592 Size: 800000
WARNING: : COMMUNIC_end: Task 05 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310656 Size: 800000
WARNING: : COMMUNIC_end: Task 06 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310704 Size: 800000
WARNING: : COMMUNIC_end: Task 07 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310758 Size: 800000
WARNING: : COMMUNIC_end: Task 08 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310795 Size: 800000
WARNING: : COMMUNIC_end: Task 09 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310839 Size: 800000
WARNING: : COMMUNIC_end: Task 10 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310866 Size: 800000
WARNING: : COMMUNIC_end: Task 11 ends with 1 message(s) pending to send:
          * Thread 00 -> Dest-: T00 Tag: 04 CommId: 310901 Size: 800000
WARNING: : COMMUNIC_end: Task 13 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T14  t-1, Destination T13  t00  Tag: 03 CommId: 259875 Size: 20000
WARNING: : COMMUNIC_end: Task 14 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T15  t-1, Destination T14  t00  Tag: 03 CommId: 259833 Size: 20000
WARNING: : COMMUNIC_end: Task 14 ends with 1 message(s) in reception queue:
          -> Sender: T13 t00  destT14  destt-1  Tag: 02 CommId: 259795 Size: 20000
WARNING: : COMMUNIC_end: Task 15 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T16  t-1, Destination T15  t00  Tag: 03 CommId: 178908 Size: 20000
WARNING: : COMMUNIC_end: Task 15 ends with 1 message(s) in reception queue:
          -> Sender: T14 t00  destT15  destt-1  Tag: 02 CommId: 259758 Size: 20000
WARNING: : COMMUNIC_end: Task 16 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T17  t-1, Destination T16  t00  Tag: 03 CommId: 178767 Size: 20000
WARNING: : COMMUNIC_end: Task 16 ends with 1 message(s) in reception queue:
          -> Sender: T15 t00  destT16  destt-1  Tag: 02 CommId: 259712 Size: 20000
WARNING: : COMMUNIC_end: Task 17 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T18  t-1, Destination T17  t00  Tag: 03 CommId: 178668 Size: 20000
WARNING: : COMMUNIC_end: Task 17 ends with 1 message(s) in reception queue:
          -> Sender: T16 t00  destT17  destt-1  Tag: 02 CommId: 178690 Size: 20000
WARNING: : COMMUNIC_end: Task 18 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T19  t-1, Destination T18  t00  Tag: 03 CommId: 178534 Size: 20000
WARNING: : COMMUNIC_end: Task 18 ends with 1 message(s) in reception queue:
          -> Sender: T17 t00  destT18  destt-1  Tag: 02 CommId: 178553 Size: 20000
WARNING: : COMMUNIC_end: Task 19 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T20  t-1, Destination T19  t00  Tag: 03 CommId: 178447 Size: 20000
WARNING: : COMMUNIC_end: Task 19 ends with 1 message(s) in reception queue:
          -> Sender: T18 t00  destT19  destt-1  Tag: 02 CommId: 178474 Size: 20000
WARNING: : COMMUNIC_end: Task 20 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T21  t-1, Destination T20  t00  Tag: 03 CommId: 178317 Size: 20000
WARNING: : COMMUNIC_end: Task 20 ends with 1 message(s) in reception queue:
          -> Sender: T19 t00  destT20  destt-1  Tag: 02 CommId: 178332 Size: 20000
WARNING: : COMMUNIC_end: Task 21 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T22  t-1, Destination T21  t00  Tag: 03 CommId: 178239 Size: 20000
WARNING: : COMMUNIC_end: Task 21 ends with 1 message(s) in reception queue:
          -> Sender: T20 t00  destT21  destt-1  Tag: 02 CommId: 178249 Size: 20000
WARNING: : COMMUNIC_end: Task 22 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T23  t-1, Destination T22  t00  Tag: 03 CommId: 178116 Size: 20000
WARNING: : COMMUNIC_end: Task 22 ends with 1 message(s) in reception queue:
          -> Sender: T21 t00  destT22  destt-1  Tag: 02 CommId: 178132 Size: 20000
WARNING: : COMMUNIC_end: Task 23 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T24  t-1, Destination T23  t00  Tag: 03 CommId: 178045 Size: 20000
WARNING: : COMMUNIC_end: Task 23 ends with 1 message(s) in reception queue:
          -> Sender: T22 t00  destT23  destt-1  Tag: 02 CommId: 178066 Size: 20000
WARNING: : COMMUNIC_end: Task 24 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T25  t-1, Destination T24  t00  Tag: 03 CommId: 177986 Size: 20000
WARNING: : COMMUNIC_end: Task 24 ends with 1 message(s) in reception queue:
          -> Sender: T23 t00  destT24  destt-1  Tag: 02 CommId: 177972 Size: 20000
WARNING: : COMMUNIC_end: Task 25 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T26  t-1, Destination T25  t00  Tag: 03 CommId: 177935 Size: 20000
WARNING: : COMMUNIC_end: Task 25 ends with 1 message(s) in reception queue:
          -> Sender: T24 t00  destT25  destt-1  Tag: 02 CommId: 177892 Size: 20000
WARNING: : COMMUNIC_end: Task 26 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T27  t-1, Destination T26  t00  Tag: 03 CommId: 177805 Size: 20000
WARNING: : COMMUNIC_end: Task 26 ends with 1 message(s) in reception queue:
          -> Sender: T25 t00  destT26  destt-1  Tag: 02 CommId: 177831 Size: 20000
WARNING: : COMMUNIC_end: Task 27 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T28  t-1, Destination T27  t00  Tag: 03 CommId: 177755 Size: 20000
WARNING: : COMMUNIC_end: Task 27 ends with 1 message(s) in reception queue:
          -> Sender: T26 t00  destT27  destt-1  Tag: 02 CommId: 177791 Size: 20000
WARNING: : COMMUNIC_end: Task 28 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T29  t-1, Destination T28  t00  Tag: 03 CommId: 177695 Size: 20000
WARNING: : COMMUNIC_end: Task 28 ends with 1 message(s) in reception queue:
          -> Sender: T27 t00  destT28  destt-1  Tag: 02 CommId: 177651 Size: 20000
WARNING: : COMMUNIC_end: Task 29 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T30  t-1, Destination T29  t00  Tag: 03 CommId: 177625 Size: 20000
WARNING: : COMMUNIC_end: Task 29 ends with 1 message(s) in reception queue:
          -> Sender: T28 t00  destT29  destt-1  Tag: 02 CommId: 177611 Size: 20000
WARNING: : COMMUNIC_end: Task 30 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T31  t-1, Destination T30  t00  Tag: 03 CommId: 177534 Size: 20000
WARNING: : COMMUNIC_end: Task 30 ends with 1 message(s) in reception queue:
          -> Sender: T29 t00  destT30  destt-1  Tag: 02 CommId: 177555 Size: 20000
WARNING: : COMMUNIC_end: Task 31 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T32  t-1, Destination T31  t00  Tag: 03 CommId: 95662 Size: 20000
WARNING: : COMMUNIC_end: Task 31 ends with 1 message(s) in reception queue:
          -> Sender: T30 t00  destT31  destt-1  Tag: 02 CommId: 177481 Size: 20000
WARNING: : COMMUNIC_end: Task 32 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T33  t-1, Destination T32  t00  Tag: 03 CommId: 95585 Size: 20000
WARNING: : COMMUNIC_end: Task 32 ends with 1 message(s) in reception queue:
          -> Sender: T31 t00  destT32  destt-1  Tag: 02 CommId: 177391 Size: 20000
WARNING: : COMMUNIC_end: Task 33 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T34  t-1, Destination T33  t00  Tag: 03 CommId: 95427 Size: 20000
WARNING: : COMMUNIC_end: Task 33 ends with 1 message(s) in reception queue:
          -> Sender: T32 t00  destT33  destt-1  Tag: 02 CommId: 95491 Size: 20000
WARNING: : COMMUNIC_end: Task 34 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T35  t-1, Destination T34  t00  Tag: 03 CommId: 95337 Size: 20000
WARNING: : COMMUNIC_end: Task 34 ends with 1 message(s) in reception queue:
          -> Sender: T33 t00  destT34  destt-1  Tag: 02 CommId: 95412 Size: 20000
WARNING: : COMMUNIC_end: Task 35 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T36  t-1, Destination T35  t00  Tag: 03 CommId: 95221 Size: 20000
WARNING: : COMMUNIC_end: Task 35 ends with 1 message(s) in reception queue:
          -> Sender: T34 t00  destT35  destt-1  Tag: 02 CommId: 95262 Size: 20000
WARNING: : COMMUNIC_end: Task 36 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T37  t-1, Destination T36  t00  Tag: 03 CommId: 95141 Size: 20000
WARNING: : COMMUNIC_end: Task 36 ends with 1 message(s) in reception queue:
          -> Sender: T35 t00  destT36  destt-1  Tag: 02 CommId: 95181 Size: 20000
WARNING: : COMMUNIC_end: Task 37 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T38  t-1, Destination T37  t00  Tag: 03 CommId: 95083 Size: 20000
WARNING: : COMMUNIC_end: Task 37 ends with 1 message(s) in reception queue:
          -> Sender: T36 t00  destT37  destt-1  Tag: 02 CommId: 95050 Size: 20000
WARNING: : COMMUNIC_end: Task 38 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T39  t-1, Destination T38  t00  Tag: 03 CommId: 95015 Size: 20000
WARNING: : COMMUNIC_end: Task 38 ends with 1 message(s) in reception queue:
          -> Sender: T37 t00  destT38  destt-1  Tag: 02 CommId: 94971 Size: 20000
WARNING: : COMMUNIC_end: Task 39 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T40  t-1, Destination T39  t00  Tag: 03 CommId: 94945 Size: 20000
WARNING: : COMMUNIC_end: Task 39 ends with 1 message(s) in reception queue:
          -> Sender: T38 t00  destT39  destt-1  Tag: 02 CommId: 94910 Size: 20000
WARNING: : COMMUNIC_end: Task 40 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T41  t-1, Destination T40  t00  Tag: 03 CommId: 94884 Size: 20000
WARNING: : COMMUNIC_end: Task 40 ends with 1 message(s) in reception queue:
          -> Sender: T39 t00  destT40  destt-1  Tag: 02 CommId: 94850 Size: 20000
WARNING: : COMMUNIC_end: Task 41 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T42  t-1, Destination T41  t00  Tag: 03 CommId: 94833 Size: 20000
WARNING: : COMMUNIC_end: Task 41 ends with 1 message(s) in reception queue:
          -> Sender: T40 t00  destT41  destt-1  Tag: 02 CommId: 94780 Size: 20000
WARNING: : COMMUNIC_end: Task 42 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T43  t-1, Destination T42  t00  Tag: 03 CommId: 94764 Size: 20000
WARNING: : COMMUNIC_end: Task 42 ends with 1 message(s) in reception queue:
          -> Sender: T41 t00  destT42  destt-1  Tag: 02 CommId: 94710 Size: 20000
WARNING: : COMMUNIC_end: Task 43 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T44  t-1, Destination T43  t00  Tag: 03 CommId: 94694 Size: 20000
WARNING: : COMMUNIC_end: Task 43 ends with 1 message(s) in reception queue:
          -> Sender: T42 t00  destT43  destt-1  Tag: 02 CommId: 94669 Size: 20000
WARNING: : COMMUNIC_end: Task 44 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T45  t-1, Destination T44  t00  Tag: 03 CommId: 94644 Size: 20000
WARNING: : COMMUNIC_end: Task 44 ends with 1 message(s) in reception queue:
          -> Sender: T43 t00  destT44  destt-1  Tag: 02 CommId: 94600 Size: 20000
WARNING: : COMMUNIC_end: Task 45 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T46  t-1, Destination T45  t00  Tag: 03 CommId: 94574 Size: 20000
WARNING: : COMMUNIC_end: Task 45 ends with 1 message(s) in reception queue:
          -> Sender: T44 t00  destT45  destt-1  Tag: 02 CommId: 94539 Size: 20000
WARNING: : COMMUNIC_end: Task 46 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T47  t-1, Destination T46  t00  Tag: 03 CommId: 94523 Size: 20000
WARNING: : COMMUNIC_end: Task 46 ends with 1 message(s) in reception queue:
          -> Sender: T45 t00  destT46  destt-1  Tag: 02 CommId: 94490 Size: 20000
WARNING: : COMMUNIC_end: Task 47 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T48  t-1, Destination T47  t00  Tag: 03 CommId: 561429 Size: 20000
WARNING: : COMMUNIC_end: Task 47 ends with 1 message(s) in reception queue:
          -> Sender: T46 t00  destT47  destt-1  Tag: 02 CommId: 94429 Size: 20000
WARNING: : COMMUNIC_end: Task 48 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T49  t-1, Destination T48  t00  Tag: 03 CommId: 561348 Size: 20000
WARNING: : COMMUNIC_end: Task 48 ends with 1 message(s) in reception queue:
          -> Sender: T47 t00  destT48  destt-1  Tag: 02 CommId: 94379 Size: 20000
WARNING: : COMMUNIC_end: Task 49 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T50  t-1, Destination T49  t00  Tag: 03 CommId: 561277 Size: 20000
WARNING: : COMMUNIC_end: Task 49 ends with 1 message(s) in reception queue:
          -> Sender: T48 t00  destT49  destt-1  Tag: 02 CommId: 561210 Size: 20000
WARNING: : COMMUNIC_end: Task 50 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T51  t-1, Destination T50  t00  Tag: 03 CommId: 561127 Size: 20000
WARNING: : COMMUNIC_end: Task 50 ends with 1 message(s) in reception queue:
          -> Sender: T49 t00  destT50  destt-1  Tag: 02 CommId: 561143 Size: 20000
WARNING: : COMMUNIC_end: Task 51 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T52  t-1, Destination T51  t00  Tag: 03 CommId: 561047 Size: 20000
WARNING: : COMMUNIC_end: Task 51 ends with 1 message(s) in reception queue:
          -> Sender: T50 t00  destT51  destt-1  Tag: 02 CommId: 561063 Size: 20000
WARNING: : COMMUNIC_end: Task 52 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T53  t-1, Destination T52  t00  Tag: 03 CommId: 560907 Size: 20000
WARNING: : COMMUNIC_end: Task 52 ends with 1 message(s) in reception queue:
          -> Sender: T51 t00  destT52  destt-1  Tag: 02 CommId: 560923 Size: 20000
WARNING: : COMMUNIC_end: Task 53 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T54  t-1, Destination T53  t00  Tag: 03 CommId: 560836 Size: 20000
WARNING: : COMMUNIC_end: Task 53 ends with 1 message(s) in reception queue:
          -> Sender: T52 t00  destT53  destt-1  Tag: 02 CommId: 560852 Size: 20000
WARNING: : COMMUNIC_end: Task 54 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T55  t-1, Destination T54  t00  Tag: 03 CommId: 560786 Size: 20000
WARNING: : COMMUNIC_end: Task 54 ends with 1 message(s) in reception queue:
          -> Sender: T53 t00  destT54  destt-1  Tag: 02 CommId: 560722 Size: 20000
WARNING: : COMMUNIC_end: Task 55 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T56  t-1, Destination T55  t00  Tag: 03 CommId: 560646 Size: 20000
WARNING: : COMMUNIC_end: Task 55 ends with 1 message(s) in reception queue:
          -> Sender: T54 t00  destT55  destt-1  Tag: 02 CommId: 560662 Size: 20000
WARNING: : COMMUNIC_end: Task 56 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T57  t-1, Destination T56  t00  Tag: 03 CommId: 560585 Size: 20000
WARNING: : COMMUNIC_end: Task 56 ends with 1 message(s) in reception queue:
          -> Sender: T55 t00  destT56  destt-1  Tag: 02 CommId: 560601 Size: 20000
WARNING: : COMMUNIC_end: Task 57 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T58  t-1, Destination T57  t00  Tag: 03 CommId: 560536 Size: 20000
WARNING: : COMMUNIC_end: Task 57 ends with 1 message(s) in reception queue:
          -> Sender: T56 t00  destT57  destt-1  Tag: 02 CommId: 560481 Size: 20000
WARNING: : COMMUNIC_end: Task 58 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T59  t-1, Destination T58  t00  Tag: 03 CommId: 560465 Size: 20000
WARNING: : COMMUNIC_end: Task 58 ends with 1 message(s) in reception queue:
          -> Sender: T57 t00  destT58  destt-1  Tag: 02 CommId: 560411 Size: 20000
WARNING: : COMMUNIC_end: Task 59 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T60  t-1, Destination T59  t00  Tag: 03 CommId: 560375 Size: 20000
WARNING: : COMMUNIC_end: Task 59 ends with 1 message(s) in reception queue:
          -> Sender: T58 t00  destT59  destt-1  Tag: 02 CommId: 560371 Size: 20000
WARNING: : COMMUNIC_end: Task 60 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T61  t-1, Destination T60  t00  Tag: 03 CommId: 560305 Size: 20000
WARNING: : COMMUNIC_end: Task 60 ends with 1 message(s) in reception queue:
          -> Sender: T59 t00  destT60  destt-1  Tag: 02 CommId: 560302 Size: 20000
WARNING: : COMMUNIC_end: Task 61 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T62  t-1, Destination T61  t00  Tag: 03 CommId: 560244 Size: 20000
WARNING: : COMMUNIC_end: Task 61 ends with 1 message(s) in reception queue:
          -> Sender: T60 t00  destT61  destt-1  Tag: 02 CommId: 560211 Size: 20000
WARNING: : COMMUNIC_end: Task 62 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T63  t-1, Destination T62  t00  Tag: 03 CommId: 560195 Size: 20000
WARNING: : COMMUNIC_end: Task 62 ends with 1 message(s) in reception queue:
          -> Sender: T61 t00  destT62  destt-1  Tag: 02 CommId: 560151 Size: 20000
WARNING: : COMMUNIC_end: Task 63 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T64  t-1, Destination T63  t00  Tag: 03 CommId: 9967 Size: 20000
WARNING: : COMMUNIC_end: Task 63 ends with 1 message(s) in reception queue:
          -> Sender: T62 t00  destT63  destt-1  Tag: 02 CommId: 560100 Size: 20000
WARNING: : COMMUNIC_end: Task 64 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T65  t-1, Destination T64  t00  Tag: 03 CommId: 9887 Size: 20000
WARNING: : COMMUNIC_end: Task 64 ends with 1 message(s) in reception queue:
          -> Sender: T63 t00  destT64  destt-1  Tag: 02 CommId: 560051 Size: 20000
WARNING: : COMMUNIC_end: Task 65 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T66  t-1, Destination T65  t00  Tag: 03 CommId: 9816 Size: 20000
WARNING: : COMMUNIC_end: Task 65 ends with 1 message(s) in reception queue:
          -> Sender: T64 t00  destT65  destt-1  Tag: 02 CommId: 9762 Size: 20000
WARNING: : COMMUNIC_end: Task 66 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T67  t-1, Destination T66  t00  Tag: 03 CommId: 9737 Size: 20000
WARNING: : COMMUNIC_end: Task 66 ends with 1 message(s) in reception queue:
          -> Sender: T65 t00  destT66  destt-1  Tag: 02 CommId: 9692 Size: 20000
WARNING: : COMMUNIC_end: Task 67 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T68  t-1, Destination T67  t00  Tag: 03 CommId: 9666 Size: 20000
WARNING: : COMMUNIC_end: Task 67 ends with 1 message(s) in reception queue:
          -> Sender: T66 t00  destT67  destt-1  Tag: 02 CommId: 9616 Size: 20000
WARNING: : COMMUNIC_end: Task 68 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T69  t-1, Destination T68  t00  Tag: 03 CommId: 9526 Size: 20000
WARNING: : COMMUNIC_end: Task 68 ends with 1 message(s) in reception queue:
          -> Sender: T67 t00  destT68  destt-1  Tag: 02 CommId: 9547 Size: 20000
WARNING: : COMMUNIC_end: Task 69 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T70  t-1, Destination T69  t00  Tag: 03 CommId: 9446 Size: 20000
WARNING: : COMMUNIC_end: Task 69 ends with 1 message(s) in reception queue:
          -> Sender: T68 t00  destT69  destt-1  Tag: 02 CommId: 9481 Size: 20000
WARNING: : COMMUNIC_end: Task 70 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T71  t-1, Destination T70  t00  Tag: 03 CommId: 9325 Size: 20000
WARNING: : COMMUNIC_end: Task 70 ends with 1 message(s) in reception queue:
          -> Sender: T69 t00  destT70  destt-1  Tag: 02 CommId: 9351 Size: 20000
WARNING: : COMMUNIC_end: Task 71 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T72  t-1, Destination T71  t00  Tag: 03 CommId: 9255 Size: 20000
WARNING: : COMMUNIC_end: Task 71 ends with 1 message(s) in reception queue:
          -> Sender: T70 t00  destT71  destt-1  Tag: 02 CommId: 9291 Size: 20000
WARNING: : COMMUNIC_end: Task 72 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T73  t-1, Destination T72  t00  Tag: 03 CommId: 9205 Size: 20000
WARNING: : COMMUNIC_end: Task 72 ends with 1 message(s) in reception queue:
          -> Sender: T71 t00  destT72  destt-1  Tag: 02 CommId: 9180 Size: 20000
WARNING: : COMMUNIC_end: Task 73 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T74  t-1, Destination T73  t00  Tag: 03 CommId: 9144 Size: 20000
WARNING: : COMMUNIC_end: Task 73 ends with 1 message(s) in reception queue:
          -> Sender: T72 t00  destT73  destt-1  Tag: 02 CommId: 9101 Size: 20000
WARNING: : COMMUNIC_end: Task 74 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T75  t-1, Destination T74  t00  Tag: 03 CommId: 9024 Size: 20000
WARNING: : COMMUNIC_end: Task 74 ends with 1 message(s) in reception queue:
          -> Sender: T73 t00  destT74  destt-1  Tag: 02 CommId: 9050 Size: 20000
WARNING: : COMMUNIC_end: Task 75 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T76  t-1, Destination T75  t00  Tag: 03 CommId: 8964 Size: 20000
WARNING: : COMMUNIC_end: Task 75 ends with 1 message(s) in reception queue:
          -> Sender: T74 t00  destT75  destt-1  Tag: 02 CommId: 8990 Size: 20000
WARNING: : COMMUNIC_end: Task 76 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T77  t-1, Destination T76  t00  Tag: 03 CommId: 8914 Size: 20000
WARNING: : COMMUNIC_end: Task 76 ends with 1 message(s) in reception queue:
          -> Sender: T75 t00  destT76  destt-1  Tag: 02 CommId: 8870 Size: 20000
WARNING: : COMMUNIC_end: Task 77 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T78  t-1, Destination T77  t00  Tag: 03 CommId: 8844 Size: 20000
WARNING: : COMMUNIC_end: Task 77 ends with 1 message(s) in reception queue:
          -> Sender: T76 t00  destT77  destt-1  Tag: 02 CommId: 8820 Size: 20000
WARNING: : COMMUNIC_end: Task 78 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T79  t-1, Destination T78  t00  Tag: 03 CommId: 8754 Size: 20000
WARNING: : COMMUNIC_end: Task 78 ends with 1 message(s) in reception queue:
          -> Sender: T77 t00  destT78  destt-1  Tag: 02 CommId: 8770 Size: 20000
WARNING: : COMMUNIC_end: Task 79 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T80  t-1, Destination T79  t00  Tag: 03 CommId: 475618 Size: 20000
WARNING: : COMMUNIC_end: Task 79 ends with 1 message(s) in reception queue:
          -> Sender: T78 t00  destT79  destt-1  Tag: 02 CommId: 8700 Size: 20000
WARNING: : COMMUNIC_end: Task 80 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T81  t-1, Destination T80  t00  Tag: 03 CommId: 475535 Size: 20000
WARNING: : COMMUNIC_end: Task 80 ends with 1 message(s) in reception queue:
          -> Sender: T79 t00  destT80  destt-1  Tag: 02 CommId: 8600 Size: 20000
WARNING: : COMMUNIC_end: Task 81 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T82  t-1, Destination T81  t00  Tag: 03 CommId: 475464 Size: 20000
WARNING: : COMMUNIC_end: Task 81 ends with 1 message(s) in reception queue:
          -> Sender: T80 t00  destT81  destt-1  Tag: 02 CommId: 475417 Size: 20000
WARNING: : COMMUNIC_end: Task 82 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T83  t-1, Destination T82  t00  Tag: 03 CommId: 475382 Size: 20000
WARNING: : COMMUNIC_end: Task 82 ends with 1 message(s) in reception queue:
          -> Sender: T81 t00  destT82  destt-1  Tag: 02 CommId: 475347 Size: 20000
WARNING: : COMMUNIC_end: Task 83 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T84  t-1, Destination T83  t00  Tag: 03 CommId: 475238 Size: 20000
WARNING: : COMMUNIC_end: Task 83 ends with 1 message(s) in reception queue:
          -> Sender: T82 t00  destT83  destt-1  Tag: 02 CommId: 475274 Size: 20000
WARNING: : COMMUNIC_end: Task 84 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T85  t-1, Destination T84  t00  Tag: 03 CommId: 475155 Size: 20000
WARNING: : COMMUNIC_end: Task 84 ends with 1 message(s) in reception queue:
          -> Sender: T83 t00  destT84  destt-1  Tag: 02 CommId: 475191 Size: 20000
WARNING: : COMMUNIC_end: Task 85 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T86  t-1, Destination T85  t00  Tag: 03 CommId: 475095 Size: 20000
WARNING: : COMMUNIC_end: Task 85 ends with 1 message(s) in reception queue:
          -> Sender: T84 t00  destT85  destt-1  Tag: 02 CommId: 475059 Size: 20000
WARNING: : COMMUNIC_end: Task 86 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T87  t-1, Destination T86  t00  Tag: 03 CommId: 475021 Size: 20000
WARNING: : COMMUNIC_end: Task 86 ends with 1 message(s) in reception queue:
          -> Sender: T85 t00  destT86  destt-1  Tag: 02 CommId: 474991 Size: 20000
WARNING: : COMMUNIC_end: Task 87 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T88  t-1, Destination T87  t00  Tag: 03 CommId: 474898 Size: 20000
WARNING: : COMMUNIC_end: Task 87 ends with 1 message(s) in reception queue:
          -> Sender: T86 t00  destT87  destt-1  Tag: 02 CommId: 474925 Size: 20000
WARNING: : COMMUNIC_end: Task 88 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T89  t-1, Destination T88  t00  Tag: 03 CommId: 474828 Size: 20000
WARNING: : COMMUNIC_end: Task 88 ends with 1 message(s) in reception queue:
          -> Sender: T87 t00  destT88  destt-1  Tag: 02 CommId: 474874 Size: 20000
WARNING: : COMMUNIC_end: Task 89 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T90  t-1, Destination T89  t00  Tag: 03 CommId: 474705 Size: 20000
WARNING: : COMMUNIC_end: Task 89 ends with 1 message(s) in reception queue:
          -> Sender: T88 t00  destT89  destt-1  Tag: 02 CommId: 474741 Size: 20000
WARNING: : COMMUNIC_end: Task 90 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T91  t-1, Destination T90  t00  Tag: 03 CommId: 474641 Size: 20000
WARNING: : COMMUNIC_end: Task 90 ends with 1 message(s) in reception queue:
          -> Sender: T89 t00  destT90  destt-1  Tag: 02 CommId: 474678 Size: 20000
WARNING: : COMMUNIC_end: Task 91 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T92  t-1, Destination T91  t00  Tag: 03 CommId: 474598 Size: 20000
WARNING: : COMMUNIC_end: Task 91 ends with 1 message(s) in reception queue:
          -> Sender: T90 t00  destT91  destt-1  Tag: 02 CommId: 474555 Size: 20000
WARNING: : COMMUNIC_end: Task 92 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T93  t-1, Destination T92  t00  Tag: 03 CommId: 474528 Size: 20000
WARNING: : COMMUNIC_end: Task 92 ends with 1 message(s) in reception queue:
          -> Sender: T91 t00  destT92  destt-1  Tag: 02 CommId: 474494 Size: 20000
WARNING: : COMMUNIC_end: Task 93 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T94  t-1, Destination T93  t00  Tag: 03 CommId: 474475 Size: 20000
WARNING: : COMMUNIC_end: Task 93 ends with 1 message(s) in reception queue:
          -> Sender: T92 t00  destT93  destt-1  Tag: 02 CommId: 474441 Size: 20000
WARNING: : COMMUNIC_end: Task 94 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T95  t-1, Destination T94  t00  Tag: 03 CommId: 474415 Size: 20000
WARNING: : COMMUNIC_end: Task 94 ends with 1 message(s) in reception queue:
          -> Sender: T93 t00  destT94  destt-1  Tag: 02 CommId: 474378 Size: 20000
WARNING: : COMMUNIC_end: Task 95 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T96  t-1, Destination T95  t00  Tag: 03 CommId: 392628 Size: 20000
WARNING: : COMMUNIC_end: Task 95 ends with 1 message(s) in reception queue:
          -> Sender: T94 t00  destT95  destt-1  Tag: 02 CommId: 474328 Size: 20000
WARNING: : COMMUNIC_end: Task 96 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T97  t-1, Destination T96  t00  Tag: 03 CommId: 392548 Size: 20000
WARNING: : COMMUNIC_end: Task 96 ends with 1 message(s) in reception queue:
          -> Sender: T95 t00  destT96  destt-1  Tag: 02 CommId: 474264 Size: 20000
WARNING: : COMMUNIC_end: Task 97 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T98  t-1, Destination T97  t00  Tag: 03 CommId: 392398 Size: 20000
WARNING: : COMMUNIC_end: Task 97 ends with 1 message(s) in reception queue:
          -> Sender: T96 t00  destT97  destt-1  Tag: 02 CommId: 392473 Size: 20000
WARNING: : COMMUNIC_end: Task 98 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T99  t-1, Destination T98  t00  Tag: 03 CommId: 392317 Size: 20000
WARNING: : COMMUNIC_end: Task 98 ends with 1 message(s) in reception queue:
          -> Sender: T97 t00  destT98  destt-1  Tag: 02 CommId: 392394 Size: 20000
WARNING: : COMMUNIC_end: Task 99 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T100  t-1, Destination T99  t00  Tag: 03 CommId: 392177 Size: 20000
WARNING: : COMMUNIC_end: Task 99 ends with 1 message(s) in reception queue:
          -> Sender: T98 t00  destT99  destt-1  Tag: 02 CommId: 392244 Size: 20000
WARNING: : COMMUNIC_end: Task 100 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T101  t-1, Destination T100  t00  Tag: 03 CommId: 392097 Size: 20000
WARNING: : COMMUNIC_end: Task 100 ends with 1 message(s) in reception queue:
          -> Sender: T99 t00  destT100  destt-1  Tag: 02 CommId: 392163 Size: 20000
WARNING: : COMMUNIC_end: Task 101 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T102  t-1, Destination T101  t00  Tag: 03 CommId: 392046 Size: 20000
WARNING: : COMMUNIC_end: Task 101 ends with 1 message(s) in reception queue:
          -> Sender: T100 t00  destT101  destt-1  Tag: 02 CommId: 392032 Size: 20000
WARNING: : COMMUNIC_end: Task 102 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T103  t-1, Destination T102  t00  Tag: 03 CommId: 391976 Size: 20000
WARNING: : COMMUNIC_end: Task 102 ends with 1 message(s) in reception queue:
          -> Sender: T101 t00  destT102  destt-1  Tag: 02 CommId: 391953 Size: 20000
WARNING: : COMMUNIC_end: Task 103 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T104  t-1, Destination T103  t00  Tag: 03 CommId: 391907 Size: 20000
WARNING: : COMMUNIC_end: Task 103 ends with 1 message(s) in reception queue:
          -> Sender: T102 t00  destT103  destt-1  Tag: 02 CommId: 391892 Size: 20000
WARNING: : COMMUNIC_end: Task 104 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T105  t-1, Destination T104  t00  Tag: 03 CommId: 391795 Size: 20000
WARNING: : COMMUNIC_end: Task 104 ends with 1 message(s) in reception queue:
          -> Sender: T103 t00  destT104  destt-1  Tag: 02 CommId: 391832 Size: 20000
WARNING: : COMMUNIC_end: Task 105 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T106  t-1, Destination T105  t00  Tag: 03 CommId: 391716 Size: 20000
WARNING: : COMMUNIC_end: Task 105 ends with 1 message(s) in reception queue:
          -> Sender: T104 t00  destT105  destt-1  Tag: 02 CommId: 391762 Size: 20000
WARNING: : COMMUNIC_end: Task 106 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T107  t-1, Destination T106  t00  Tag: 03 CommId: 391665 Size: 20000
WARNING: : COMMUNIC_end: Task 106 ends with 1 message(s) in reception queue:
          -> Sender: T105 t00  destT106  destt-1  Tag: 02 CommId: 391651 Size: 20000
WARNING: : COMMUNIC_end: Task 107 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T108  t-1, Destination T107  t00  Tag: 03 CommId: 391615 Size: 20000
WARNING: : COMMUNIC_end: Task 107 ends with 1 message(s) in reception queue:
          -> Sender: T106 t00  destT107  destt-1  Tag: 02 CommId: 391572 Size: 20000
WARNING: : COMMUNIC_end: Task 108 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T109  t-1, Destination T108  t00  Tag: 03 CommId: 391495 Size: 20000
WARNING: : COMMUNIC_end: Task 108 ends with 1 message(s) in reception queue:
          -> Sender: T107 t00  destT108  destt-1  Tag: 02 CommId: 391521 Size: 20000
WARNING: : COMMUNIC_end: Task 109 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T110  t-1, Destination T109  t00  Tag: 03 CommId: 391445 Size: 20000
WARNING: : COMMUNIC_end: Task 109 ends with 1 message(s) in reception queue:
          -> Sender: T108 t00  destT109  destt-1  Tag: 02 CommId: 391471 Size: 20000
WARNING: : COMMUNIC_end: Task 110 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T111  t-1, Destination T110  t00  Tag: 03 CommId: 391385 Size: 20000
WARNING: : COMMUNIC_end: Task 110 ends with 1 message(s) in reception queue:
          -> Sender: T109 t00  destT110  destt-1  Tag: 02 CommId: 391341 Size: 20000
WARNING: : COMMUNIC_end: Task 111 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T112  t-1, Destination T111  t00  Tag: 03 CommId: 313958 Size: 20000
WARNING: : COMMUNIC_end: Task 111 ends with 1 message(s) in reception queue:
          -> Sender: T110 t00  destT111  destt-1  Tag: 02 CommId: 391310 Size: 20000
WARNING: : COMMUNIC_end: Task 112 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T113  t-1, Destination T112  t00  Tag: 03 CommId: 313865 Size: 20000
WARNING: : COMMUNIC_end: Task 112 ends with 1 message(s) in reception queue:
          -> Sender: T111 t00  destT112  destt-1  Tag: 02 CommId: 391251 Size: 20000
WARNING: : COMMUNIC_end: Task 113 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T114  t-1, Destination T113  t00  Tag: 03 CommId: 313806 Size: 20000
WARNING: : COMMUNIC_end: Task 113 ends with 1 message(s) in reception queue:
          -> Sender: T112 t00  destT113  destt-1  Tag: 02 CommId: 313800 Size: 20000
WARNING: : COMMUNIC_end: Task 114 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T115  t-1, Destination T114  t00  Tag: 03 CommId: 313714 Size: 20000
WARNING: : COMMUNIC_end: Task 114 ends with 1 message(s) in reception queue:
          -> Sender: T113 t00  destT114  destt-1  Tag: 02 CommId: 313708 Size: 20000
WARNING: : COMMUNIC_end: Task 115 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T116  t-1, Destination T115  t00  Tag: 03 CommId: 313663 Size: 20000
WARNING: : COMMUNIC_end: Task 115 ends with 1 message(s) in reception queue:
          -> Sender: T114 t00  destT115  destt-1  Tag: 02 CommId: 313659 Size: 20000
WARNING: : COMMUNIC_end: Task 116 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T117  t-1, Destination T116  t00  Tag: 03 CommId: 313580 Size: 20000
WARNING: : COMMUNIC_end: Task 116 ends with 1 message(s) in reception queue:
          -> Sender: T115 t00  destT116  destt-1  Tag: 02 CommId: 313567 Size: 20000
WARNING: : COMMUNIC_end: Task 117 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T118  t-1, Destination T117  t00  Tag: 03 CommId: 313476 Size: 20000
WARNING: : COMMUNIC_end: Task 117 ends with 1 message(s) in reception queue:
          -> Sender: T116 t00  destT117  destt-1  Tag: 02 CommId: 313518 Size: 20000
WARNING: : COMMUNIC_end: Task 118 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T119  t-1, Destination T118  t00  Tag: 03 CommId: 313360 Size: 20000
WARNING: : COMMUNIC_end: Task 118 ends with 1 message(s) in reception queue:
          -> Sender: T117 t00  destT118  destt-1  Tag: 02 CommId: 313435 Size: 20000
WARNING: : COMMUNIC_end: Task 119 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T120  t-1, Destination T119  t00  Tag: 03 CommId: 313249 Size: 20000
WARNING: : COMMUNIC_end: Task 119 ends with 1 message(s) in reception queue:
          -> Sender: T118 t00  destT119  destt-1  Tag: 02 CommId: 313314 Size: 20000
WARNING: : COMMUNIC_end: Task 120 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T121  t-1, Destination T120  t00  Tag: 03 CommId: 313182 Size: 20000
WARNING: : COMMUNIC_end: Task 120 ends with 1 message(s) in reception queue:
          -> Sender: T119 t00  destT120  destt-1  Tag: 02 CommId: 313202 Size: 20000
WARNING: : COMMUNIC_end: Task 121 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T122  t-1, Destination T121  t00  Tag: 03 CommId: 313123 Size: 20000
WARNING: : COMMUNIC_end: Task 121 ends with 1 message(s) in reception queue:
          -> Sender: T120 t00  destT121  destt-1  Tag: 02 CommId: 313081 Size: 20000
WARNING: : COMMUNIC_end: Task 122 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T123  t-1, Destination T122  t00  Tag: 03 CommId: 313056 Size: 20000
WARNING: : COMMUNIC_end: Task 122 ends with 1 message(s) in reception queue:
          -> Sender: T121 t00  destT122  destt-1  Tag: 02 CommId: 313022 Size: 20000
WARNING: : COMMUNIC_end: Task 123 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T124  t-1, Destination T123  t00  Tag: 03 CommId: 312993 Size: 20000
WARNING: : COMMUNIC_end: Task 123 ends with 1 message(s) in reception queue:
          -> Sender: T122 t00  destT123  destt-1  Tag: 02 CommId: 312970 Size: 20000
WARNING: : COMMUNIC_end: Task 124 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T125  t-1, Destination T124  t00  Tag: 03 CommId: 312884 Size: 20000
WARNING: : COMMUNIC_end: Task 124 ends with 1 message(s) in reception queue:
          -> Sender: T123 t00  destT124  destt-1  Tag: 02 CommId: 312911 Size: 20000
WARNING: : COMMUNIC_end: Task 125 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T126  t-1, Destination T125  t00  Tag: 03 CommId: 312823 Size: 20000
WARNING: : COMMUNIC_end: Task 125 ends with 1 message(s) in reception queue:
          -> Sender: T124 t00  destT125  destt-1  Tag: 02 CommId: 312849 Size: 20000
WARNING: : COMMUNIC_end: Task 126 ends with 1 threads waiting to recv a message:
          * Thread 00 <-          * Sender: T127  t-1, Destination T126  t00  Tag: 03 CommId: 312778 Size: 20000
WARNING: : COMMUNIC_end: Task 126 ends with 1 message(s) in reception queue:
          -> Sender: T125 t00  destT126  destt-1  Tag: 02 CommId: 312746 Size: 20000
WARNING: : COMMUNIC_end: Task 127 ends with 6 message(s) in reception queue:
          -> Sender: T00 t00  destT127  destt-1  Tag: 01 CommId: 253549 Size: 4
          -> Sender: T00 t00  destT127  destt-1  Tag: 01 CommId: 253552 Size: 4
          -> Sender: T00 t00  destT127  destt-1  Tag: 01 CommId: 253555 Size: 4
          -> Sender: T00 t00  destT127  destt-1  Tag: 01 CommId: 253558 Size: 4
          -> Sender: T00 t00  destT127  destt-1  Tag: 01 CommId: 253561 Size: 4
          -> Sender: T126 t00  destT127  destt-1  Tag: 02 CommId: 312696 Size: 20000
  **** Application 0 (2016-05-14-2D_Heat_Transfer_5k_5k_500.dim) ****

  **** Total Statistics ****

Execution time:	1610.000000000
Speedup:	94.25 
CPU Time:	151740.239082560

 Id.	Computation	%time	Communication
  0	1329.708089867	45.23	0.085248000
  1	1329.711359601	45.23	0.259644982
  2	1329.709756242	45.23	0.261610180
  3	1329.709755307	45.23	0.261976045
  4	1329.709758303	45.23	0.262337981
  5	1329.709757251	45.23	0.262703958
  6	1329.709758724	45.23	0.263067409
  7	1329.709757371	45.23	0.263433687
  8	1329.709759673	45.23	0.263412319
  9	1329.709757755	45.23	0.264162925
 10	1329.709765926	45.23	0.264519650
 11	1329.709767552	45.23	0.264409097
 12	0.000000000	0.00	0.000000000
 13	1329.708455305	45.23	274.000507760
 14	1329.708453064	45.23	274.000615560
 15	1329.708871156	45.23	274.000047027
 16	1329.702064489	45.23	274.006699964
 17	1329.701662418	45.23	274.006948332
 18	1329.702050686	45.23	274.006409611
 19	1329.701656138	45.23	274.006650330
 20	1329.702039651	45.23	274.006116373
 21	1329.701648000	45.23	274.006354419
 22	1329.702026983	45.23	274.005824993
 23	1329.701642041	45.23	274.006056371
 24	1329.701616203	45.23	274.005931765
 25	1329.701640031	45.23	274.005757497
 26	1329.701992208	45.23	274.005254878
 27	1329.701632078	45.23	274.005461297
 28	1329.701606540	45.23	274.005336393
 29	1329.701626205	45.23	274.005166264
 30	1329.701968740	45.23	274.004673289
 31	1329.701620224	45.23	274.004868407
 32	1329.691971768	45.23	274.014366420
 33	1329.692337483	45.23	274.013850266
 34	1329.691968251	45.23	274.014065817
 35	1329.692319474	45.23	274.013564149
 36	1329.691960612	45.23	274.013769313
 37	1329.691961753	45.23	274.013617731
 38	1329.691956337	45.23	274.013472703
 39	1329.691953129	45.23	274.013325469
 40	1329.691952705	45.23	274.013175457
 41	1329.691948802	45.23	274.013028907
 42	1329.691943765	45.23	274.012883478
 43	1329.691940667	45.23	274.012736133
 44	1329.691936011	45.23	274.012590339
 45	1329.691933632	45.23	274.012442276
 46	1329.691929027	45.23	274.012296438
 47	1329.691930823	45.23	274.012144203
 48	1603.638157367	49.90	0.065767212
 49	1603.638160777	49.90	0.065612765
 50	1603.638528338	49.90	0.065094072
 51	1603.638149424	49.90	0.065318112
 52	1603.638514868	49.90	0.064801637
 53	1603.638142684	49.90	0.065018924
 54	1603.638136168	49.90	0.064874406
 55	1603.638469329	49.90	0.064390212
 56	1603.638130048	49.90	0.064574627
 57	1603.638128377	49.90	0.064425250
 58	1603.638121537	49.90	0.064281055
 59	1603.638444251	49.90	0.063807305
 60	1603.638114819	49.90	0.063982258
 61	1603.638111945	49.90	0.063834095
 62	1603.638107822	49.90	0.063687184
 63	1603.638106194	49.90	0.063537773
 64	46.453985196	2.80	1557.247507735
 65	46.454011055	2.80	1557.247330542
 66	46.453976710	2.80	1557.247213555
 67	46.454003549	2.80	1557.247035387
 68	46.454307182	2.80	1557.246580426
 69	46.453995928	2.80	1557.246736171
 70	46.454289457	2.80	1557.246291308
 71	46.453991725	2.80	1557.246433543
 72	46.453954788	2.80	1557.246319145
 73	46.453980584	2.80	1557.246142014
 74	46.454247414	2.80	1557.245723852
 75	46.453970547	2.80	1557.245845226
 76	46.453939448	2.80	1557.245724996
 77	46.453962981	2.80	1557.245550127
 78	46.454213420	2.80	1557.245148355
 79	46.453960901	2.80	1557.245245818
 80	1337.088547096	45.37	266.610508290
 81	1337.088535690	45.37	266.610369338
 82	1337.088534867	45.37	266.610219799
 83	1337.088739286	45.37	266.609865024
 84	1337.088534836	45.37	266.609915967
 85	1337.088525666	45.37	266.609774760
 86	1337.088523075	45.37	266.609626991
 87	1337.088715965	45.37	266.609283743
 88	1337.088518029	45.37	266.609328486
 89	1337.088701824	45.37	266.608994332
 90	1337.088512860	45.37	266.609029784
 91	1337.088508527	45.37	266.608883759
 92	1337.088506256	45.37	266.608735671
 93	1337.088502481	45.37	266.608589090
 94	1337.088499567	45.37	266.608441652
 95	1337.088667112	45.37	266.608123756
 96	1337.084967316	45.37	266.611670066
 97	1337.085120074	45.37	266.611366949
 98	1337.084961159	45.37	266.611372360
 99	1337.085108977	45.37	266.611074182
100	1337.084954432	45.37	266.611075216
101	1337.084944972	45.37	266.610934316
102	1337.084947835	45.37	266.610781094
103	1337.084938742	45.37	266.610639827
104	1337.085074095	45.37	266.610354118
105	1337.084932994	45.37	266.610341660
106	1337.084937750	45.37	266.610186540
107	1337.084926680	45.37	266.610047224
108	1337.085049658	45.37	266.609773888
109	1337.084921396	45.37	266.609748946
110	1337.084924605	45.37	266.609595377
111	1337.084915467	45.37	266.609454160
112	1337.069517322	45.37	266.624701943
113	1337.069510147	45.37	266.624558758
114	1337.069512840	45.37	266.624405705
115	1337.069504352	45.37	266.624263832
116	1337.069506112	45.37	266.624111718
117	1337.069592823	45.37	266.623874646
118	1337.069594310	45.37	266.623719651
119	1337.069581788	45.37	266.623578596
120	1337.069495329	45.37	266.623511739
121	1337.069510093	45.37	266.623346615
122	1337.069488532	45.37	266.623217815
123	1337.069481425	45.37	266.623074565
124	1337.069558715	45.37	266.622846915
125	1337.069477178	45.37	266.622775089
126	1337.069477501	45.37	266.622624392
127	0.000000000	0.00	0.000000000

 Id.	Mess.sent	Bytes sent	Immediate recv	Waiting recv	Bytes recv	Coll.op.	Block time	Comm. time	Wait link time	Wait buses time	I/O time
  0	7.560000e+02	9.920252e+07	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  1	5.030000e+02	1.080001e+07	5.000000e+00	5.010000e+02	1.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  2	1.003000e+03	2.080001e+07	6.500000e+02	3.560000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  3	1.003000e+03	2.080001e+07	7.970000e+02	2.090000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  4	1.003000e+03	2.080001e+07	7.230000e+02	2.830000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  5	1.003000e+03	2.080001e+07	8.000000e+02	2.060000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  6	1.003000e+03	2.080001e+07	8.280000e+02	1.780000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  7	1.003000e+03	2.080001e+07	7.580000e+02	2.480000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  8	1.003000e+03	2.080001e+07	9.560000e+02	5.000000e+01	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
  9	1.003000e+03	2.080001e+07	8.520000e+02	1.540000e+02	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 10	1.003000e+03	2.080001e+07	1.003000e+03	3.000000e+00	2.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 11	5.030000e+02	1.080001e+07	5.000000e+00	5.010000e+02	1.080002e+07	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 12	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 13	1.140000e+02	2.280000e+06	4.000000e+00	1.160000e+02	3.060020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 14	2.260000e+02	4.520000e+06	1.170000e+02	1.150000e+02	5.300020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 15	2.240000e+02	4.480000e+06	1.190000e+02	1.110000e+02	5.260020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 16	2.220000e+02	4.440000e+06	1.150000e+02	1.130000e+02	5.220020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 17	2.200000e+02	4.400000e+06	1.140000e+02	1.120000e+02	5.180020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 18	2.180000e+02	4.360000e+06	1.150000e+02	1.090000e+02	5.140020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 19	2.160000e+02	4.320000e+06	1.120000e+02	1.100000e+02	5.100020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 20	2.140000e+02	4.280000e+06	1.130000e+02	1.070000e+02	5.060020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 21	2.120000e+02	4.240000e+06	1.100000e+02	1.080000e+02	5.020020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 22	2.100000e+02	4.200000e+06	1.110000e+02	1.050000e+02	4.980020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 23	2.080000e+02	4.160000e+06	1.090000e+02	1.050000e+02	4.940020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 24	2.060000e+02	4.120000e+06	1.060000e+02	1.060000e+02	4.900020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 25	2.040000e+02	4.080000e+06	1.060000e+02	1.040000e+02	4.860020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 26	2.020000e+02	4.040000e+06	1.060000e+02	1.020000e+02	4.820020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 27	2.000000e+02	4.000000e+06	1.040000e+02	1.020000e+02	4.780020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 28	1.980000e+02	3.960000e+06	1.030000e+02	1.010000e+02	4.740020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 29	1.960000e+02	3.920000e+06	1.020000e+02	1.000000e+02	4.700020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 30	1.940000e+02	3.880000e+06	1.020000e+02	9.800000e+01	4.660020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 31	1.920000e+02	3.840000e+06	1.010000e+02	9.700000e+01	4.620020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 32	1.900000e+02	3.800000e+06	9.800000e+01	9.800000e+01	4.580020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 33	1.880000e+02	3.760000e+06	9.900000e+01	9.500000e+01	4.540020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 34	1.860000e+02	3.720000e+06	9.700000e+01	9.500000e+01	4.500020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 35	1.840000e+02	3.680000e+06	9.700000e+01	9.300000e+01	4.460020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 36	1.820000e+02	3.640000e+06	9.500000e+01	9.300000e+01	4.420020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 37	1.800000e+02	3.600000e+06	9.400000e+01	9.200000e+01	4.380020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 38	1.780000e+02	3.560000e+06	9.400000e+01	9.000000e+01	4.340020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 39	1.760000e+02	3.520000e+06	9.100000e+01	9.100000e+01	4.300020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 40	1.740000e+02	3.480000e+06	9.000000e+01	9.000000e+01	4.260020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 41	1.720000e+02	3.440000e+06	9.000000e+01	8.800000e+01	4.220020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 42	1.700000e+02	3.400000e+06	8.900000e+01	8.700000e+01	4.180020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 43	1.680000e+02	3.360000e+06	8.800000e+01	8.600000e+01	4.140020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 44	1.660000e+02	3.320000e+06	8.700000e+01	8.500000e+01	4.100020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 45	1.640000e+02	3.280000e+06	8.600000e+01	8.400000e+01	4.060020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 46	1.620000e+02	3.240000e+06	8.500000e+01	8.300000e+01	4.020020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 47	1.600000e+02	3.200000e+06	8.400000e+01	8.200000e+01	3.980020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 48	1.580000e+02	3.160000e+06	8.400000e+01	8.000000e+01	3.920020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 49	1.560000e+02	3.120000e+06	8.300000e+01	7.900000e+01	3.880020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 50	1.540000e+02	3.080000e+06	8.100000e+01	7.900000e+01	3.840020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 51	1.520000e+02	3.040000e+06	8.000000e+01	7.800000e+01	3.800020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 52	1.500000e+02	3.000000e+06	8.100000e+01	7.500000e+01	3.760020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 53	1.480000e+02	2.960000e+06	7.800000e+01	7.600000e+01	3.720020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 54	1.460000e+02	2.920000e+06	7.700000e+01	7.500000e+01	3.680020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 55	1.440000e+02	2.880000e+06	7.700000e+01	7.300000e+01	3.640020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 56	1.420000e+02	2.840000e+06	7.400000e+01	7.400000e+01	3.600020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 57	1.400000e+02	2.800000e+06	7.400000e+01	7.200000e+01	3.560020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 58	1.380000e+02	2.760000e+06	7.300000e+01	7.100000e+01	3.520020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 59	1.360000e+02	2.720000e+06	7.300000e+01	6.900000e+01	3.480020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 60	1.340000e+02	2.680000e+06	7.200000e+01	6.800000e+01	3.440020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 61	1.320000e+02	2.640000e+06	7.000000e+01	6.800000e+01	3.400020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 62	1.300000e+02	2.600000e+06	7.000000e+01	6.600000e+01	3.360020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 63	1.280000e+02	2.560000e+06	6.900000e+01	6.500000e+01	3.320020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 64	1.260000e+02	2.520000e+06	6.600000e+01	6.600000e+01	3.280020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 65	1.240000e+02	2.480000e+06	6.600000e+01	6.400000e+01	3.240020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 66	1.220000e+02	2.440000e+06	6.500000e+01	6.300000e+01	3.200020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 67	1.200000e+02	2.400000e+06	6.400000e+01	6.200000e+01	3.160020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 68	1.180000e+02	2.360000e+06	6.600000e+01	5.800000e+01	3.120020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 69	1.160000e+02	2.320000e+06	6.200000e+01	6.000000e+01	3.080020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 70	1.140000e+02	2.280000e+06	6.600000e+01	5.400000e+01	3.040020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 71	1.120000e+02	2.240000e+06	6.100000e+01	5.700000e+01	3.000020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 72	1.100000e+02	2.200000e+06	5.800000e+01	5.800000e+01	2.960020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 73	1.080000e+02	2.160000e+06	5.800000e+01	5.600000e+01	2.920020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 74	1.060000e+02	2.120000e+06	6.200000e+01	5.000000e+01	2.880020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 75	1.040000e+02	2.080000e+06	5.600000e+01	5.400000e+01	2.840020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 76	1.020000e+02	2.040000e+06	5.500000e+01	5.300000e+01	2.800020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 77	1.000000e+02	2.000000e+06	5.400000e+01	5.200000e+01	2.760020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 78	9.800000e+01	1.960000e+06	5.900000e+01	4.500000e+01	2.720020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 79	9.600000e+01	1.920000e+06	5.300000e+01	4.900000e+01	2.680020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 80	9.400000e+01	1.880000e+06	5.000000e+01	5.000000e+01	2.640020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 81	9.200000e+01	1.840000e+06	5.000000e+01	4.800000e+01	2.600020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 82	9.000000e+01	1.800000e+06	4.900000e+01	4.700000e+01	2.560020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 83	8.800000e+01	1.760000e+06	4.900000e+01	4.500000e+01	2.520020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 84	8.600000e+01	1.720000e+06	4.700000e+01	4.500000e+01	2.480020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 85	8.400000e+01	1.680000e+06	4.600000e+01	4.400000e+01	2.440020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 86	8.200000e+01	1.640000e+06	4.500000e+01	4.300000e+01	2.400020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 87	8.000000e+01	1.600000e+06	4.500000e+01	4.100000e+01	2.360020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 88	7.800000e+01	1.560000e+06	4.200000e+01	4.200000e+01	2.320020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 89	7.600000e+01	1.520000e+06	4.200000e+01	4.000000e+01	2.280020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 90	7.400000e+01	1.480000e+06	4.100000e+01	3.900000e+01	2.240020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 91	7.200000e+01	1.440000e+06	4.000000e+01	3.800000e+01	2.200020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 92	7.000000e+01	1.400000e+06	3.900000e+01	3.700000e+01	2.160020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 93	6.800000e+01	1.360000e+06	3.800000e+01	3.600000e+01	2.120020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 94	6.600000e+01	1.320000e+06	3.600000e+01	3.600000e+01	2.080020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 95	6.400000e+01	1.280000e+06	3.700000e+01	3.300000e+01	2.040020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 96	6.200000e+01	1.240000e+06	3.400000e+01	3.400000e+01	2.000020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 97	6.000000e+01	1.200000e+06	3.400000e+01	3.200000e+01	1.960020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 98	5.800000e+01	1.160000e+06	3.300000e+01	3.100000e+01	1.920020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
 99	5.600000e+01	1.120000e+06	3.400000e+01	2.800000e+01	1.880020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
100	5.400000e+01	1.080000e+06	3.200000e+01	2.800000e+01	1.840020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
101	5.200000e+01	1.040000e+06	2.800000e+01	3.000000e+01	1.800020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
102	5.000000e+01	1.000000e+06	2.900000e+01	2.700000e+01	1.760020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
103	4.800000e+01	9.600000e+05	2.900000e+01	2.500000e+01	1.720020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
104	4.600000e+01	9.200000e+05	2.800000e+01	2.400000e+01	1.680020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
105	4.400000e+01	8.800000e+05	2.500000e+01	2.500000e+01	1.640020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
106	4.200000e+01	8.400000e+05	2.500000e+01	2.300000e+01	1.600020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
107	4.000000e+01	8.000000e+05	2.400000e+01	2.200000e+01	1.560020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
108	3.800000e+01	7.600000e+05	2.500000e+01	1.900000e+01	1.520020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
109	3.600000e+01	7.200000e+05	2.300000e+01	1.900000e+01	1.480020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
110	3.400000e+01	6.800000e+05	2.100000e+01	1.900000e+01	1.440020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
111	3.200000e+01	6.400000e+05	2.100000e+01	1.700000e+01	1.400020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
112	3.000000e+01	6.000000e+05	1.800000e+01	1.800000e+01	1.360020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
113	2.800000e+01	5.600000e+05	1.800000e+01	1.600000e+01	1.320020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
114	2.600000e+01	5.200000e+05	1.700000e+01	1.500000e+01	1.280020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
115	2.400000e+01	4.800000e+05	1.600000e+01	1.400000e+01	1.240020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
116	2.200000e+01	4.400000e+05	1.500000e+01	1.300000e+01	1.200020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
117	2.000000e+01	4.000000e+05	1.300000e+01	1.300000e+01	1.160020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
118	1.800000e+01	3.600000e+05	1.300000e+01	1.100000e+01	1.120020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
119	1.600000e+01	3.200000e+05	1.300000e+01	9.000000e+00	1.080020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
120	1.400000e+01	2.800000e+05	1.000000e+01	1.000000e+01	1.040020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
121	1.200000e+01	2.400000e+05	1.000000e+01	8.000000e+00	1.000020e+06	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
122	1.000000e+01	2.000000e+05	9.000000e+00	7.000000e+00	9.600200e+05	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
123	8.000000e+00	1.600000e+05	8.000000e+00	6.000000e+00	9.200200e+05	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
124	6.000000e+00	1.200000e+05	7.000000e+00	5.000000e+00	8.800200e+05	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
125	4.000000e+00	8.000000e+04	6.000000e+00	4.000000e+00	8.400200e+05	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
126	2.000000e+00	4.000000e+04	5.000000e+00	3.000000e+00	8.000200e+05	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
127	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000000	0.000000000	0.000000000	0.000000000	0.000000000
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total	2.378500e+04	5.679226e+08	1.431600e+04	9.430000e+03	5.560625e+08	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Average	1.858203e+02	4.436895e+06	1.118438e+02	7.367188e+01	4.344238e+06	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Maximum	1.003000e+03	9.920252e+07	1.003000e+03	5.010000e+02	2.080002e+07	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Minimum	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	
Stdev	2.448592e+02	9.776150e+06	1.990684e+02	7.607676e+01	4.811872e+06	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	0.000000e+00	

Output Paraver trace "output.prv" generated

#+end_example


I am still trying to understand why all these warnings. Is something
we have done wrong during tracing?
* 2016-05-16 Analyzing the first draco trace

So we got warning when simulating the trace on Dimemas and an error on
the conversion of the paraver trace to the tit format.

Let`s take a better look at the original trace first.
We will be working at the =experiments/16-May-2016/draco1/= directory.

Let`s copy the files to the directory and convert the original trace
to the pjdump format.

#+begin_src sh
cd experiments/16-May-2016/draco1
wget https://dl.dropboxusercontent.com/u/1119921/output-ondes3d-64p-draco_1-4.tar.gz
tar xfz output-ondes3d-64p-draco_1-4.tar.gz -C . --strip-components=1
perl ../../../getpjdump/prv2pjdump.pl -i output-ondes3d-64p-draco_1-4 > output-ondes3d-64p-draco_1-4.pjdump
#+end_src

#+RESULTS:

And see what is going on with the task 16.

#+begin_src sh
cd experiments/16-May-2016/draco1
cat output-ondes3d-64p-draco_1-4.pjdump | grep 'rank-16'
#+end_src

#+RESULTS:

Nothing is going on with task 16 on the pjdump file.
Let`s check on the prv file (remember that its id on the prv file is plus 1).

#+begin_src sh
cd experiments/16-May-2016/draco1
cat output-ondes3d-64p-draco_1-4.prv | grep ':1:17:1:'
#+end_src

#+RESULTS:
: c:1:17:1:5

Nothing there too.
This can potentialy explains the warnings on Dimemas and specially why the conversion to tit was not working.

Let`s move on and look at the trace Lucas obtained with the 2d head application.




