#+TITLE: tcbozzetti's LabBook
#+AUTHOR: Tiago
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Lucas(L) Tiago(T) Arnaud(A) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Issues with =.tit= <2015-09-10 Thu>                                                   :tit:
I was testing the conversion of the =lulesh2.0_p216_n108_t1.chop1= trace
and I came across the following problem: This code snippet opens a
file for each node.
#+BEGIN_SRC perl
my $nb_proc = 0;
foreach my $node (@{$$resource_name{NODE}}) { 
	my $filename = $output."_$nb_proc.tit";
	open($fh[$nb_proc], "> $filename") or die "Cannot open > $filename: $!";
	nb_proc++;
}
#+END_SRC
While this code snippet assumes the number of opened files is equal to the number of tasks.
#+BEGIN_SRC perl
$task = $task - 1;
defined($tit_translate{$sname}) or die "Unknown state '$sname' for tit\n";
if($tit_translate{$sname} ne "") {
	print { $fh[$task] } "$task $tit_translate{$sname} $sname_param\n",
}
#+END_SRC
Currrently, I am assuming that there must be one =.tit= file for each
node, therefore, I modified the last snippet to write on the task's
node =.tit= file.

* Some notes <2015-09-13 Sun>                                                         :notes:
Understanding the performance of parallel applications is a concern in
the HPC community.  When considering platforms that are not available,
it is necessarty to simulate the application. Therefore, it is
possible to determine a cost-effective hardware configuration for that
particular application.  When considering a platform that is
available, simulating is also important since the access to
large-scale platforms can be costly.

Two frameworks for simulating MPI applications:
1. On-line simulations
	* The same amount of hardware is required to simulate.
	* Usually uses a simple network model to calculate the
          communication delays.
	* Can simulate the computational delays, but data-dependent
          application behavior is lost.
2. Off-line simulations
	* Use a trace of the parallel application.
	* Can be performed on a single computer.
	* Computational delays are scaled based on the performance
          differential between the original and the simulated
          platform.
		* If simulator uses time information, target and
                  original platform must be the same.
		* If trace is time independent, the only condition is
                  that the processors of the target platforms must be
                  of the same family as the original platform.
	* Communication delays are computed based on a network
          simulator.
	* Partially addresses the simulation of data-dependent
          application. Some data-dependent behavior can be captured in
          the trace and simulated.

Off-line simulators differ by the simulation models they employ to
compute simulated durations of CPU bursts and communication
operations.

On a time-independent trace, the CPU bursts or communication
operations are logged with its volume (in number of executed
instructions or in number of transferred bytes) instead of the time
when it begins and ends or its duration.  Therefore, the trace can not
be associated with a platform anymore (with the exception of the
processor family).  This imply that the MPI application can not modify
its execution according to the execution platform (AMPI applications).

*For large numbers of processes and/or numbers of actions, it may be
preferable to split the trace so as to obtain one trace file per
process.*

*Table: Time-independent actions corresponding to supported MPI communication operations.*
| MPI actions        | Trace entry                                  |
|--------------------+----------------------------------------------|
| =CPU burst=          | =<rank> compute <volume>=                      |
| =MPI_Send=           | =<rank> send <dst_rank> <volume>=              |
| =MPI_Isend=          | =<rank> Isend <dst_rank> <volume>=             |
| =MPI_Recv=           | =<rank> recv <src_rank> <volume>=              |
| =MPI_Irecv=          | =<rank> Irecv <src_rank> <volume>=             |
| =MPI_Broadcast=      | =<rank> bcast <volume>=                        |
| =MPI_Reduce=         | =<rank> reduce <vcomm> <vcomp>=                |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_counts> <vcomp>=   |
| =MPI_Allreduce=      | =<rank> allReduce <vcomm> <vcomp>=             |
| =MPI_Alltoall=       | =<rank> allToAll <send_volume> <recv_volume>=  |
| =MPI_Alltoallv=      | =<rank> allToAllv <send_volume> <send_counts>= |
|                    | =<recv_volume> <recv_counts>=                  |
| =MPI_Gather=         | =<rank> gather <send_volume> <recv_volume>=    |
| =MPI_Allgatherv=     | =<rank> allGatherV <send_count> <recv_counts>= |
| =MPI_Barrier=        | =<rank> barrier=                               |
| =MPI_Wait=           | =<rank> wait=                                  |
| =MPI_Waitall=        | =<rank> waitAll=                               |
Source: references/ref1.pdf
* Starting LabBook <2015-09-16 Wed> :journal:
Lucas setup the org mode file for the journal.
** Copied the entries from the README file
I have been looking at some org-mode documentation to make this
journal better. It turns out they have a ton of features and I plan
to make good use of them to make this project more organized.
* Testing the off-line simulation on SimGrid <2015-09-17 Thu> :tit:simgrid:
I was trying to simulate the time-independent trace files generated by
the =EXTRAE_Paraver_trace_mpich= trace. There was a problem, however.
More specifically, the issue was the ranks of the processes on the
=.tit= files. The little piece of code that I modified last thursday
assumed that the number of =.tit= files were equivalent to the number
of nodes, however that was only executed for state entries in the
trace. When an event was processed, the old code was executed and
therefore, the resulting =.tit= was inconsistent. I fixed this by
also changing the code that handles the events. Another problem is
that the ranks of the processes on the =.tit= files must start with
zero (I am not completely sure about this). If I set the ranks of
the processes to start with one, there is an exception during the
simulation. When I use the option =-map= on the =smpirun= command I get
the following (even when there is no rank zero on the =.tit= files):
#+BEGIN_SRC shell
[rank 0] -> graphene-1.nancy.grid5000.fr
[rank 1] -> graphene-2.nancy.grid5000.fr
[rank 2] -> graphene-3.nancy.grid5000.fr
[rank 3] -> graphene-4.nancy.grid5000.fr
[rank 4] -> graphene-5.nancy.grid5000.fr
[rank 5] -> graphene-6.nancy.grid5000.fr
[rank 6] -> graphene-7.nancy.grid5000.fr
[rank 7] -> graphene-8.nancy.grid5000.fr
#+END_SRC
This led me to believe that the ranks of the processes must start with
zero. This fact can cause some headache since the Paraver trace files
do not assume that they should start with zero. During the conversion
to the time-independent trace format, we simply compute the rank that
will be written on the =.tit= file by subtracting one. However, this
solution will not work for every case. Imagine if the Paraver trace uses
the rank zero...
* Compilation of all smpi replay operations <2015-09-17 Thu> :trace_replay:simgrid:tit:
I took a look at the source code of the =smpi_replay= (the
=references/ref1.pdf= does not contain all operations and all possible
arguments) and compiled all the smpi trace replay operations and its
arguments. I expect that this can be useful in the future.

| MPI actions        | Trace entry                                     |
|--------------------+-------------------------------------------------|
| =CPU burst=          | =<rank> compute <flops>=                          |
| =MPI_Send=           | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Isend=          | =<rank> send <dst> <comm_size> [<datatype>]=      |
| =MPI_Recv=           | =<rank> recv <src> <comm_size> [<datatype>]=      |
| =MPI_Irecv=          | =<rank> Irecv <src> <comm_size> [<datatype>]=     |
| =MPI_Broadcast=      | =<rank> bcast <comm_size> [<root> [<datatype>]]=  |
| =MPI_Reduce=         | =<rank> reduce <comm_size> <comp_size>=           |
|                    | =[<root> [<datatype>]]=                           |
| =MPI_AllReduce=      | =<rank> allReduce <comm_size> <comp_size>=        |
|                    | =[<datatype>]=                                    |
| =MPI_Reduce_scatter= | =<rank> reduceScatter <recv_sizes†> <comp_size>=  |
|                    | =[<datatype>]=                                    |
| =MPI_Gather=         | =<rank> gather <send_size> <recv_size> <root>=    |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGather=      | =<rank> allGather <send_size> <recv_size>=        |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoall=       | =<rank> allToAll <send_size> <recv_recv>=         |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Alltoallv=      | =<rank> allToAllV <send_size> <send_sizes†>=      |
|                    | =<recv_size> <recv_sizes†>=                       |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_GatherV=        | =<rank> gatherV <send_size> <recv_sizes†> <root>= |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_AllGatherV=     | =<rank> allGatherV <send_size> <recv_sizes†>=     |
|                    | =[<send_datatype> <recv_datatype>]=               |
| =MPI_Barrier=        | =<rank> barrier=                                  |
| =MPI_Wait=           | =<rank> wait=                                     |
| =MPI_Waitall=        | =<rank> waitAll=                                  |
| =MPI_Init=           | =<rank> init [<set_default_double>]=              |
| =MPI_Finalize=       | =<rank> finalize=                                 |
| =MPI_Comm_size=      | =<rank> comm_size <size>=                         |
| =MPI_Comm_split=     | =<rank> comm_split=                               |
| =MPI_Comm_dup=       | =<rank> comm_dup=                                 |
† =send_sizes/recv_sizes= is an array, the number of elements must be equal
to the number of processes in the communicator.

* More Paraver traces with more MPI primitives <2015-09-20 Sun> :tracing:paraver:
We need to make sure the conversion script is able to handle any kind of Paraver trace.
So far, our script does not handle some MPI primitives that are less used, such as MPI_AlltoAllv.
I tried to install the Extrae tool to use it to generate some traces but I failed in completing this task.

* New script for converting to =.tit= format <2015-10-01 Thu> :tit:
I created a new script based on the old one so I could have a better ideia of what was being done.
The script started supporting the MPI calls on the =EXTRAE_Paraver_trace_mpich= trace

* Immediate send format on the Paraver trace <2015-10-01 Thu> :paraver:tracing:
The =cgpop.linux_icc_mt.180x120.24tasks.chop1= trace contains immediate
sends and this was one of the reasons the script was failing. I
examined how the trace tool generates the entries in the trace file
when an immediate send happens. First we have a state entry telling us
that the task is in an immediate send state, then we have a event
entry (right after the state entry) with the =MPI_Isend= event. Keep in
mind that the parameters we need for an immediate send is the destiny
and the communication size. The event entry, however, does not provide
any of those. If we continue on the trace, we will find a
communication entry that is associated with the immediate send. This
communication entry is not necessarily after the event entry.  Also,
this entry contains the destiny and the message size. In the example
below the immediate send event entry is the first line while the
communication entry of the immediate send is the last line.
#+BEGIN_SRC shell
2:3:1:3:1:52372287:50000001:3:42000050:14833...
1:6:1:6:1:52372820:52379709:1
2:6:1:6:1:52372820:50000001:0:42000050:8708...
1:6:1:6:1:52379709:52390070:11
2:6:1:6:1:52379709:50000001:4:42000050:2191...
1:3:1:3:1:52389139:52396811:1
2:3:1:3:1:52389139:50000001:0
3:3:1:3:1:52372287:52389139:2:1:2:1:52853369:53050068:9912:103
#+END_SRC

* 2015-09-15 Questions
The list below contains all questions that I came across during this project.
Feel free to contribute with questions or answers :)
1. The use of a time-independent trace collected on a platform X can
   be used to simulate a platform Y under what conditions? Condition
   example, X and Y must contain the same family of processors or same
   number of processors. The reference 1 was not so clear about this.

* 2015-10-02 Using Arnaud's init.org                                  :Lucas:

Please, follow the instructions of:

http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php

* 2015-10-02 Installing Extrae                                  :Lucas:Tiago:

That's really hard, lots of dependencies, problems, log below tries to
handle all that, but several indications are Debian testing-specific.

Download latest Extrae from:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Dyninst dependencies in my box:

#+BEGIN_SRC sh
sudo apt-get install libboost-thread-dev libboost-system-dev libelf-dev
#+END_SRC

Download dyinst, configure and install:

#+begin_src sh :results output :session :exports both
cd ~/misc
wget --quiet http://www.paradyn.org/release9.0.3/DyninstAPI-9.0.3.tgz
tar xfz DyninstAPI-9.0.3.tgz
cd DyninstAPI-9.0.3
mkdir build
cd build
cmake ..
make
make install
#+end_src

Let's install other Extrae's dependencies:

#+begin_src R :results output :session :exports both
sudo apt-get install libopempi-dev libdwarf-dev libpapi-dev libbinutils-dev
#+end_src

I have put in my *misc* directory:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.1.0.tar.bz2
cd extrae-3.1.0
./configure --prefix=$HOME/install/extrae-3.1.0/ \
            --with-mpi=/usr/lib/openmpi/ \
            --with-unwind=/usr \
            --with-dyninst=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/home/schnorr/install/dyninst-9.0.3/ \
            --with-dwarf=/usr/ --with-papi=/usr --with-binutils=/usr
make
make install
#+END_SRC

After the configure and before make, I had to comment the line on the
Makefile that contained -lsymLite, since I was unable to find such
library in my system (or any other solution in the internet). I have
mailed tools@bsc.es to get a proper solution.

* 2015-10-02 Checking if Extrae was correctly installed               :Lucas:

So, I have both Extrae and Dyninst installed.

These are the steps to see if it is working:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR="/home/schnorr/install/stow/"
rm -rf $STOW_DIR/*
mkdir -p $STOW_DIR
echo $STOW_DIR
stow /home/schnorr/install/dyninst-9.0.3/
stow /home/schnorr/install/extrae-3.1.0/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin/
extrae
#+END_SRC

I wasn't able to run the following commands from org-mode.

If it works, you should have something like this:

#+BEGIN_SRC sh :results output :session :exports both
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
extrae
#+END_SRC

You should see the message:

+ *Extrae: You have to provide a binary to instrument*

* 2015-10-02 Checking if Extrae is able to instrument                 :Lucas:

Environment variables:

#+BEGIN_SRC sh :results output :session myses
export STOW_DIR=/home/schnorr/install/stow/
export LD_LIBRARY_PATH=$STOW_DIR/lib/
export PATH=$PATH:$STOW_DIR/bin
export EXTRAE_HOME=/home/schnorr/install/extrae-3.1.0/
#+END_SRC

#+RESULTS:


#+BEGIN_SRC C :tangle mpi_init_finalize.c
#include <mpi.h>
int main (int argc, char **argv)
{
  MPI_Init(&argc, &argv);
  MPI_Finalize();
}
#+END_SRC

Compile:

#+BEGIN_SRC sh :results output :session myses
mpicc mpi_init_finalize.c
#+END_SRC

#+RESULTS:

Instrument:

#+BEGIN_SRC sh :results output :session myses
export EXTRAE_CONFIG_FILE=/home/schnorr/install/extrae-3.1.0/share/example/MPI/extrae.xml
extrae a.out
#+END_SRC

#+RESULTS:
: 
: Welcome to Extrae 3.1.0 revision 3316 based on extrae/trunk launcher using DynInst 9.0.3
: Extrae: Creating process for image binary a.out
: Extrae: Error creating the target application process

* 2015-10-15 New Paraver traces                                       :Tiago:
After installing EXTRAE, we were able to generate the traces of a few
simple MPI applications.  Those applications can be found in the
directory =applications=, and a script to generate their traces is also
on the directory.  You need to have EXTRAE installed, however.  The
traces generated will be helpful to validade the translation to tit.

* 2015-10-15 New approach                                             :Tiago:
We came to the conclusion that we can not generate the =.tit= entries
one by one while reading the Paraver trace file.  The new script
=prv2tit.pl= uses buffers to store the MPI calls that have missing
parameters.  Those parameters can be obtained in a trace entry that
can be 10 or 20 lines after the event.  The immediate send MPI call is
an example of this situation.  Anyway, the script is able to translate
all MPI calls supported by SIMGRID except for the V operations.  The V
operations require some extra functionality in the script that will be
soon implemented.

* 2015-10-15 How to run it?                                           :Tiago:
To convert a Paraver trace file to the time-independent trace format execute the following command:
#+BEGIN_SRC shell
$ perl prv2tit.pl -i <paraver_trace_file>
#+END_SRC

* 2015-10-16 Meeting with Tiago                                 :Lucas:Tiago:

+ The /MPI_Comm_size/ is a function that receives the size of the
  communicator. It is pretty easy to obtain, Tiago will do it soon.
  An MPI application might have multiple communicators, but luckly the
  paraver trace file keeps track of all this. So we basically need a
  hash mapping the communicator identifier to its size. When the event
  /MPI_Comm_size/ appears for a given process, we only have to lookup in
  that hash table.
+ All operations that receive as parameter the *size* (of something:
  receive or send sizes particular to each process) are going to be
  implemented by Tiago soon. They are not yet translated because they
  are slightly more complicated to get the size parameter for each
  processes. Tiago already has an idea on how to deal with that, he's
  going to implement that soon. These are the concerned functions:
  /MPI_Reduce_scatter/, /MPI_Alltoallv/, /MPI_Allgatherv/, /MPI_GatherV/.
+ All operations that have /comp_size/ are not yet completed (for
  example: allReduce and reduce). This comp size means "computation"
  since they run an operation for the reduce. We have to take into
  account the computation cost of doing so, but as of now, we have no
  idea on how to obtain such metric from the trace. We could take the
  time spent in the reduce operation, but that means it would also
  take the communication time. We intend to discuss this with Arnaud
  to get his opinion.
+ Micro-applications for validation: Tiago has already done four
  micro-applications (in the applications dir). He is going to extend
  that in order to check if every single MPI operation supported by
  tit is being correctly translated.
+ The next step is simulating the tit traces using SimGrid. It would
  be better to use the *git* version since any changes can be considered
  at the moment they are pushed to there. Here's the git you have to
  clone:

  git+ssh://schnorr@scm.gforge.inria.fr//gitroot//simgrid/simgrid.git

+ We have used smpi2pj.sh script, but reading of tit files generated
  by Tiago's script is not working. The current idea of why is that
  ranks are starting at 1, instead of zero. We have looked in Arnaud's
  script and he indeed subtract 1 from task identifiers. We have done
  so rapidly (see commit) and it works.

+ We convert the paraver to tit, we simulate with /smpi_replay/ and we
  got a paje trace file, and then we convert it to CSV using /pj_dump/
  from pajeng package [1]. With that, we have the "SimGrid size".  To
  have the "Dimemas size", we get the paraver, we feed to to dimemas,
  we get another paraver that reflects the simulation that has been
  conducted by dimemas (so timestamps are going to change), and then
  we have to convert this second simulated paraver trace file to Paje,
  and then use /pj_dump/ to CSV, to finally tackle the comparison
  between Simgrid and Dimemas.

  + "SimGrid size" is almost ready.
  + "Dimemas size" depends on paraver2paje (Tiago has to build a
    paraver to paje converter as well, which is going a copy of the
    prv2tit, but much more easy).

[1]: https://github.com/schnorr/pajeng/

* 2015-10-17 SMPI replay and MPI_Comm_* calls                         :Tiago:
The MPI calls =MPI_Comm_size=, =MPI_Comm_split= and =MPI_Comm_dup= are
supported by the smpi replay command. These calls, however, do not
affect the simulation process in any way. Therefore, we will be
ignoring the events in the trace file that contain these MPI calls.

* 2015-10-18 Support of "V" operations                                :Tiago:
In order to support the "v" operations we made a tweak in the script
so it stores the communicators information.  Each "v" operation has a
buffer for storing its events in the communicator data structure.  To
generate a tit entry for that v operation, the mpi call event from all
the tasks in the communicator must be read first.  Currently
=MPI_Gatherv=, =MPI_Allgatherv=, =MPI_Reduce_scatter= are supported.
=MPI_Alltoallv= has a slightly more different implementation and will
be supported soon.  The next step will be to create a few MPI
applications for testing the translation of this MPI calls.

* 2015-10-24 New MPI apps                                             :Tiago:
I created a few more MPI applications on the =applications/=
folder. They contain all MPI calls supported by SIMGRID. The shell
script in the applications folder was used to generate their Paraver
traces, which can be found on the =paraver_traces/= folder. The next
step is use the conversion script to make sure all MPI calls are being
converted correctly.

* 2015-10-25 Validation with the new apps traces                      :Tiago:
I tested the generated application traces with the conversion
scripts. Only a few minor adjustments had to be made (print the
correct task index for example).  All of the MPI calls are correctly
translated into its tit version with one exception.  The MPI call
=MPI_Alltoallv= requires a serie of parameters that apparently can not
be obtained from the Paraver trace.  The tit format of this call
requires:

+ the size of the send buffer
+ an array containing the size of the msg sent to each process in the communicator
+ the size of the receive buffer and 
+ an array with containing the size of the msg to be received from with each process in the communicator.

The Paraver trace, however, only contain the total size of the msg
sent of each process. Therefore, the script is currently not
supporting this call until we find a workaround for this problem.
* 2016-01-18 Dynamic instrumentation only with Dyninst 8.2.1          :Lucas:

Here's the e-mail message I got from German Llort (from BSC).

We've noticed in the output logs that you sent that you're using
Dyninst version 9.0.3.

The last version that we've tested of Dyninst is 8.2.1, and it is very
likely that with the upgrade to version 9.x there's been some major
changes that broke compatibility. While we run some tests with this
last version, I can suggest you two ways to proceed:

- You can try installing Dyninst 8.2.1, which should work.

- If you don't need to instrument user functions, you can trace your
  application with the alternate method based on LD_PRELOAD (this
  mechanism doesn't use Dyninst). You have examples of this mechanism
  in section 7.2 of the user guide, and under the
  "share/examples/MPI/ld-preload" directory Extrae's home directory.

* 2016-01-18 Extrae installation in another system (Sara)             :Lucas:

I skip dyninst installation since I don't think I need it now to trace MPI.

I have download the latest 3.2.1 version from extrae here:

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

I have installed *libiberty-dev*.

Now I have issued the following commands:

#+BEGIN_SRC sh :results output :session :exports both
cd misc
tar xfj extrae-3.2.1.tar.bz2
cd extrae-3.2.1
#I need MPICC on Sara.
MPICC=/usr/bin/mpicc ./configure --prefix=/home/schnorr/install/extrae-3.2.1/ \
   --with-mpi=/usr/lib/openmpi/ \
   --without-unwind \
   --without-dyninst \
   --without-papi \
   --disable-openmp
make
make install
#+END_SRC

Configure, make and make install.
* 2016-01-18 Checking if extrae is able to instrument                 :Lucas:
TODO
* 2016-01-18 Problems of converted traces during replay               :Lucas:

I finally was able to run a replay from a tit file generated by the
conversion script. Here's how to do it. I'm supposing latest simgrid
from git was compiled and installed, all tools including =smpi_replay=
are in the PATH and can be correctly executed (=LD_LIBRARY_PATH= is also
configured).

#+begin_src sh :results output :session :exports both
export PATH=$PATH:$HOME/install/stow/bin/
export LD_LIBRARY_PATH=$HOME/install/stow/lib/
smpirun -keep-temps --log=replay.thresh:critical --log=smpi_replay.thresh:verbose --log=no_loc --cfg=smpi/running_power:1 --cfg=smpi/cpu_threshold:-1 -np 8 -platform griffon.xml -hostfile machine.txt smpi_replay paraver_traces/test.tit
#+end_src

#+RESULTS:

The smpirun command above seg faults, so running with keep-temps I can
run it manually with gdb. I was able to find out that the problem
comes from the gather event.

+ *Problems to be solved*:
  + =<comp_size>= should be replaced by 0 or the computational cost (if
    available, which is not the case for the paraver trace file)
  + =gather= event is not being correctly translated, that's the reason
    it segfaults. Documentation tells that we are not providing all
    parameters necessary for the gather. Here's where one of the
    problems appear:

    #+BEGIN_SRC text
    #4  0x00007ffff7a14188 in action_gather (action=action@entry=0x8280b0) at /home/schnorr/workspace/simgrid/src/smpi/smpi_replay.c:694
    694	    MPI_CURRENT_TYPE2=decode_datatype(action[6]);
    #+END_SRC

    From the documentation in =src/smpi/smpi_replay.c=:

    The structure of the gather action for the rank 0 (total 4
    processes) is the following:

    0 gather 68 68 0 0 0

    where: 
    1) 68 is the sendcounts
    2) 68 is the recvcounts
    3) 0 is the root node
    4) 0 is the send datatype id, see =decode_datatype()=
    5) 0 is the recv datatype id, see =decode_datatype()=

    Fred Suter told me there is something strange in the
    implementation of the gather replay. He just committed a solution
    that solves the problem very quickly.
* 2016-01-18 Current situation of translation                         :Lucas:

The following table is inspired from Table 1 of:
+ https://hal.inria.fr/hal-01064561/document

And also from the SimGrid code (=smpi_replay.c=) as of January 19th, 2016.


|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| *Name of the MPI action* | *TIT Entry*                                                                                                                                         | *Converted*                  |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Init=               | <r> init [default datatype]                                                                                                                       | ok (no datatype)           |
| =MPI_Finalize=           | <r> finalize (*not implemented in* =mpi_replay.c)=.                                                                                                   | ok                         |
| =MPI_Comm_size=          | <r> =comm_size= <double>                                                                                                                            |                            |
| =MPI_Comm_split=         | <r> =comm_split=                                                                                                                                    | ok                         |
| =MPI_Comm_dup=           | <r> =comm_dup=                                                                                                                                      | ok                         |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Send=               | <r> send <dst> <volume> [datatype]                                                                                                                | ok (no datatype)           |
| =MPI_Isend=              | <r> isend <dst> <volume> [datatype]                                                                                                               | ok (no datatype)           |
| =MPI_Recv=               | <r> recv <src> <volume> [datatype]                                                                                                                | ok (no datatype)           |
| =MPI_Irecv=              | <r> irecv <src> <volume> [datatype]                                                                                                               | ok (no datatype)           |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Test=               | <r> test                                                                                                                                          |                            |
| =MPI_Wait=               | <r> wait                                                                                                                                          | ok                         |
| =MPI_Waitall=            | <r> waitall                                                                                                                                       | ok                         |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| =MPI_Barrier=            | <r> barrier                                                                                                                                       | ok                         |
| =MPI_Bcast=              | <r> bcast <volume> [root] [datatype]                                                                                                              | ok (no datatype) / BUG     |
| =MPI_Reduce=             | <r> reduce <vcomm> <vcomp> [root] [datatype]                                                                                                      | ok (no datatype) / BUG     |
| =MPI_Allreduce=          | <r> allreduce <vcomm> <vcomp> [datatype]                                                                                                          | ok (no datatype)           |
| =MPI_Alltoall=           | <r> alltoall <sendvolume> <recvvolume> [datatype send] [datatype recv]                                                                            | ok (no datatype)           |
| =MPI_Alltoallv=          | <r> alltoallv <sendvolume> <sendcounts times communicator size> <recvvolume> <recvcounts times communicator size> [datatype send] [datatype recv] |                            |
| =MPI_Gather=             | <r> gather <sendvolume> <recvcounts> [send datatype] [recv datatype]                                                                              | ok (no datatypes) / BUG    |
| =MPI_Gatherv=            | <r> gatherv <sendvolume> <recvvolume times communicator size> <root> [send datatype] [recv datatype]                                              | ok (no datatypes)          |
| =MPI_Allgather=          | <r> allgather <sendvolume> <recvvolume> [send datatype] [recv datatype]                                                                           | ok (no datatypes)          |
| =MPI_Allgatherv=         | <r> allgatherv <sendvolume> <recvvolume times communicator size> [send datatype] [recv datatype]                                                  | ok (no datatypes)          |
| =MPI_Reduce_scatter=     | <r> reducescatter <recvvolume times communicator size> <vcomp> [datatype]                                                                         | ok (no datatypes)          |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|
| CPU burst              | <r> compute <volume>                                                                                                                              | ok (check =power_reference=) |
|------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------|

** Datatype translation

Check function =decode_datatype= at =smpi_replay.c=.

|----------------------+------------------|
| *Datatype translation* | *Kind*           |
|----------------------+------------------|
|                    0 | =MPI_DOUBLE=       |
|                    1 | =MPI_INT=          |
|                    2 | =MPI_CHAR=         |
|                    3 | =MPI_SHORT=        |
|                    4 | =MPI_LONG=         |
|                    5 | =MPI_FLOAT=        |
|                    6 | =MPI_BYTE=         |
|                    7 | =MPI_DEFAULT_TYPE= |
|----------------------+------------------|

** Action replay in Simgrid

#+BEGIN_SRC C
    xbt_replay_action_register("init",       action_init);
    xbt_replay_action_register("finalize",   action_finalize);
    xbt_replay_action_register("comm_size",  action_comm_size);
    xbt_replay_action_register("comm_split", action_comm_split);
    xbt_replay_action_register("comm_dup",   action_comm_dup);
    xbt_replay_action_register("send",       action_send);
    xbt_replay_action_register("Isend",      action_Isend);
    xbt_replay_action_register("recv",       action_recv);
    xbt_replay_action_register("Irecv",      action_Irecv);
    xbt_replay_action_register("test",       action_test);
    xbt_replay_action_register("wait",       action_wait);
    xbt_replay_action_register("waitAll",    action_waitall);
    xbt_replay_action_register("barrier",    action_barrier);
    xbt_replay_action_register("bcast",      action_bcast);
    xbt_replay_action_register("reduce",     action_reduce);
    xbt_replay_action_register("allReduce",  action_allReduce);
    xbt_replay_action_register("allToAll",   action_allToAll);
    xbt_replay_action_register("allToAllV",  action_allToAllv);
    xbt_replay_action_register("gather",  action_gather);
    xbt_replay_action_register("gatherV",  action_gatherv);
    xbt_replay_action_register("allGather",  action_allgather);
    xbt_replay_action_register("allGatherV",  action_allgatherv);
    xbt_replay_action_register("reduceScatter",  action_reducescatter);
    xbt_replay_action_register("compute",    action_compute);
#+END_SRC
** Comments about the implementation

+ =prv2tit.pl= / =power_reference= / =compute_action=
+ 
* 2016-02-15 DIMEMAS Installation                                     :Lucas:

Download DIMEMAS from

+ https://www.bsc.es/computer-sciences/performance-tools/downloads

Current version is:

+ dimemas-5.2.12.tar.gz

There are dependencies, check the README file and install them.

#+BEGIN_SRC sh
cd misc
tar xfz dimemas-5.2.12.tar.gz
cd dimemas-5.2.12
./configure --prefix=$HOME/install/dimemas-5.2.12/
make
make install
#+END_SRC

Here's the content after installation:

#+begin_src sh :results output :session :exports both
find /home/schnorr/install/dimemas-5.2.12/
#+END_SRC

#+RESULTS:
#+begin_example
/home/schnorr/install/dimemas-5.2.12/
/home/schnorr/install/dimemas-5.2.12/lib
/home/schnorr/install/dimemas-5.2.12/lib/GUI
/home/schnorr/install/dimemas-5.2.12/lib/GUI/dimemas-gui-5.2.12.jar
/home/schnorr/install/dimemas-5.2.12/lib/GUI/commons-io-2.4.jar
/home/schnorr/install/dimemas-5.2.12/sendrecv4.dim
/home/schnorr/install/dimemas-5.2.12/bin
/home/schnorr/install/dimemas-5.2.12/bin/trf2dim
/home/schnorr/install/dimemas-5.2.12/bin/DimemasGUI
/home/schnorr/install/dimemas-5.2.12/bin/DimemasUpdateCFG
/home/schnorr/install/dimemas-5.2.12/bin/Dimemas
/home/schnorr/install/dimemas-5.2.12/bin/prv2dim
/home/schnorr/install/dimemas-5.2.12/include
/home/schnorr/install/dimemas-5.2.12/include/extern_comm_model.h
/home/schnorr/install/dimemas-5.2.12/share
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/README
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/extern_comm_model.c
/home/schnorr/install/dimemas-5.2.12/share/lib_extern_model_example/Makefile
/home/schnorr/install/dimemas-5.2.12/sendrecv4.pcf
/home/schnorr/install/dimemas-5.2.12/sendrecv4.row
#+end_example

We have a =Dimemas= binary and a =prv2dim= that interest us.

Converting a file from =prv= to =dim= file format.

#+begin_src sh :results output :session :exports both
cd ~/install/dimemas-5.2.12/
./bin/prv2dim ~/svn/bozzetti/paraver_traces2/sendrecv4.prv sendrecv4.dim
#+END_SRC

#+RESULTS:
#+begin_example
INITIALIZING PARSER... OK!
SPLITTING COMMUNICATIONS 000 %SPLITTING COMMUNICATIONS 002 %SPLITTING COMMUNICATIONS 003 %SPLITTING COMMUNICATIONS 004 %SPLITTING COMMUNICATIONS 005 %SPLITTING COMMUNICATIONS 006 %SPLITTING COMMUNICATIONS 007 %SPLITTING COMMUNICATIONS 008 %SPLITTING COMMUNICATIONS 009 %SPLITTING COMMUNICATIONS 010 %SPLITTING COMMUNICATIONS 011 %SPLITTING COMMUNICATIONS 012 %SPLITTING COMMUNICATIONS 013 %SPLITTING COMMUNICATIONS 014 %SPLITTING COMMUNICATIONS 015 %SPLITTING COMMUNICATIONS 016 %SPLITTING COMMUNICATIONS 017 %SPLITTING COMMUNICATIONS 018 %SPLITTING COMMUNICATIONS 019 %SPLITTING COMMUNICATIONS 020 %SPLITTING COMMUNICATIONS 021 %SPLITTING COMMUNICATIONS 022 %SPLITTING COMMUNICATIONS 023 %SPLITTING COMMUNICATIONS 024 %SPLITTING COMMUNICATIONS 025 %SPLITTING COMMUNICATIONS 026 %SPLITTING COMMUNICATIONS 027 %SPLITTING COMMUNICATIONS 028 %SPLITTING COMMUNICATIONS 029 %SPLITTING COMMUNICATIONS 030 %SPLITTING COMMUNICATIONS 031 %SPLITTING COMMUNICATIONS 032 %SPLITTING COMMUNICATIONS 033 %SPLITTING COMMUNICATIONS 034 %SPLITTING COMMUNICATIONS 035 %SPLITTING COMMUNICATIONS 036 %SPLITTING COMMUNICATIONS 037 %SPLITTING COMMUNICATIONS 038 %SPLITTING COMMUNICATIONS 039 %SPLITTING COMMUNICATIONS 040 %SPLITTING COMMUNICATIONS 041 %SPLITTING COMMUNICATIONS 042 %SPLITTING COMMUNICATIONS 043 %SPLITTING COMMUNICATIONS 044 %SPLITTING COMMUNICATIONS 045 %SPLITTING COMMUNICATIONS 046 %SPLITTING COMMUNICATIONS 047 %SPLITTING COMMUNICATIONS 048 %SPLITTING COMMUNICATIONS 049 %SPLITTING COMMUNICATIONS 050 %SPLITTING COMMUNICATIONS 051 %SPLITTING COMMUNICATIONS 052 %SPLITTING COMMUNICATIONS 053 %SPLITTING COMMUNICATIONS 054 %SPLITTING COMMUNICATIONS 055 %SPLITTING COMMUNICATIONS 056 %SPLITTING COMMUNICATIONS 057 %SPLITTING COMMUNICATIONS 058 %SPLITTING COMMUNICATIONS 059 %SPLITTING COMMUNICATIONS 060 %SPLITTING COMMUNICATIONS 061 %SPLITTING COMMUNICATIONS 062 %SPLITTING COMMUNICATIONS 063 %SPLITTING COMMUNICATIONS 064 %SPLITTING COMMUNICATIONS 065 %SPLITTING COMMUNICATIONS 066 %SPLITTING COMMUNICATIONS 067 %SPLITTING COMMUNICATIONS 068 %SPLITTING COMMUNICATIONS 069 %SPLITTING COMMUNICATIONS 070 %SPLITTING COMMUNICATIONS 071 %SPLITTING COMMUNICATIONS 072 %SPLITTING COMMUNICATIONS 073 %SPLITTING COMMUNICATIONS 074 %SPLITTING COMMUNICATIONS 075 %SPLITTING COMMUNICATIONS 076 %SPLITTING COMMUNICATIONS 077 %SPLITTING COMMUNICATIONS 078 %SPLITTING COMMUNICATIONS 079 %SPLITTING COMMUNICATIONS 080 %SPLITTING COMMUNICATIONS 081 %SPLITTING COMMUNICATIONS 082 %SPLITTING COMMUNICATIONS 083 %SPLITTING COMMUNICATIONS 084 %SPLITTING COMMUNICATIONS 085 %SPLITTING COMMUNICATIONS 086 %SPLITTING COMMUNICATIONS 087 %SPLITTING COMMUNICATIONS 088 %SPLITTING COMMUNICATIONS 089 %SPLITTING COMMUNICATIONS 090 %SPLITTING COMMUNICATIONS 091 %SPLITTING COMMUNICATIONS 092 %SPLITTING COMMUNICATIONS 093 %SPLITTING COMMUNICATIONS 094 %SPLITTING COMMUNICATIONS 095 %SPLITTING COMMUNICATIONS 096 %SPLITTING COMMUNICATIONS 097 %SPLITTING COMMUNICATIONS 098 %SPLITTING COMMUNICATIONS 099 %SPLITTING COMMUNICATIONS 100 %
-> Trace first pass finished (communications split)
   * Records parsed:          210
   * Splitted communications 30
SORTING PARTIAL COMMUNICATIONS
COMMUNICATIONS SORTED
INITIALIZING TRANSLATION...  OK
CREATING TRANSLATION STRUCTURES  001/010CREATING TRANSLATION STRUCTURES  002/010CREATING TRANSLATION STRUCTURES  003/010CREATING TRANSLATION STRUCTURES  004/010CREATING TRANSLATION STRUCTURES  005/010CREATING TRANSLATION STRUCTURES  006/010CREATING TRANSLATION STRUCTURES  007/010CREATING TRANSLATION STRUCTURES  008/010CREATING TRANSLATION STRUCTURES  009/010CREATING TRANSLATION STRUCTURES  010/010CREATING TRANSLATION STRUCTURES  010/010
WRITING HEADER... OK
TRANSLATING COMMUNICATORS... OK
RECORD TRANSLATION
TRANSLATING RECORDS 000 %TRANSLATING RECORDS 002 %TRANSLATING RECORDS 003 %TRANSLATING RECORDS 004 %TRANSLATING RECORDS 005 %TRANSLATING RECORDS 006 %TRANSLATING RECORDS 007 %TRANSLATING RECORDS 008 %TRANSLATING RECORDS 009 %TRANSLATING RECORDS 010 %TRANSLATING RECORDS 011 %TRANSLATING RECORDS 012 %TRANSLATING RECORDS 013 %TRANSLATING RECORDS 014 %TRANSLATING RECORDS 015 %TRANSLATING RECORDS 016 %TRANSLATING RECORDS 017 %TRANSLATING RECORDS 018 %TRANSLATING RECORDS 019 %TRANSLATING RECORDS 020 %TRANSLATING RECORDS 021 %TRANSLATING RECORDS 022 %TRANSLATING RECORDS 023 %TRANSLATING RECORDS 024 %TRANSLATING RECORDS 025 %TRANSLATING RECORDS 026 %TRANSLATING RECORDS 027 %TRANSLATING RECORDS 028 %TRANSLATING RECORDS 029 %TRANSLATING RECORDS 030 %TRANSLATING RECORDS 031 %TRANSLATING RECORDS 032 %TRANSLATING RECORDS 033 %TRANSLATING RECORDS 034 %TRANSLATING RECORDS 035 %TRANSLATING RECORDS 036 %TRANSLATING RECORDS 037 %TRANSLATING RECORDS 038 %TRANSLATING RECORDS 039 %TRANSLATING RECORDS 040 %TRANSLATING RECORDS 041 %TRANSLATING RECORDS 042 %TRANSLATING RECORDS 043 %TRANSLATING RECORDS 044 %TRANSLATING RECORDS 045 %TRANSLATING RECORDS 046 %TRANSLATING RECORDS 047 %TRANSLATING RECORDS 048 %TRANSLATING RECORDS 049 %TRANSLATING RECORDS 050 %TRANSLATING RECORDS 051 %TRANSLATING RECORDS 052 %TRANSLATING RECORDS 053 %TRANSLATING RECORDS 054 %TRANSLATING RECORDS 055 %TRANSLATING RECORDS 056 %TRANSLATING RECORDS 057 %TRANSLATING RECORDS 058 %TRANSLATING RECORDS 059 %TRANSLATING RECORDS 060 %TRANSLATING RECORDS 061 %TRANSLATING RECORDS 062 %TRANSLATING RECORDS 063 %TRANSLATING RECORDS 064 %TRANSLATING RECORDS 065 %TRANSLATING RECORDS 066 %TRANSLATING RECORDS 067 %TRANSLATING RECORDS 069 %TRANSLATING RECORDS 070 %TRANSLATING RECORDS 071 %TRANSLATING RECORDS 072 %TRANSLATING RECORDS 073 %TRANSLATING RECORDS 074 %TRANSLATING RECORDS 075 %TRANSLATING RECORDS 076 %TRANSLATING RECORDS 077 %TRANSLATING RECORDS 078 %TRANSLATING RECORDS 079 %TRANSLATING RECORDS 080 %TRANSLATING RECORDS 081 %TRANSLATING RECORDS 082 %TRANSLATING RECORDS 083 %TRANSLATING RECORDS 084 %TRANSLATING RECORDS 085 %TRANSLATING RECORDS 086 %TRANSLATING RECORDS 087 %TRANSLATING RECORDS 088 %TRANSLATING RECORDS 089 %TRANSLATING RECORDS 090 %TRANSLATING RECORDS 091 %TRANSLATING RECORDS 092 %TRANSLATING RECORDS 093 %TRANSLATING RECORDS 094 %TRANSLATING RECORDS 095 %TRANSLATING RECORDS 096 %TRANSLATING RECORDS 097 %TRANSLATING RECORDS 098 %TRANSLATING RECORDS 099 %TRANSLATING RECORDS 100 %
MERGING PARTIAL OUTPUT TRACES
   * Merging task    1 100 %   * Merging task    2 100 %   * Merging task    3 100 %   * Merging task    4 100 %   * Merging task    5 100 %   * Merging task    6 100 %   * Merging task    7 100 %   * Merging task    8 100 %   * Merging task    9 100 %   * Merging task   10 100 %   * All task merged!         

********************************************************************************
 *                               WARNING                                        *
********************************************************************************
5 tasks of this application execute 'non-deterministic' communications 
primitives (MPI_Test[*] | MPI_Waitany | MPI_Waitall | MPI_Waitsome)
The simulation of this trace will not capture the possible indeterminism 
********************************************************************************

TRANSLATION FINISHED
GENERATING PCF
   * Input PCF /home/schnorr/svn/bozzetti/paraver_traces2/sendrecv4.pcf correctly copied to sendrecv4.pcf
COPYING ROW FILE
#+end_example

Great, it works.

Dimemas CLI simulator:

#+begin_src sh :results output :session :exports both
cd ~/install/dimemas-5.2.12/
./bin/Dimemas -h
#+end_src

#+RESULTS:
#+begin_example
Dimemas version 5.2.12

Compiled on Mon Feb 15 07:27:46 BRST 2016 
Usage: ./bin/Dimemas [-h] [-v] [-d] [-x[s|e|p]] [-o[l] output-file] [-T time] [-l] [-C]
	[-p[a|b] paraver-file [-y time] -z time]] [-x[s|e]]
	[-e event_type] [-g event_output_info] [-F] [-S sync_size]
	[-w] [-ES] [-Eo eee_network_definition] [-Ef eee_frame_size]
	[--dim input-trace] [--bw bandwidth] [--lat latency]
	[--ppn processors_per_node] [--fill] [--interlvd]
	 config-file

Required arguments:
	config-file	Dimemas configuration file

Supported options:
	-h		Display this help message
	-v		Display Dimemas version information
	-d		Enable debug output
	-xa		Force assertations - Vladimir: check if optimizations caused some errors
	-xs		Enable extra scheduling debug output
	-xe		Enable extra event manager debug output
	-xp		Enable extra Paraver trace generation debug output
	-o[l] file	Set output file (default: stdout) (l:long info) 
	-t		Show only simulation time as output
	-T time		Set simulation stop time
	-l		Enable in-core operation
	-C		Perform critical path analysis
	-p file		Generate Paraver tracefile (ASCII)
	-pc file	Use the given file as output Paraver configuration file
	-y time		Set Paraver tracefile start time
	-z time		Set Paraver tracefile stop time
	-e event_type	Show time distance between events occurrence
	-g event_output	File for output information on events occurrence
	-F		Ignore synchronism send trace field
	-S sync_size	Minimum message size to use Rendez vous
	-w	When generating Paraver trace, output the LOGICAL_RECV times when Wait primitives take place (Default: at IRecv execution time)
	-ES	Enables the EEE network model (you must use '-Eo' and '-Ef'
	-Eo eee_network_definition	Set the filename where the EEE network is defined
	-Ef frame_size	Sets the EEE network frame size (in bytes)
	--dim input-trace	Set input trace (overrides the configuration file)
	--bw  bandwidth	Set inter-node bandwidth (MBps, overrides the configuration file)
	--lat latency	Set inter-node latency for all nodes (seconds, overrides the configuration file)
	--fill	Set node filling task mapping (overrides the configuration file)
	--ppn tasks_per_node	Set 'n' tasks per node mapping (overrides the configuration file)
	--interlvd	Set interleaved node tasks mapping (overrides the configuration file)
#+end_example

Now I have to find out how to use Dimemas.

Looks like a configuration file is essential.

There is some interesting tutorials here:
+ https://www.bsc.es/computer-sciences/performance-tools/documentation

More specifically these ones:
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/2.introduction_to_dimemas.tar.gz
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/3.introduction_to_paraver_and_dimemas_methodology.tar.gz
+ https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/4.methodology_of_analysis.tar.gz

User-guide is here:
+ https://www.bsc.es/media/1324.pdf

Download all this:

#+begin_src sh :results output :session :exports both
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/2.introduction_to_dimemas.tar.gz
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/3.introduction_to_paraver_and_dimemas_methodology.tar.gz
wget -q https://www.bsc.es/sites/default/files/public/computer_science/performance_tools/4.methodology_of_analysis.tar.gz
wget -q https://www.bsc.es/media/1324.pdf
#+end_src

#+RESULTS:

* 2016-03-29 Plan for next weeks                                      :Lucas:

Here's what I think is missing from the technical side:

- A converter from paraver to =pj_dump=-like. I think Arnaud already did
  that, but we should merge what he did with the latest changes I have
  made in the =prv2tit.pl= script.
- Perhaps some performance improvements in the conversion script
  - Dump rightaway whenever information is complete, release memory
  - Today that's not the case, we buffer everything in memory (slow)
  - This is not strictly necessary (focus on next experimental part)

Them, for the experiments (from a non-partial paraver trace file):

1. _Replay with Dimemas_, get the trace from the replay (in prv format)
   - Understand how Dimemas model the Marenostrum machine
   - Get a complete trace from Barcelonians to start working with
2. _Platform file for Simgrid_
   - Create a Marenostrum platform file for Simgrid
   - Convert original paraver file to tit with =prv2tit.pl=
3. _Replay with SimGrid_, using the tit file
   - Get the trace, convert with =pj_dump= to the CSV-like
4. _Find a comparison metric_
   - It might be a simple gantt-chart with ggplot2
   - Then evolve proposing a metric (should discuss this with Arnaud)

You might find some information here.
- http://simgrid.gforge.inria.fr/contrib/smpi-paraver.php
